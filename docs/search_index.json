[["index.html", "FST 5984: SS Data Analytics for Food and Ag Science Compiled course lectures from Spring 2022 Preface Course description Course Learning Objectives What should I do with this material?", " FST 5984: SS Data Analytics for Food and Ag Science Compiled course lectures from Spring 2022 Jacob Lahne 2022-07-20 Preface This bookdown compiles lectures I wrote for my first attempt to teach a Data Science course to Food and Ag Science students at Virginia Tech. In general, the material here is aimed at advanced undergrads or first-year graduate students who are working in quantitative sciences, have taken 1-2 statistics courses, but do not have formal experience with coding or data science. If you want to know more about how this was built, you can take a look at the github repo through this link or by clicking the edit button above. If you want to know more about the course, you can take a look at the short description and learning objectives below: Course description Programming, algorithmic thinking, and data analysis for research projects in food and agricultural sciences. Writing R scripts to import, clean, and wrangle data. Data visualization, exploratory data analysis, and inference using programmatic approaches. Data Analytics for Food and Ag Science introduces graduate students in Food Science &amp; Technology to modern principles of coding for data wrangling and analysis. Students will develop competency in the R programming and data analysis language. They will learn basic principles of coding including control loops and functions, apply these skills to the acquisition, importing, and cleaning of data, and develop intuitive approaches to the exploratory and inferential analysis of data from food and agricultural sciences. Students will apply these competencies to several experiential projects in which they will be responsible for both defining and achieving analytical goals related to data they are assigned and will generate themselves. Course Learning Objectives Achieve fluency in the use of R for data analysis including proficient use of integrated development environment tools, data transformation, control flow logic, troubleshooting, and debugging. Implement transparent, reproducible data analysis workflows for agricultural applications including data collection and manipulation, visualization, statistical analyses, and presentation of conclusions. Differentiate among appropriate analytical techniques to make informed operational decisions in agricultural settings. Develop long-term durability in problem solving and critical thinking by “learning to learn” different techniques and strategies for data analysis. What should I do with this material? This is the first time I have built a bookdown. The material here is the first time I taught a course on coding, and the first time I tried to teach it using R Markdown. That’s a lot of firsts! I am sure I have made many weird choices (read: mistakes), and so this is very much a “learn by doing” situation for me. Please feel free to use this material if it’s helpful for you. I would appreciate feedback on it–you can email me at jlahne at vt dot edu with comments, suggestions, or to let me know how you’re using the material. "],["day-1---welcome-to-fst-5984-2022.html", "1 Day 1 - Welcome to FST 5984 (2022) 1.1 Coding 1.2 How to make errors and annoy computers 1.3 Inputs and data types in R 1.4 Metacognition: Learning to Learn in R", " 1 Day 1 - Welcome to FST 5984 (2022) Welcome to your first day in FST 5984: Data Analytics for Food and Ag Science! In this class, we’re going to use R and RStudio to learn about “coding for research”, a concept that I have taken from the Software Carpentries organization. The basic goal of this course is to teach you concepts of coding and programming for dealing with data in a research context. This is different from being a programmer who codes software for a living, making apps, etc. I will be calling that “coding for production”. In this course we are interested in developing our ability to do basic coding, and to “learn to learn” around coding, programming, and data science. As the expert instructor teaching this course, I can confidently say that I don’t know everything there is to know about data analysis and science, and I am certainly not the best, fastest, or most efficient programmer (and I have no idea how to make an app)! But I do know the basics of how coding works–the “control loops”, functions, data types–and I am able to use these basic building blocks to find resources to help me accomplish the tasks I need to do for research: for example, in the last year I taught myself to use the machine-learning toolkit keras (in R, as it happens) to do some text analysis. My main goal for the class is that you develop the basic skills and confidence to do the same things, with your own research needs. In this class we are going to be learning to code in R, using the RStudio interface/IDE! There are a couple reasons for this. First, of programming languages, R and RStudio are built for data analysis, and so are often presented to students at University as an alternative to programs like Minitab, SAS, SPSS, or JMP, which are only “free” to students because of expensive deals at the University level, which are passed back on to students as part of student fees. Second, unlike those tools, R is open source, and is constantly being updated with new functionality. It is almost impossible to find a type of data analysis that isn’t supported in R. Third, unlike most other programming languages, R is built for research programming (data analysis), rather than for production programming. The only other alternative that is as widely supported in the research community is Python, but–honesty time here–I have never learned Python very well, and so we are learning R. And, in addition, Python doesn’t have as good an IDE as RStudio. RStudio is an “Interactive Development Environment” (IDE) for working with R. Without going into a lot of detail, that means that R lives on its own on your computer in a separate directory, and RStudio provides a bunch of better functionality for things like writing multiple files at once, making editing easier, autofilling code, and displaying plots. You can learn more about RStudio here. With that out of the way, I am going to be sloppy in terminology and say/type “R” a lot of the times I mean “RStudio”. I will be very clear if the distinction actually matters. RStudio is going to make your life way easier, and when you try to learn Python you are going to be sad :( 1.0.1 Syllabus Review We are going to spend some time reviewing the class syllabus in Word (*gasp*). I want to make sure I answer everyone’s questions and concerns. We will also check out Canvas, make sure it all makes sense, and be ready to launch ourselves into the glorious future of the semester. 1.0.2 Getting set up for class In this class, we are going to be doing a lot of “live coding” (see below). Our weeks will follow a predictable rhythm: On Tuesdays, I will give a somewhat lecture-y live coding talk in which we will work through the week’s material. I say “lecture-y” because I will be talking a lot, but in general I don’t think I’m going to use slides. Instead, I will be switching between the rendered version of that week’s lecture notes (in HTML from R Markdown–don’t worry, we’ll learn about these things) and R itself. I will write out code as we go and show you how it executes. I’ll make mistakes, get frustrated, have to troubleshoot–this is all part of coding. To prepare for Tuesday classes, you should: Do the assigned reading/watching/exercises for that week. Bring your laptop to class, with the assigned (empty) Markdown file for the week. Be prepared to “code along”–in order to learn and practice, you will be participating in class by writing in the code chunks (again, don’t worry about it yet) into the R Markdown file. Be logged into the class Zoom room, which is the hack-y way we will set up screen sharing in this old classroom. On Thursdays, we will split the class into two parts. The main purpose of Thursdays is for you to work on livecoding. This will be the opportunity for you to practice what we did on Tuesday (and in previous weeks). In order to make this work, here’s what I am expecting you to do: Spend at least 1 hour trying to complete the weekly homework assignment between Tuesday and Thursday (this will generally be a coding assignment). It’s ok if you can’t finish it, run into errors, or get stuck. This is what class is for. Bring your assignment draft to class, and be prepared to ask questions. This is where you can get unstuck, and get suggestions from me and from your classmates on what to do. Several times over the semester, you will be expected to be the “lead” presenter of the homework, in which you will walk through everything you’ve done. On other days, you can be a problem solver or just ask specific questions. We can and will also go over how to fill out code chunks from the lecture, etc. This is, however, secondary. Finally, we will sometimes (knowing me, unfortunately, often) have some leftover material from Tuesday that we will cover as time allows. In order to make sure you can participate in class, your attendance expectation includes the following: Bring your laptop to class with enough battery to use it for the entire period. Download the week’s R Markdown file. (On Thursdays) bring in your draft assignment. Follow any other specific instructions for the week. 1.1 Coding With that introduction out of the way, let’s do some coding. This course is going to include a lot of “live coding”. That means that someone (me, but shortly you!) will be sharing their screen and typing and executing commands in R. This is because coding is like any other skill we develop–the only way to improve is to try something, see why it didn’t work, and try again! Live coding helps us see that everyone, me included, messes up, has to look for help, etc. An error message is just the first step to getting your code to work! What is coding, though? This document is an “R Markdown” document. That means it contains mixes of formatted text/multimedia and code. They look nicer than plain code. RStudio tells them apart because (among several things) RMarkdown files have a file extension of .Rmd, while normal scripts are plain text (.txt) or, by default, .R. Code in RMarkdown files lives in “chunks” that are delineated with triple backticks ``` on both ends. Without going into the mechanics, this tells the program (RStudio) that whatever’s in there isn’t just more text, but instead is code that can be run. Let’s take a look. print(&quot;Hello World&quot;) ## [1] &quot;Hello World&quot; It’s traditional in programming to have your first program say “hello world”. So we did that. We can also use these chunks to do some more useful stuff. R can be a calculator: 2 + 5 ## [1] 7 1300 * pi / 17^2 ## [1] 14.13173 We can ask R to do lots of neat stuff, like generate random numbers for us. For example, here are 100 random numbers from a normal distribution with mean of 0 and standard deviation of 1. rnorm(n = 100, mean = 0, sd = 1) ## [1] -1.3572594946 -0.5532073915 0.5398173741 0.9358743125 -0.2607471767 ## [6] -0.7924009217 -0.2938038129 -1.3060731396 -0.4469650196 -1.2201805390 ## [11] 0.0346690435 0.2293843191 -0.5293076892 0.2256660572 1.3151642099 ## [16] -1.6060439299 -0.5077579672 1.6528416449 0.6554347644 0.2735535096 ## [21] 0.2776140673 0.2495769024 -1.9352797265 -0.9177436106 2.9350194350 ## [26] 0.2138848632 -1.2313316930 0.1815047878 -0.0001928865 0.0285104869 ## [31] -1.0831225369 -0.5141562216 -0.5179313311 1.3726723247 -0.8337516284 ## [36] -0.3834555293 -0.1432776305 1.1041582059 -1.0650010122 -1.1719961068 ## [41] 0.2589669502 -0.3220053612 0.6717785785 -0.0880113766 -0.7991680320 ## [46] 0.3533267827 -0.5468060233 -1.1012836488 2.0499121881 -0.4690042893 ## [51] 0.6343963317 0.1444403525 -0.6761447208 0.9897492236 1.4087605831 ## [56] 0.5879784407 -0.5222958560 0.9795571866 2.0712832105 -3.4473855644 ## [61] -0.7543109072 -0.4735069971 -1.0753115148 0.9973642103 -0.3950685142 ## [66] -0.9101213626 0.6911403701 -0.1016053708 1.7202005883 -0.5006289379 ## [71] -0.9358110461 1.9858916501 -0.8960202448 1.0559972864 0.8800741709 ## [76] 0.5359729547 0.1950182227 0.7770604788 -0.3674226679 0.6429431680 ## [81] -0.8575215799 -0.3857283399 -0.2907557431 1.3648062410 -1.6614424720 ## [86] -0.1928847585 -1.0317387739 -0.2699448714 0.5574379955 1.2712481385 ## [91] 0.7323656876 0.4263217270 0.7598277532 0.8928244464 -0.3192189969 ## [96] 1.4492983613 1.1003609250 -1.8979135063 -0.5817651485 -1.7487197472 And, of course, we can look at structured data… mtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 And even visualize that structured data (and I promise we can make it prettier than this). plot(x = mtcars$wt, y = mtcars$mpg, xlab = &quot;car weight&quot;, ylab = &quot;car MPG&quot;) Where R starts to distinguish itself from a simple calculator or even a sophisticated program like SPSS or SAS or JMP is that it lets us programmatically create and store variables, which will let us build up workflows that are reproducible, portable, etc. The first step of that is the first thing you should really commit to memory: how to store objects in R. This set of characters is the assignment operator: &lt;-. It works like this: x &lt;- 100 hi &lt;- &quot;hello world&quot; data_set &lt;- rnorm(n = 100, mean = 0, sd = 1) … but that didn’t do anything! Where’s the output? Well, we can do two things. First, look at the “Environment” tab in your RStudio after you run the above code chunk. You’ll notice that there are 3 new things there: x, hi, and data_set. In general I am going to call those objects–they are stored variables, which R now knows about by name. How did it learn about them? You guessed it: the assignment operator: &lt;-. To be explicit: x &lt;- 100 can be read in English as “x gets 100” (what a lot of programmers like to say) or, in a clearer but longer way, “assign 100 to a variable called x”. NB: R also allows you to use = as an assignment operator. DO NOT DO THIS!. There are two good reasons. It is ambiguous, because it is not directional (how are you sure what is getting assigned where?) This makes your code super confusing because = is the only assignment operator for arguments in functions (as in print(quote = FALSE) see below for more on this) Anyone who has used R for a while who sees you do this will roll their eyes and kind of make fun of you a little bit NB2: Because it is directional, it is actually possible to use -&gt; as an assignment operator as well. What do you think it does? Check and find out. And of course we can use R code for doing statistics (if we run out of more exciting things to do). summary( lm(mpg ~ wt, data = mtcars) ) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 So that’s great. I think you’ve probably learned all this already in a course on statistics. Why are we here? Well… how would you run ANOVA on this dataset? Or, even more importantly (who cares about these cars?), how would you get your own data into R? How would you simulate data? How would you make a pretty plot? These are the kinds of things we’re going to learn in this course. 1.2 How to make errors and annoy computers We could actually call any course on coding for research by the above title… Let’s back it up and look at what we did above. What’s with the different colored text, and the parentheses, and the quotation marks? It does seem kind of perverse that, in order to get hello world We are instead typing almost twice as much: print(\"hello world\"). What’s going on? Hello World # Let&#39;s make an error ## Error: &lt;text&gt;:1:7: unexpected symbol ## 1: Hello World ## ^ The programming tradeoff is that your computer is much faster than you at doing tedious calculations (see the above 100 random numbers, for example), but much worse than you at interpreting unstructured commands and suggestions. So coding is really a structured language that tells the computer exactly what you want it to do. The first tip to reading code is that, in R, code is “from the inside out”. And parentheses “()” tell us what is inside and what is outside. So first we have \"hello world\" and it is inside print(). But we just saw that typing “hello world” into R gives us an error, right? &quot;hello world&quot; ## [1] &quot;hello world&quot; Actually, no! We just didn’t tell R that “hello world” was nonsense text (to R, at least), by enclosing it in quotations. Technically, this tells R that it is a string variable—a collection of non-numeric characters (non 0-9) characters. But we don’t need to quote the calculations (using numbers and symbols)… 1 + 1 # this works just fine ## [1] 2 or even named objects like “pi” (the mathematical constant) or the dataset (“mtcars) like those we did previously. Why is this? pi # this is also fine ## [1] 3.141593 To answer this question, let’s talk about what the various things you can type into R are. 1.3 Inputs and data types in R 1.3.1 Whitespace First things first. R as a programming language is very generous about whitespace use. “Whitespace” means any characters like spaces, tabs, returns/newlines, etc. These are actually different, although we don’t think about them very much, and for almost everything you do in R you don’t need to think about this distinction. In some languages (like Python or C++) you have to be careful about whitespace. In R, most of the time you can use whitespace to make your code readable and it will not change anything. The main use for whitespace in R is to delineate objects, operations, and functions. This means that spaces tell R that you are done telling it about one thing and are ready to tell it about another. For example, if we write pi + 2 then R sees 3 things it needs to know about - the object called “pi” (which is a system-level constant), the + operator, and a numeric constant: 2. If you then hit enter, R will take these 3 things and interpret them: it knows that + takes the object immediately to the left and to the right and performs addition. In general R will try to interpret what you give it, adding spaces implicitly when it can figure out how. So, for example, the following will all do the same thing: 100 + 1 ## [1] 101 100+ 1 ## [1] 101 100+1 ## [1] 101 100+ 1 ## [1] 101 100 + 1 ## [1] 101 The last one is particularly pathological–try not to do this! But what R really cares about is that groups of characters that represent operations, characters, and variables not be interrupted by spaces. When it encounters special characters like + or ( it doesn’t need spaces around them–it infers them. This is also why you can’t create objects with those characters in them: bad(_name &lt;- 2 ## Error: &lt;text&gt;:1:6: unexpected symbol ## 1: bad(_name ## ^ (Technically you can do this using some workarounds, but I don’t recommend it right now. Ask me if you want to know.) This is also one of the reasons why typing something like hello world will cause an error. First off, we haven’t created objects called hello or world. But even if we did, we would get an error: hello &lt;- &quot;hello&quot; world &lt;- &quot;world&quot; hello world hello world ## Error: &lt;text&gt;:5:7: unexpected symbol ## 4: world ## 5: hello world ## ^ When you give R a variable with no other instructions, it will try to help you by calling print() on it. But if you give it two, separated by a space, it has no idea what to do. So it throws an error. But what kinds of things are data, anyway? 1.3.2 Data types In the programming tradeoff is that, as part of the bargain of being extremely exact with our instructions, we agree to not only tell computers exactly what to do step by step, but also to define what everything they encounter is. This includes data types. This isn’t a computer science class, so I am going to be defining these types of data on a functional basis, rather than a scientific or technically correct one. That is, these are my personal definitions of what these data types are and how I encounter them. You can ask about what kind of data an object is by using the class() function on the object. In my experience, these are listed in increasing order of complexity. 1.3.2.1 Logical The most basic type of variable is a binary, yes/no, TRUE/FALSE variable. In R, these are called “logical variables”, although I think in other languages they might be called “boolean” variables. These can take only two values, which are both reserved words (cannot be changed, have default values) in R: TRUE and FALSE. These are used frequently when we want to define experimental levels or indicate group membership: yes &lt;- TRUE no &lt;- FALSE yes ## [1] TRUE no ## [1] FALSE class(yes) ## [1] &quot;logical&quot; These variables are also connected tightly to “control flow” statements like if, for, while, and to statements of logical equivalency that are known as “boolean logic”. We will go over these in a later class, but an example is the following. # is a random variable larger than 0? x &lt;- rnorm(n = 1, mean = 0, sd = 1) x &gt; 0 ## [1] TRUE # if that random variable is larger than 0, print something to tell us if(x &gt; 0) print(&quot;Well, there was a 50% chance!&quot;) ## [1] &quot;Well, there was a 50% chance!&quot; As a note, R will helpfully transform logical variables into FALSE = 0 and TRUE = 1 if you try to do algebra with them. So for example: sum(FALSE, FALSE, TRUE, TRUE, TRUE) ## [1] 3 0 and 1 are integers. 1.3.2.2 Integer Integers, as you may remember from math class, are positive or negative whole numbers, and include 0 (they’d have to, but let’s not get into set theory.) These are the next step up in complexity. Integers are dealt with differently by computers than “double” or “floating point” numbers. But in general we don’t use them much, except to pass to and from other languages (C++ and Fortran, according to the help page for integer()) that need integers. R indicates that numbers are integers by writing them as &lt;whole number&gt;L. So, for example, 3L is the same thing as 3. This can be confusing, but you should never have to do this explicitly. That’s because, for our purposes, we will always be dealing with numeric variables. Hey, psst, there’s no code chunk here, so… remember what I said about that help file? Try typing ?integer into the console and see what happens. We’ll talk about it below, but ? followed immediately by the term you want help on will get you that help. 1.3.2.3 Numeric We care especially about numbers, because most of our data analysis in food and ag sciences will be on numeric data. For computers, real numbers (numbers that can be represented with decimal precision) are called numeric or double. The second term (double) refers to the level of precision the computer actually offers, but this is beyond our scope (and my understanding!)–suffice it to say that, for our purposes, we will not be running into what are called “numerical overflow/underflow” errors. a_integer &lt;- 3L a_double &lt;- 3 class(a_integer) ## [1] &quot;integer&quot; class(a_double) ## [1] &quot;numeric&quot; # Do these two things equal each other? a_double == a_integer ## [1] TRUE # Is the square root of the (integer) 3 an integer? class(sqrt(a_integer)) ## [1] &quot;numeric&quot; 1.3.2.4 Character Ok, but we have seen already us using things that definitely aren’t numbers. For these, R defines the chatacter data type (which I also often call string data, which is a term used by some other programming languages). In R, character data is quoted and not interpreted–it does not have mathematical or functional/object properties. It is the most general data type. a_integer &lt;- 3L a_character &lt;- &quot;3&quot; a_string &lt;- &quot;hello_world&quot; hello_world &lt;- function() print(&quot;hello world&quot;) a_string ## [1] &quot;hello_world&quot; hello_world() ## [1] &quot;hello world&quot; print(hello_world) ## function() print(&quot;hello world&quot;) # Does R know that the character &quot;3&quot; represents the integer 3? Sort of, but not enough to do math. a_integer + a_character ## Error in a_integer + a_character: non-numeric argument to binary operator hello_ world # hmm, what went wrong here? ## Error: &lt;text&gt;:1:8: unexpected symbol ## 1: hello_ world ## ^ These can get kind of tricky, because R is interpreted as a language, and the interpreter tries to be smart. So if you ask R whether 3L == \"3\", it will tell you TRUE. This is because R uses “conversion”, and automatically runs conversion functions to try to compare different data types. as.character(3L) ## [1] &quot;3&quot; as.character(3L) == &quot;3&quot; ## [1] TRUE 3L == &quot;3&quot; ## [1] TRUE We’ll talk more about as_*() functions to convert between data types in future classes. 1.3.2.5 And bears, oh my! Yeah, this is where we’d talk about vector, matrix or more complex objects. We will get to these. But the general thing I want to emphasize is that these objects are collections of the simple objects we’re discussing here. In R, it’s turtles all the way down–complex things in R are built out of simple things in R. That doesn’t mean they’re easy–but it does mean that if we can get an idea of how simple things work, we can start to pick apart the complex things. To start with, we’ll talk about just one type of complex object: vector. Vectors are ordered lists of one type of data. In R, they are defined using the c() function: c(1, 2, 3, 4, 5) ## [1] 1 2 3 4 5 Vectors have a couple of properties. All objects in a vector are of the same type. See what happens if you try to make a vector like c(\"red\", 1, TRUE). What is R doing in the background? The vector has the same type as the objects in it. So a vector of numeric objects is a numeric vector. Actually, single objects in R are technically one item vectors: 1L == c(1L) 2. Vectors in general behave the way you would expect from linear algebra. Don’t remember linear algebra? That’s ok, but it’s still true. Vectors are ordered. This is important, because it will allow us, in a later class, to access items in the vector. a_vector &lt;- c(&quot;this&quot;, &quot;is&quot;, &quot;a&quot;, &quot;character&quot;, &quot;vector&quot;) a_vector ## [1] &quot;this&quot; &quot;is&quot; &quot;a&quot; &quot;character&quot; &quot;vector&quot; a_vector[3] # What&#39;s the third word in this sentence? ## [1] &quot;a&quot; 1.3.3 Ok, but what kind of object is print() then? Short answer is that print() and anything else that is of the form &lt;text&gt;() is a function. We will be discussing functions more in later classes, because they are a big part of programming, but we can best understand them right now as bit of pre-written code that take input and spit out results. The term “function” comes from As we will learn, R functions are (mostly) written in R. So you can write new functions, like so (note the use of the function() function!): hello_world &lt;- function(){ print(&quot;hello world&quot;) } hello_world() ## [1] &quot;hello world&quot; But another great side-effect of this is that you can look into how R functions you didn’t write work because they are written in R syntax. This is because if you ask R about the object rather than run the function, R will spit back the code that defines the function. …what? Well, what that means is that if you type hello_world() R will run the function called “hello_world”. But, if you type hello_world without the (), R will spit back the R code that defines the function (because that is what’s assigned to that object above, where we started by writing hello_world &lt;-): hello_world ## function(){ ## print(&quot;hello world&quot;) ## } And functions can be awesome, and complicated. Here is one that will randomly draw 1000 observations from the normal distribution and then plot a density curve in order to give you a nice density curve (something that can be a pain to draw by yourself). make_a_nice_bell_curve &lt;- function(plotnum = NULL){ if(!is.null(plotnum)){ plotname &lt;- paste0(&quot;This is plot #&quot;, plotnum) }else{ plotname &lt;- &quot;A nice plot&quot; } samples &lt;- rnorm(1000, 0, 1) plot(density(samples), main = plotname) } op &lt;- par(mfrow = c(2, 2)) for(i in 1:4) make_a_nice_bell_curve(plotnum = i) par(op) But we could just copy and paste the code above 3 times, or even just hit cmd + enter with those two commands highlighted. Why make a function? Well, we will learn a lot more about functions in the coming weeks, but here are some great reasons to get you started: It’s much easier to use functions in control logic (like the for loop above) You’re less likely to make a mistake when you use a function, which is prewritten, than if you are in copying/pasting/editing mode Your code is much easier to reuse if you make functions There is a lot of R functionality (apply(), map()) that works with functions, not copy/paste 1.4 Metacognition: Learning to Learn in R Above, I mentioned “help files”. How do we get help when we (inevitably) run into problems in R? There are a couple steps we will do a lot in this class: Look up the help file for whatever you’re doing. Do this by using the syntax ?&lt;search item&gt; (for example ?c gets help on the vector command) as a shortcut on the console. Search the help files for a term you think is related. Can’t remember the command for making a sequence of integers? Go to the “Help” pane in RStudio and search in the search box for “sequence”. See if some of the top results get you what you need. The internet. Seriously. I am not kidding even a little bit. R has one of the most active and (surprisingly) helpful user communities I’ve ever encountered. Try going to google and searching for “How do I make a sequence of numbers in R?” You will find quite a bit of useful help. I find the following sites particularly helpful: Stack Overflow Cross Validated/Stack Exchange Seriously, Google will get you most of the way to helpful answers for many basic R questions. We will come back to this, but I want to emphasize that looking up help is normal. I do it all the time. Learning to ask questions in helpful ways, how to quickly parse the information you find, and how to slightly alter the answers to suit your particular situation are a key skill from this class I want you to take away. "],["extending-r-with-packages-and-r-markdown.html", "2 Extending R with packages and R Markdown 2.1 All about R Markdown 2.2 Extending R - Packages 2.3 Complex R Objects", " 2 Extending R with packages and R Markdown Welcome to week 2 of the semester. This week we’re going to be focusing on a couple of topics. First, we’re going to go into R Markdown a little bit more–you’ve already encountered R Markdown, of course, in the lecture notes and assignments, but I want to give you some more experience with what it can do. Then we’re going to talk about extending R–in particular how to install, use, and uninstall packages. We’re also going to review how to get help and learn more about R. Finally, we’ll spend some time getting into the guts of R with more advanced types of R objects: the matrices, data.frames, and lists that be a key part of what will make R actually do things efficiently for us as a data-analysis tool. 2.1 All about R Markdown First, we’re going to talk about these crazy documents you’ve gotten used to looking at in the first couple days of class. We’ve already introduced the idea of R Markdown documents in this class, and it’s pretty easy to make a new one that will act ok without messing around with default settings. But what are they actually, and why are we using them instead of text files (.txt), word documents (.doc/.docx), or even nice, quiet PDFs (actually called by the extension, .pdf, but stands for “Portable Document File”). R Markdown, to quote the project page, helps you create dynamic analysis documents that combine code, rendered output (such as figures), and prose. You bring your data, code, and ideas, and R Markdown renders your content into a polished document that can be used to: Do data science interactively within the RStudio IDE, Reproduce your analyses, Collaborate and share code with others, and Communicate your results with others. R Markdown documents can be rendered to many output formats including HTML documents, PDFs, Word files, slideshows, and more, allowing you to focus on the content while R Markdown takes care of your presentation. The idea for R Markdown comes from the more general idea of (Markdown)[https://daringfireball.net/projects/markdown/], which was introduced way back in the prehistoric year of 2004. To paraphrase the original idea, Markdown is a way to write in plain text (not using a word-processor like Microsoft Word) that can then be rendered into formatted HTML using a programming language (in this case Perl, which is a character-oriented scripting language). The name “Markdown” is a reference to that HTML-relationship. HTML stands for “HyperText Markup Language”–so Markdown is the simple way to write formatted documents in HTML (and, as we’ll see, way way more). The key reason for Markdown, according to the authors, is to make easy-(-ish, in my opinion)-to-write documents that are readable as plaintext, but that can be rendered into pretty HTML documents. Why am I telling you these details? In general, I think that the problem with coding and programming languages (and their cousins and relatives, like R Markdown) is that they are often presented as “easy to use” things without any context, and the context itself dictates why they behave in certain ways. So it matters that R Markdown has this close relationship to HTML–it is why we will see later that it outputs by default as HTML files, and also why (we won’t cover this as much) we can use HTML and CSS (Cascading Style Sheets) to provide granular formatting and rendering to our documents. History and context matter! 2.1.1 What is an R script? But to really understand R Markdown, we should contrast it to an R Script. Everyone should go ahead and open an R Script now, by going to File &gt; New File &gt; R Script or by pressing cmd + shift + N (cmd = ctrl on Windows). MacOS menu for creating a new R script. One of the key differences in an R Script is that there is no such thing as narrative plain text. An R Script is a plain-text (unformatted) file, which is meant to have every character literally interpreted by the R interpreter. Every thing you write in an R Script is an instruction to R. So, to check this out, in your R script, try typing “This line cannot be executed”. Then highlight it, and select the “Run” button in the right-hand side of the script pane. What happens? For reference, the way to insert some plain text into your R Scripts is to use comments. Every programming language has a special character that tells the interpreter “ignore this line”. In R it is the “#”, or hash(tag) symbol. One or more hashes will tell R to ignore everything after them. # This whole line will be ignored # print(&quot;hello world&quot;) # This line will also be completely ignored print(&quot;goodbye world&quot;) # Only text AFTER the hashtag is ignored. ## [1] &quot;goodbye world&quot; Commenting your code is a really good idea, because when you come back in 6 months you’ll be able to understand what the heck you were doing! Meanwhile, in this R Markdown file, we can write whatever we want! R Markdown, by default, assumes that text is just text. It is not interpreted by default. Rather, the rendering engine for R Markdown will look for key characters that indicate that something is code (as well as other key characters for formatting that we’ll learn about today) and treat those specific parts of the document as code, but otherwise just let us write narrative text. This allows us to write nice-looking reports and documents in R Markdown that still can run code–so these are perfect for writing lab reports (hint hint) as well as for doing “reproducible” research. Many of the websites and interactive textbooks I have linked to so far are written in R Markdown. But R Markdown is useful because it goes beyond HTML to outputting into many of the more “typical” document formats that we use professionally all the time. R Markdown uses something called pandoc to easily convert your easy-to-write text .Rmd file into Word documents, PDFs, even slides. We’ll play with some of these options today as demos. Why, then, do we not just use R Markdown for everything we do in R? Why do R scripts exist at all? The answer, which will only become more apparent as you get more comfortable using R and doing coding, is that a plain script has “low overhead”–we don’t have to do code chunks and mess with a bunch of formatting to make our code work. We can just open it up, write a few lines, and hit “Run”. So we like R Scripts when we are doing real data exploration or analysis. But once we have or workflow down and our problem solved, it’s a good idea to turn back to R Markdown to make a shareable, reproducible, understandable version of our code. 2.1.2 How do we make an R Markdown document? The simplest way to make an R Markdown document is a lot like making a new R Script: File &gt; New File &gt; R Markdown.... This will bring up a menu with some options to select from, which should look very much like this on your computer: Menu with options for a new Markdown document. Notice that you immediately have a lot of options. This can be a little scary, but you can change all of the options afterwards by editing the YAML Header (which we go over below). So don’t feel like you have to be too careful when you’re doing this. The advantage of using this interactive menu is that it will autopopulate the document header (the YAML header) for you, so you don’t have to memorize options. It will also autopopulate some examples of R Markdown syntax for you into the document. This is nice when you’re just learning R Markdown, but it quickly gets annoying. Therefore, there are a couple other ways to get a blank R Markdown document. The first and easiest to remember is to select “Create Empty Document” on that same menu. But another way that I like is to create a new R Script and then to change the type of document in RStudio using the little menu at the bottom right of your Source pane. What this is doing is telling RStudio how to treat a particular document. How to select different document types in RStudio. You’ll notice there are a lot of options in here, but we’re going to ignore them for now. 2.1.3 Elements of R Markdown documents Like any new tool, R Markdown offers a huge selection of options. This can be really hard for the new user, and so I recommend getting to grips with some basic tools and then learning the rest as you go. Another moment of honesty: this is how I learned (and continue to learn, in fact) R Markdown. I am not at all a master of this technology–in this class I am certain that we will encounter situations together in which my response will be “Huh! Let’s figure out why it did that.” We’re going to first talk about basic text formatting, which is super easy and fun in R Markdown. Then we’ll cover how to include code in your document. Then we’ll talk about this mysterious “YAML Header”, which defines how the document will be rendered into HTML, Word, etc. Then we’ll talk a little bit about inserting external stuff into the document (like links or images). 2.1.3.1 Text body One of the nicest things about R Markdown is how easy it is to do basic text formatting. There are really only a few basic formatting “tags” that make text render nicely when knit, and you might even know the basics from some online tools, which increasingly use Markdown conventions for easy formatting. 2.1.3.1.1 Emphasis Italics and Bold text are the most basic things to do in Markdown. To make italics, surround the text you want to make into italics with either stars (“*”) or underscores (“_”) on both sides. So, for example, “_italic_” = italic when rendered. To make something bold, use two of the same symbol, either stars or underscores (e.g., “__”, “**”). 2.1.3.1.2 Inline code Similarly, to make something render as code (with text highlighting and monospaced characters), use the backtick “`” symbol to surround the text. So “`print()`” = print(). To make code actually run, you have to tell R Markdown that the code in the the backticks is actually R code, by following the opening (first) backtick with r. Then when the R Markdown is rendered, it will actually process the command: sqrt(27234) = 165.0272705. 2.1.3.1.3 Lists To make a list in Markdown, you need a newline (enter/return), followed by exactly how you’d expect a list to look: numbers or bullets start each next line, followed by a space and then the list contents. So, for example: We can use Bullets Or even We can use dashes Mixed with Bullets Numbered lists just need a format like &lt;number&gt;. followed by a space. Note that you don’t have to increment the numbers yourself. You can just have 1. on the start of each line, and R Markdown will do the rest for you. This Is A Numbered list To indent a list item, you only need to provide at least 1 tab character (= 4 spaces) before the bullet or number. This is an item Followed by a subitem and another subitem And now we’re back! 2.1.3.1.4 Headers To make headers (like those you use in Word), you start a line with a “#” character. You use 1 for a Level 1 (biggest) header, 2 for a Level 2 header, and so on. Note that if you have a numbered Table of Contents (as in this document), R Markdown will automatically number it based on these headers. 2.1.3.1.4.1 This is an example Level 5 header, but it doesn’t have any content 2.1.3.1.5 Escape character: \\ If you look carefully at the R Markdown for this document (it will not be visible in the HTML or PDF render), you’ll notice that when I give examples of the special characters in R Markdown (like “_”) it is preceded with a “\\” in the raw R Markdown. The backslash character is a common programming language “escape character”. It tells the interpreter that is reading the text to ignore the immediately following character, which is mostly necessary when that next character does something programmatic in the language. For example, if we didn’t write “\\_” then R Markdown would try to make the underscore into an italics mark, rather than printing it as a visible character. By escaping it using the backslash “\\”, we tell it to treat it instead as the plain character version. This isn’t super critical to memorize, but when you see this character in raw code you can understand what it’s doing there. 2.1.3.2 Code chunks OK, so you’ve gotten some nice formatting into your document, but so far we haven’t done anything that you couldn’t do as easily with Word. What’s the point of this? Well, the big selling point of R Markdown is that it integrates with your interactively processed code. So you don’t copy paste results out of R into your report–you just include the code in the right place, and the ANOVA table, plot, or what have you will pop into place. And if you change your data or need to rerun the analysis, you change only that section, and everything else will update. Code chunks are the heart of this capability. We are only going to cover the basics right now, and we will add functionality as we go on. The R Markdown documentation linked in your reading for this week gives a lot more detail. At its most basic, a code chunk in Markdown is a set of lines surrounded by triple backticks: ```. So this is a code chunk print(&quot;code goes here&quot;) But the above code chunk is a generic, multiline code chunk. It tells R Markdown to render the text within as monospace, but it doesn’t actually execute the code (or give you the option to do so). In order to make this into actual, runnable R code, we have to tell R Markdown that this is an R code chunk, using the code chunk header brackets after the first triple backticks: ```{r} This is the equivalent of telling R that the inline code you have typed is executable, as we learned above. print(&quot;This is a code chunk that will execute&quot;) ## [1] &quot;This is a code chunk that will execute&quot; Typing out triple backticks, then “{r}”, then, another set of triple backticks is a real pain in the butt. Happily, RStudio has a keyboard shortcut for that: cmd + alt + I (use ctrl instead of cmd on Windows). The chunk header brackets allow you to also set up options for that chunk. Options are set by naming them, and setting them to a value: for example, echo=FALSE. There are many, and I will not be covering them comprehensively but introducing them as I need them. However, a couple key ones are useful, especially those options that dictate whether, for a particular chunk, the code runs, or whether the code gives output, errors, or messages. Here are some examples, but I like the detail given in R for Data Science’s section on this. Sometimes you don’t want your chunk to run at all–in that case you’d tell it to not evaluate, using the eval=FALSE option: for(i in 1:1000) sqrt(factorial(i)) # This is a pointlessly long calculation for demonstration purposes only On the other hand, sometimes you want a code chunk to execute, but you don’t want to show the code itself. This might happen because you want to just store some results in memory, or more frequently because you want to show a table or plot made by the code. In that case, you can set the echo=FALSE option. In these R Markdown documents I make liberal use of another option: error=TRUE. This tells R to keep knitting the document, even if that chunk returns an error: This is nonsense ## Error: &lt;text&gt;:1:6: unexpected symbol ## 1: This is ## ^ What do you think would happen if I didn’t use the error=TRUE option? Try editing your version of this document to find out. 2.1.3.2.1 Setup chunk A particularly useful set of options is found in the default setup chunk that RStudio will make you if you make a new R Markdown document. It demonstrates two useful attributes: a name (setup) and the include=FALSE option. In fact, you won’t be able to see the chunk in this rendered output, but open the raw .Rmd file and it will be right below this, with the header written as follows: {r setup, include = FALSE}. Working backwards, the include = FALSE option tells R to run the chunk, but to not give any output and not to print the chunk in the rendered document. So it is useful for setup: for any packages you need to load, datasets you need to load, etc. Stuff the rest of your document will need. Chunks can be named by following the r in the header with a space and the name. Any characters can go in the name, although generally we don’t include spaces because they can cause errors in other places if you try to use advanced methods to refer back to the chunks. You can name chunks anything: doing so is useful because if you give them descriptive names, you know what they’re supposed to do when you come back to the document later. 2.1.3.3 YAML Header At the top of this R Markdown file is a funny bit of syntax that looks more like code than it does like plain text, but isn’t R Syntax. Here it is for reference (in an unsigned code chunk!): --- title: &quot;2 - Packages and R Markdown&quot; author: &quot;JL&quot; date: &quot;10/11/2021&quot; output: html_document: toc: true number_sections: true toc_float: true --- This is called the YAML Header—“YAML” stands for “Yet Another Markup Language”, although recently it has been renamed as “YAML Ain’t Markup Language”–both are stupid programming jokes. More importantly, this header is a compact way of defining how the R Markdown document will render. I am not at all an expert on YAML, so I will use this section to give a few pointers and some tips on how to make this bit of code work for you. YAML is very unlike R syntax in several key ways (technically it is more like Python, but that is probably not meaningful to most of us). It is a set of key/value pairs, which are defined as &lt;key&gt;: &lt;value&gt;. What this means is that, in our example above, the value of title: is set to \"2 - Packages and R Markdown. This is different from how we’d set a variable called title in R–can you remember how to do this?. You’ll notice that the first few lines of the YAML header directly translate to the output of this R Markdown document—it has “JL” as the author, the title specified, etc. But the more mysterious section is the output: section. There are two things to observe here. First, notice the way that this is structured by whitespaces: sure, it looks like the output type is html_document, but that is also followed by a colon (:), and then several further keys that are set. Those following “keys” are indented under the html_document: key, showing us that they are part of that general key-value pair set. What this tells us is that these are variables specific to the html_document output type. In fact, they are numbering the section headers for us automatically, creating a table of contents (toc: true) and making that table of contents float on the side and update nicely. If we wanted to output this R Markdown as a PDF we’d add a pdf_document value to the output: section. If we wanted to make this into slides (yes, that’s possible!), we’d add the correct call (e.g., output: ioslides_presentation). But these are all specific use cases, and as I said, I am not an expert. I find the YAML section in this crash course super helpful to get started. The key things to remember about YAML: It uses syntax in which whitespace (spaces and new lines/returns) are meaningful, so be careful Note the different capitalization Options for different outputs in R Markdown are specific to the type of output, so you will have to read the help files/search the internet 2.1.3.4 Links, images, etc One of the wonderful things about (R) Markdown is how easy it is to drop links and images into the document. The syntax for web links is super simple. To make a link you pick some text you want to make into a link, surround it by square braces([]), and then follow those braces immediately with a set of parentheses (()) containing the link. So, for example, this link goes to a picture of a Siamese cat, and is written like this: [this link](https://en.wikipedia.org/wiki/Siamese_cat#/media/File:Siam_lilacpoint.jpg). Super easy. Even nicer, adding images works very similarly. To insert an image, you use an exclamation point followed by square braces (![]), followed by parentheses again (()), but instead of a hyperlink those parentheses should include the file path to your image. So, for example, here is a picture of my cat, Guybrush Meepwood: The cutest cat. And it’s written like this: ![The cutest cat.](Images/Week 2/Little guy.JPG). Anything you put in the brackets becomes the caption for the picture. NB: in order to render the caption correctly, make sure to remember to add a return (newline) after the image link. You can also use the image linking format to link to web images, so we can add that picture of a random Siamese cat from the static hyperlink we used for a text link above: A less cute cat. 2.1.3.5 Tables Tables are also sort of easy to make in R Markdown… if you’re outputting them from R. So, say you have a data table, like the famous mtcars dataset, included in base R. The simple kable() function, found in the knitr package (which is actually what is making our R Markdown files, see below) renders tabular data into a nice format by default. knitr::kable(mtcars[1:10, ]) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 This is a huge area that I am also not super expert in, but you could be the next data-table wizard. Maybe start here? 2.1.4 “Knitting” R Markdown documents The jargon for transforming a raw text file (like this one) into a rendered document through RStudio is called “knitting”. This is why the knitr package mentioned above is named as such. The easiest way to do so is to click the button labeled “Knit” at the top of your Source editor in RStudio: When that button is clicked, it will knit to the type of output you specified in the YAML header (remember that?)–but you can also use the little triangle button next to the “Knit” to select other output options. Note that if you knit to a different option than already is specified in your header, RStudio will add an output key/value to your YAML header, and so next time you knit it will compile documents in all formats you have ever specified. You can of course go back and edit your YAML header to fix this if you didn’t want that to happen. 2.1.4.1 File output of knitting Knitting creates new files of the type you have specified. By default, these will live in the same directory as the current Markdown file. So when you knit, you will end up with a file with the same name but the new file extension you specified (e.g., if output: html_document is the option you set, you will end up with a file called &lt;name of markdown file&gt;.html in the same directory). You can change the output directory if you like to keep things clean. Generally, HTML documents are the fastest to knit, and PDF and Word documents (and other formats) will require more processing. Therefore, I tend to use HTML as the default output. 2.1.4.2 Previewing (in RStudio version &gt; 1.4) You can also check to make sure your R Markdown is going to render properly by using the RStudio Visual Markdown Editor, which turns R Markdown into something closer to a Word Processor. To access this mode, click the little compass icon at the top of the source editor: Click this button to access the RStudio Visual Markdown Editor Important: this does not knit your document—it gives you a preview of what it will look like, but it does not create a document that is rendered and ready for sharing. Also important: this will give you a warning the first time you use it noting that it will transform your Markdown to properly formatted syntax. This may cause some errors. For example, I have seen the following: Changing unsigned code blocks (e.g., those marked by ``` without the “{r}”) to completely unindented. This would screw up, for example, the YAML example above. Changing spaces in file names (e.g., .../Files/Week 2/) to %20, which I believe is a version of the HTML unicode. This is undesirable behavior and I had to fix it. This may feel like a great way to check your work, but I don’t personally like using this editor to write R Markdown–I find it obscures important details of what’s going on in the base file. I also got sick of fixing the changes it made to my code. 2.1.4.3 Outputting to more exotic formats (like slides, etc) As mentioned above, R Markdown can output to all kinds of formats. All you have to do is add the appropriate value/key pair to the YAML header. So, for example, if we add output: ioslides_presentation to the top of this document, we will get an automatically formatted ioslides presentation (an HTML slide deck). R Markdown will make some automatic decisions on how to make slides (based on the headers we’ve indicated with various # marks). It won’t look good as it is (I have way too much text), but this shows how easily we can flip between different formats. 2.1.5 Joplin - an easy way to learn basic Markdown We are coming to the end of our crash course in R Markdown. Learning R Markdown will be a great way for you to develop a skill that will be super useful in many future parts of your career. But it will take practice. To be completely honest, I have tried to learn R Markdown several times, and it wasn’t until this class that I really started to learn how to use it effectively. This is a common phenomenon: having to use something is the best way to actually learn it. This is why at Virginia Tech we are trying to emphasize experiential learning. In preparation for this class, I started using a virtual notebook/labbook that is based on Markdown (not R Markdown, but as we discused they are closely related “flavors”), called Joplin. It has a couple of other fantastic features that I love (it syncs across devices, lets you easily organize notes, create checklists, etc) that are cool, but I mainly like it because it’s been a great way to practice Markdown. Maybe it will be something useful for you? 2.2 Extending R - Packages In one of the code chunks above, I wrote something we haven’t seen before: knitr::kable(mtcars). The latter half of this looks familiar–it’s just a function that we are using on mtcars. What’s with the ::? It indicates that the function kable() is part of the knitr package–and we are using the :: syntax to access the function without loading the package into memory (more on this in a minute). You can see all the packages that are currently installed in your version of R by looking at the “Packages” Pane in RStudio, and you can see what packages you’ve loaded in this session by running sessionInfo(). sessionInfo() ## R version 4.2.1 (2022-06-23) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.27 digest_0.6.29 R6_2.5.1 jsonlite_1.8.0 ## [5] magrittr_2.0.3 evaluate_0.15 highr_0.9 stringi_1.7.8 ## [9] cachem_1.0.6 rlang_1.0.4 cli_3.3.0 rstudioapi_0.13 ## [13] jquerylib_0.1.4 bslib_0.4.0 rmarkdown_2.14 tools_4.2.1 ## [17] stringr_1.4.0 xfun_0.31 yaml_2.3.5 fastmap_1.1.0 ## [21] compiler_4.2.1 htmltools_0.5.3 knitr_1.39 sass_0.4.2 In this list, the “attached” lists are the loaded packages. The “namespace” list is a programming convention you can safely ignore for now. 2.2.1 What are packages? In R, packages are sets of functions and data that offer new functionality. Because R is open source, anyone can create new R packages. That sounds great in practice, but what’s to guarantee that packages work as advertised (i.e., do the analysis that they are supposed to, or even more fundamentally don’t harm your computer)? Like many open-source projects, R relies on centralized volunteers to check R packages. The CRAN (Comprehensive R Archive Network), which distributes R itself, also checks and distributes user-submitted packages. When you install packages in R Studio, by default you are installing them from CRAN. There are other repositories that focus on different disciplinary expertise–the most significant one I know of is Bioconductor, which focuses on biostatistical and genomic packages. You can also install packages directly, either by downloading them or from user-hosted repositories. Most commonly this would be from something like github, where alpha and beta versions of software are often hosted by developers prior to CRAN submission. Usually this requires using a different installer function (e.g., devtools::install_github()), and we won’t cover that explicitly here. 2.2.2 Installing packages There are two main ways to install packages. The “point-and-click” way is to use the Tools &gt; Install Packages… command in RStudio: Menu command for installing packages The advantage to this approach is that RStudio will help you autofill package names, and it will also let you install multiple packages by just putting commas between them. The disadvantage is that you don’t learn what is actually going on. The menu command is just a shortcut for the base R command: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) # The name of the package goes in quotes here Note that this code chunk has eval=FALSE set, because it’s rude to install packages on someone else’s computer without their permission. **In your console, go ahead and install the tidyverse package. It will install a set of packages that are “dependencies”. We will be using these later in the course. The menu command is fine to use, but it’s good to know how to do this on your own. It helps you understand when something doesn’t work correctly. 2.2.3 Using packages Technically, once a package is installed in your version of R through the above commands, it is available to use. So if you want to use kable(), you can always type knitr::kable(). But that’s pretty tedious! It is much better to attach (load) packages when you need them, and then you can call their functions (and datasets) directly. To do this, all you need to do is use the library(&lt;unquoted name of package&gt;) command. So, for example, let’s try using kable() without loading knitr, then see what happens once we load it properly. kable(mtcars) ## Error in kable(mtcars): could not find function &quot;kable&quot; library(knitr) # load the knitr package, note the lack of quotes around &lt;knitr&gt; kable(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Note that you can also load packages using point-and-click, by going to the Packages Pane in RStudio and clicking on the white box next to a package name. This is fine for working interactively, but obviously won’t work if you want to send someone an R Script that needs to load some packages. It will also be tedious if you are doing the same workflow multiple times. Better to learn how to use the library() function! 2.2.4 Detaching (unloading) packages Soemtimes you want to detach (unload) packages. The technically correct way to do so is to call detach(package:&lt;package name&gt;), which is slightly more tedious to type. You can also detach packages by unchecking the box next to their name in the RStudio Packages Pane. 2.2.5 Uninstalling packages And sometimes you want to completely remove a package from R. The most frequent reason for wanting to do this is when you start getting persistent errors. This can happen because you have an old version of a package, or it installed improperly (this will often be accompanied by install error messages in the console). You might also want to reinstall a package and be certain that the old version is gone. Generally you dont’ need to manually uninstall a package, but the simple command is just the opposite of installation: remove.packages(\"&lt;package name&gt;\"). You can also use the Package Pane to remove packages by clicking the “X” to the far right of the package name. Since removing packages isn’t something you do all that often, using the point-and-click shortcut is no problem. 2.2.6 Getting help - a deeper drive With more packages you’re going to more frequently run into the need to look up how to do things, which means dealing with help files. As a reminder, typing ?&lt;search term&gt; will make the help documentation for whatever you’ve searched for appear. But what if you don’t know what to search for? By typing ??&lt;search term&gt; you will search all help files for the search term. R will return a list of matching articles to you in the help pane. This is considerably slower, since it’s searching hundreds or thousands of text files. Try typing ??install into your console to see how this works. You will notice that there are two types of results in the help list for install. The help pages should be familiar. But what are “vignettes”? Try clicking on one to find out. Vignettes are formatted, conversational walkthroughs that are increasingly common (and helpful!) for R packages. Rather than explaining a single function they usually explain some aspect of a package, and how to use it. And, even better for our purposes, they are written in R Markdown. Click the “source” link next to the vignette name in order to see how the author wrote it in R Markdown. This is a great way to learn new tricks. While you can find vignettes as we just did, a better way is to use the function browseVignettes(). This opens a web browser window that lists all vignettes installed on your computer. You can then use cmd/ctrl + F to search using terms in the web browser and quickly find package names, function names, or topics you are looking for. Your first major assignment in the class will be to write your own vignette for a task or functionality. We will talk about this more in the next week. 2.3 Complex R Objects To summarize where we ended up last week: we went over data types for variables. Who can list the basic data types we reviewed? We also went over the basics of functions, which we’ll come back to pretty much every week. So far this week we’ve spent a lot of time on “meta” topics–tools for interacting with R and with our coding and data. We haven’t actually learned that much more about how R works than we ended up knowing last week. Let’s change that! 2.3.1 Vectors: a review Last week we mentioned vectors very briefly. But vectors are really at the heart of what makes R useful. A vector is an ordered list of the same kind of variable. Let’s take a look at two different vectors to make this idea concrete. a &lt;- c(1, 1, 2, 1, 100) b &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) a ## [1] 1 1 2 1 100 b ## [1] TRUE FALSE TRUE TRUE FALSE The c() function is R’s shortcut for making vectors, as shown above. In strict programming terms, both a and b are atomic vectors because all of their elements are of the same type (what kind of vector is a? what kind of vector is b?). If you don’t know what kind of vector a given vector is, you can use the typeof() function to ask R. typeof(a) ## [1] &quot;double&quot; typeof(b) ## [1] &quot;logical&quot; typeof(a) == typeof(b) ## [1] FALSE Another property vectors have is length. Length is just how many elements are in the list. For these vectors it’s easy to see the length, but we will often deal with long vectors we don’t want to count by hand. We can query the length of any vector (and even of complex objects) using the length() command. length(a) ## [1] 5 length(b) ## [1] 5 length(a) == length(b) ## [1] TRUE Interestingly, in R most things are vectors. So R, behind the scenes, thinks that 100 is a numeric vector of length = 1: length(100) ## [1] 1 typeof(100) ## [1] &quot;double&quot; We can add elements to pre-existing vectors, or even combine two pre-existing vectors, using the same c() function. c(a, 101) ## [1] 1 1 2 1 100 101 c(b, FALSE, FALSE, FALSE) ## [1] TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE If we try to put things that are of different type in the same vector, R will either try to “coerce” them into being the same type (remember R will change TRUE to 1 if we try to add it, for example) or give us an error. The fact that, usually, R will not give an error can sometimes be a problem, because it might change things to characters or add up logical vectors when you don’t want it to! c(a, b) ## [1] 1 1 2 1 100 1 0 1 1 0 c(a, &quot;ring&quot;) ## [1] &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;1&quot; &quot;100&quot; &quot;ring&quot; 2.3.2 Vector math R will happily do math with vectors for you, which will save a lot of time. But there are some pitfalls to think about. I’m going to just give the basic intro to this topic, and I recommend Hadley Wickham’s summary of the topic to give you an idea of some of the possible problems. 2.3.2.1 Piecewise (scalar) math A scalar is a 1-item vector: 1 is a scalar (as is \"dog\", technically, but we can’t do normal math with it). The basic arithmetic we’re used to is scalar mathematics. But vector math is a more powerful, compact form, and is useful for data analysis, as we’ll see. When we take a scalar and do basic mathematical operations in R, it acts piece-wise: the operation is applied to each object in the vector. a + 10 ## [1] 11 11 12 11 110 a * 10 ## [1] 10 10 20 10 1000 a - 1 ## [1] 0 0 1 0 99 a / 10 ## [1] 0.1 0.1 0.2 0.1 10.0 The reverse also applies, with an asterisk: you will always end up with a vector, never with a scalar. 10 / a ## [1] 10.0 10.0 5.0 10.0 0.1 10 * a ## [1] 10 10 20 10 1000 2.3.2.2 Vector math This is not a linear algebra class, so I will not be going into the rules of vector (and matrix) algebra. But you should know that all operations are not equivalent. Here is an example to give you fair warning. x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) x * y # scalar (pointwise) multiplication = c(1 * 4, 2 * 5, 3 * 6) ## [1] 4 10 18 x %*% y # matrix multiplication with two column vectors = 1 * 4 + 2 * 5 + 3 * 6 ## [,1] ## [1,] 32 x %*% t(y) # matrix multiplication with a column and a row vector ## [,1] [,2] [,3] ## [1,] 4 5 6 ## [2,] 8 10 12 ## [3,] 12 15 18 2.3.3 “Indexing”: getting elements from a vector This is a good place to (re)introduce the key programming concept of indexing. Ordered lists like vectors (and actual lists, as well as matrices, arrays, and data frames below) are indexed. For a vector, which is an ordered list, the index is the number of an entry. So, if we look back at our a vector, we can pull out each entry by its corresponding index. a ## [1] 1 1 2 1 100 a[1] ## [1] 1 a[2] ## [1] 1 a[3] ## [1] 2 a[4] ## [1] 1 a[5] ## [1] 100 As you might guess, the R operator to extract an item from a vector is []. If you write a[2], you can read this as the second item in a. The [] operators have even more functionality. We can use a vector of indices (!!!) to get just those objects from our first vector. index_vector &lt;- c(1, 3, 5) # say we want the first, third, and fifth items a[index_vector] ## [1] 1 2 100 We can also use negative indices to drop objects from our vector. a[-2] # drop the second object from a ## [1] 1 2 1 100 a[c(-1, -5)] # drop the first and fifth objects from a ## [1] 1 2 1 Note that you can’t combine positive and negative indices for subsetting in one step. a[c(1, -5)] ## Error in a[c(1, -5)]: only 0&#39;s may be mixed with negative subscripts We can also use a vector of the same length but of type logical to select or drop specific entries. c_logical &lt;- c(TRUE, TRUE, FALSE, FALSE, TRUE) a[c_logical] ## [1] 1 1 100 This seems a little bit useless, but in fact it will be super helpful when we start using logical operators in coding. Here is an example: if we don’t know what numbers are in a, but we know they should be small, we might want to drop any numbers that are “too large” a &lt; 10 # R has all the classic comparison operators, &quot;&lt;&quot; means &quot;less than&quot; ## [1] TRUE TRUE TRUE TRUE FALSE a[a &lt; 10] # give us everything in a &lt; 10 ## [1] 1 1 2 1 Important but advanced: in programming terms, R is 1-indexed, meaning that a[1] gives the first object in a. This is not the case in many programming languages, including Python, which are 0-indexed, meaning that you’d write a[0] to get the first item in a. This is important to know if you continue on in other programming languages for research. 2.3.4 Multidimensional vectors: Matrices and Arrays Most of you have encountered a matrix before. Matrices are 2-dimensional arrays (usually of numbers). The main reason we care about matrices is that they are efficient ways of organizing and analyzing numerical results. In R, matrices are simply vectors (long lists of numbers) with a set number of rows and columns. Because they are really just augmented vectors, matrices (and arrays) are atomic: they can only have one type of data in them. c &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2) c ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 dim(c) ## [1] 2 3 str(c) ## num [1:2, 1:3] 1 2 3 4 5 6 The dim() function means “dimensions”, and tells you what the dimensions of an object are. So here dim(c) tells us that c has 2 dimensions: 2 rows and 3 columns. If you call str() on a matrix, you get both an idea of what’s in it as well as the dimensions. If you are used to matrix algebra, one of the weird things that R does is that it, by default, fills matrices column-wise. In the example above, c is filled from top to bottom, left to right. This is contrary to most matrix math. It can be overridden by specifying matrix(byrow = TRUE), but in practice it doesn’t matter as long as you’re aware of it. The main reason to bring up matrices is to talk about indexing. Now, rather than saying “give us the fourth item in a”, we might want to say “give us the first item in the second row of c”. As you might guess, we will use the [] operator. The only difference is that we will provide an index for each dimension. To index in this way, you separate each dimension you are wanting to index by ,. If you leave a dimension blank, you will get everything back in that dimension. c[1, 2] # get the first row, second column entry ## [1] 3 c[1, ] # get the first row ## [1] 1 3 5 c[, 2] # get the second column ## [1] 3 4 You can also use logical vectors for subsetting, as with vectors. Arrays are vectors with more than 2 dimensions. So a 3-dimensional array can be thought of as a vector of matrices. These are also called tensors in the machine learning community. They are important for programming and for a number of advanced data analyses, but we will not be working with them as much initially. d &lt;- array(c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) dim(d) # d is a &quot;cube&quot; - 2 matrices, each 2 x 2 ## [1] 2 2 2 d # when you ask R to print an array, it will print it as matrix slices ## , , 1 ## ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 ## ## , , 2 ## ## [,1] [,2] ## [1,] 5 7 ## [2,] 6 8 str(d) # as with most R objects, str() probably gives the most concise, useful information ## num [1:2, 1:2, 1:2] 1 2 3 4 5 6 7 8 We index arrays the same way we would matrices, just adding an extra , for each additional dimension. But it gets harder to keep track of, and keeping good notes and experimenting to make sure you’re doing what you think you’re doing is key. d[1, 2, 1] # get the number in the first row of the second column in the first matrix ## [1] 3 d[, , 1] # get the first matrix &quot;slice&quot; ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 d[1, 1, ] # get the number at the first row and first column for both matrices ## [1] 1 5 2.3.5 Lists: non-atomic vectors If you want to keep different things in one vector, R provides a non-atomic vector type called a list, which is made using the list() function. list(TRUE, &quot;dog&quot;, 3L, 3.45) ## [[1]] ## [1] TRUE ## ## [[2]] ## [1] &quot;dog&quot; ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 3.45 list(1, 2, 3, 4) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 Lists do not coerce data of different type, because they store each vector in its own, distinct slot. If we remember that R treats individual data as 1-length vectors (e.g., \"dog\" is a 1-length character vector), we can understand that lists are “vectors of vectors”. Each item in the list is a vector. list(c(1, 2, 3, 4), &quot;dog&quot;, c(TRUE, FALSE)) ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;dog&quot; ## ## [[3]] ## [1] TRUE FALSE We can even make a list of lists list(list(1, 2, 3), list(c(&quot;dog&quot;, &quot;cat&quot;)), TRUE) ## [[1]] ## [[1]][[1]] ## [1] 1 ## ## [[1]][[2]] ## [1] 2 ## ## [[1]][[3]] ## [1] 3 ## ## ## [[2]] ## [[2]][[1]] ## [1] &quot;dog&quot; &quot;cat&quot; ## ## ## [[3]] ## [1] TRUE This makes lists rather difficult to conceptualize, but very easy and flexible to store items in. 2.3.5.1 The str() function Because the way that R prints lists by default is hard to understand, an important function for dealing with them is str(), which you can remember as meaning structure. If we look at the last (slightly pathological) list I created, l &lt;- list(list(1, 2, 3), list(c(&quot;dog&quot;, &quot;cat&quot;)), TRUE) str(l) ## List of 3 ## $ :List of 3 ## ..$ : num 1 ## ..$ : num 2 ## ..$ : num 3 ## $ :List of 1 ## ..$ : chr [1:2] &quot;dog&quot; &quot;cat&quot; ## $ : logi TRUE This is much clearer! We have a list that contains another list with 3 items, a vector of characters, and a single logical variable. We can also use str() on any R object, and it is usually quite helpful in letting us understand what we’re looking at. 2.3.5.2 Naming and indexing lists We can actually make named atomic lists, but they are not as common. When we start working with lists, because of their structure, it is often useful to name the elements of our list. Let’s make our pathological list a little easier to understand by giving each of its elements a name. Important: Names for lists (and data frames, below) don’t get quoted, even though they are character strings. This is because they are names (a type of R object that is distinct but rarely created explicitly). named_list &lt;- list(numbers = list(1, 2, 3), pets = c(&quot;dog&quot;, &quot;cat&quot;), cats_are_best = TRUE) str(named_list) ## List of 3 ## $ numbers :List of 3 ## ..$ : num 1 ## ..$ : num 2 ## ..$ : num 3 ## $ pets : chr [1:2] &quot;dog&quot; &quot;cat&quot; ## $ cats_are_best: logi TRUE Now we have an idea of what all of the sublists hold. How can we get them back out of our list? Through indexing, of course. The indexing function for lists works a little differently than for atomic vectors and arrays… or rather, it works logically if you look in a certain light, and otherwise is super confusing. Let’s say we want to get our character vector of pets. That’s the second item in our list, right? named_list[2] ## $pets ## [1] &quot;dog&quot; &quot;cat&quot; str(named_list[2]) ## List of 1 ## $ pets: chr [1:2] &quot;dog&quot; &quot;cat&quot; This is still a list! It turns out that [] for lists returns the same general shape of list with a subset of the original items. If we want to “pop out” our original vector, we need to use the [[]] operator, which can be read as “go down one level”. named_list[[2]] ## [1] &quot;dog&quot; &quot;cat&quot; str(named_list[[2]]) ## chr [1:2] &quot;dog&quot; &quot;cat&quot; There we go. Honestly, this behavior is one of the hardest things about lists, and will take trial and error to get used to. Happily, if you have a named list, you can use a syntactical shortcut provided by R to do this more easily. The $ operator is kind of like [[]], except instead of indexing you give it a name. So we can just ask for our vector of pets back: named_list$pets ## [1] &quot;dog&quot; &quot;cat&quot; As you will see, $ gets used a lot with data frames. 2.3.6 data.frame: Tables of data Finally, with that background behind us, we can talk about the special kind of list known as data frames. Data frames are as close as (base) R gets to the familiar, tabular data from Excel. Think of a data frame as a “rectangular list”–it’s a list of columns. Each column has to have the same number of rows. But each column can have its own data type–so you can have a column of names (character), then a column of ages (integer), then a column of heights and a column of weights (numeric). Like so: cats &lt;- data.frame(names = c(&quot;Margot&quot;, &quot;Nick&quot;, &quot;Guybrush&quot;), ages = c(13L, 14L, 2L), heights = c(2, 2, 3), weights = c(10, 9, 14)) cats ## names ages heights weights ## 1 Margot 13 2 10 ## 2 Nick 14 2 9 ## 3 Guybrush 2 3 14 Oh, did you think this was a data frame of people? We’ve actually already seen a data frame: mtcars. class(mtcars) ## [1] &quot;data.frame&quot; typeof(mtcars) ## [1] &quot;list&quot; Data frames are extremely useful for storing data. Because they are rectangular, we can index them as we normally do with matrices: cats[2, 4] # get Nick&#39;s weight ## [1] 9 cats[3, ] # get all of Little Guy&#39;s vital stats ## names ages heights weights ## 3 Guybrush 2 3 14 But we can also use list-type $ indexing to get our named variables cats$names # what are my cats&#39; names? ## [1] &quot;Margot&quot; &quot;Nick&quot; &quot;Guybrush&quot; An augmented class of dataframes that we are going to be learning about extensively in the next classes is the tibble. This is a tidyverse convention for an augmented, well-behaving data frame. We can make a tibble by just calling the tibble::tibble() function on an existing data frame: cat_tbl &lt;- tibble::tibble(cats) class(cat_tbl) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; cat_tbl # but they print nicer ## # A tibble: 3 × 4 ## names ages heights weights ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Margot 13 2 10 ## 2 Nick 14 2 9 ## 3 Guybrush 2 3 14 If you read the section of R for Data Science I have suggested for this class, you will see a lot of reference to tibbles. This section here is mostly to let you know that they are close (mostly improved) cousins to the data frame. "],["control-flow-and-functions.html", "3 Control flow and functions 3.1 Functions 3.2 Control structure - making your code hustle and flow 3.3 Simulating and sampling data in R References", " 3 Control flow and functions The real power of a coding approach to data analysis, like that we’re learning with R, lies in the flexibility and customization that coding offers. Unlike a menu-based (“point-and-click”) approach, in R (or in Python if you want to learn another language), you can: Write custom analysis functions that act exactly like you want Run simulations based on your data Write scripts to do tedious repetitions, like reading data files from a directory and running a set of analyses I like to call this a programmatic approach to data analysis, as in: you are writing a program that will execute itself and do the things you want it to do. This is the real power of a data-science / coding / R approach to our analyses. By taking this programmatic viewpoint, we will be able to move away from having to memorize menu commands and make repetitive selections. The downside, of course, is that we have to learn how R (and, more broadly, computers) think. This is what we’ve been learning. We’ve learned about: Data types Storing variables Writing code and #comments Functional syntax We’ve learned, in particular, that computers will do things that are really hard for us, but that we have to learn to put things into the exact terms that they will understand. I think the first couple weeks of using R feel extremely difficult, and the payoff seems far away. So the goal of class today is to motivate this a bit more. We will be discussing simulation and sampling in particular to talk about simple data-generation tasks that are very difficult to do in Excel or JMP. But these tools in particular are even more powerful when we can integrate them into programmatic approaches, so today we’re going to spend the bulk of our time talking about logic and about functions. These are the building blocks of the programmatic approach, and once you start to learn a bit more about them, the way that R works will become increasingly clear. Then, we will look at some of the cool, simple simulations we can build with these tools. This is where most of us live in coding expertise, from XKCD. 3.1 Functions The first piece of the puzzle that will help us make R useful for research is formalizing the idea of a function. We’ve been using functions since the very first day of class: everything from install.packages() to print() to c() is a function in R. But just because we’ve been using these doesn’t given us any actual idea of what a function is. It turns out that R has a very pragmatic definition of functions and that, in fact, we can even write our own very easily, which will be a difficult but key step in making our coding effective for our specialized research needs. 3.1.1 Introduction - quasi-mathematical definition We all learned about functions (I think probably in precalculus?) with the idea of \\(f(x)\\). The “\\(f\\)” literally is shorthand for “function”. In a mathematical sense, functions take input(s)–like \\(x\\)–and map them to an output–often called \\(y\\) in our basic math classes–by applying some transformation that is defined in \\(f\\). Schematic function diagram from Wikipedia. While in math we spent a lot of time learning about how functions should operate (for example, each x should only map to one y, although the reverse does not need to be true, e.g. \\(f(x) = x^2\\)), in coding we can consider functions more as “black box machines”: we do not need to know exactly how they work internally–most of the time–as long as we understand what inputs (\\(x\\)s) they need, and what outputs (\\(y\\)s) they give us. 3.1.2 Everything we’ve been using in R is a function One of the nice things about R is that (most of) it is written in R. That means that most of the R functions are written with R syntax, and we can learn to understand what they’re doing. The very basic functions are not (they are written in a crazy mix of C++ and old languages like Fortran, I believe), but for the most part we can look at how functions work by just printing them out. Let’s look at the function for calculating the standard deviation of a set of numbers, sd(). sd(c(0, 1, 2, 3, 4, 5)) # this will give the standard deviation of these 6 numbers ## [1] 1.870829 sd # Note that by not including the &#39;()&#39; I am asking for the OBJECT &#39;sd&#39;, not the function sd() ## function (x, na.rm = FALSE) ## sqrt(var(if (is.vector(x) || is.factor(x)) x else as.double(x), ## na.rm = na.rm)) ## &lt;bytecode: 0x7fcc040b62e8&gt; ## &lt;environment: namespace:stats&gt; While that’s not pretty, we can actually learn a lot from it–but don’t worry, I’m going to spare you for now. The main point here is that you can see some elements you’re probably starting to recognize: other R functions, if() and else statements… this is something we can examine. To return to our quasimathematical definition, we also can see that sd() takes some inputs and gives us some outputs. sd(1) ## [1] NA sd(c(1, 2)) ## [1] 0.7071068 sd(c(&quot;dog&quot;, &quot;cat&quot;)) ## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm = ## na.rm): NAs introduced by coercion ## [1] NA sd(c(1, 2, 3, NA)) ## [1] NA sd(c(1, 2, 3, NA), na.rm = TRUE) ## [1] 1 It seems like sd() doesn’t like single numbers (“scalars”, recall), because sd(1) returns NA. We can learn why this is by thinking about the definition of standard deviation. More predictably, it also doesn’t like character vectors. It also doesn’t know how to deal with the special value NA, but we can provide it with an argument na.rm = TRUE that tells it to remove NAs in order to do it’s calculations. So it seems like sd(), as a function, takes (as \\(x\\)) a vector numbers of length &gt; 1, and returns a numeric vector of length = 1. 3.1.3 Back to ?: Reading the function structure This was a lot of work to do to dissect a function so simple you can run it on a scientific calculator. A much better option is to use our old friend ? to investigate how functions work–in particular, their arguments and the values they return. ?sd # run this yourself You’ll notice that the typical help page has sections like “Usage”, “Arguments”, and “Details”. Let’s examine these: Usage - this section gives an example of the function with its arguments. It also displays the default values for the arguments. Arguments - this gives details on how the arguments the function accepts work. In the case of sd(), there are two arguments. x is the data you want the standard deviation of, and na.rm asks whether it should remove NA values. Note that the default for na.rm = FALSE. This means you don’t have to set it explicitly (the function already has a value for it), but if you don’t set it explicitly you will be accepting that default option. Details - this section gives notes about the function and how it works. In the case of some functions, “Details” also explains what values the function returns. Some optional elements on these pages which are helpful if they are present are: Values - this explicitly tells you what you can expect the function to return. If sd() had this, it would note that it returns a numeric vector of length 1 with the calculated standard deviation. Examples - these are executable R code that give examples of how the function works. These can be very helpful for you to copy and paste and run yourself to get an idea of what is happening in more complex functions. 3.1.4 Function arguments The topic of function arguments deserves its own section because of how R tries to be helpful. Let’s look at a function that takes a few arguments that we’ve already seen: rnorm(). Try running ?rnorm to get an idea of how it works. Note that the help page brings up a set of related functions as well. rnorm() has 3 arguments - n, the number of samples you want, mean, the mean of the normal distribution you’re sampling from, and sd, the standard deviation of the normal distribution you’re sampling from. You’ll also notice that both mean and sd have default values, so if you don’t specify them it will give you a draw from a normal distribution with mean = 0 and sd = 1. So we can run it as follows: set.seed(123) # for reproducibility rnorm(n = 5, mean = 5, sd = 3) ## [1] 3.318573 4.309468 9.676125 5.211525 5.387863 set.seed(123) # let&#39;s do that again rnorm(5, 5, 3) ## [1] 3.318573 4.309468 9.676125 5.211525 5.387863 What just happened? How does R know that the first 5 in rnorm(5, 5, 3) is the number of samples, and the second is the mean? R uses something called “positional argument matching”, in which it will take arguments to a function and just match them up in the order the function is written. This is generally useful for writing quick code, but I encourage you to get in the habit of explicitly naming arguments. This both makes your code clearer to yourself and to others, but avoids bad errors when you forget to specify an argument that has a default, for example: rnorm(5, 3) # What do you think is happening here? Is this good code? ## [1] 4.715065 3.460916 1.734939 2.313147 2.554338 NB: RStudio makes it easy to both get help and to write good code using the “code autocomplete” function. Let’s all make sure we’ve already got this turned on: Now, when you start typing a function, you can Hit tab to complete the function name Once you’ve opened the parentheses for the function, hit tab to see the arguments from the help file. Hit tab a second time to autofill the argument with an = to help you type! 3.1.5 Writing your own functions Largely, R packages are assemblies of useful functions. But sometimes you will need to write your own. Hadley Wickham and Grolemund (2017) writes that functions should be used whenever you find yourself frequently cutting and pasting code, as well as to make your code more readable by giving clear, evocative names to the things you’re actually doing. This can go right along with our pseudocoding tricks–you make your functions read more like English and it becomes much clearer what you’re doing. Funny enough, in R you write a function using the–wait for it–function() function. Now that’s recursion! Specifically, you choose a name for your function, and then create a new object of class function by assigning it to your new object name using &lt;-. So the whole process looks like this: make_it_awesome &lt;- function(a_thing) paste(a_thing, &quot;is awesome!&quot;) make_it_awesome(&quot;ice cream&quot;) ## [1] &quot;ice cream is awesome!&quot; make_it_awesome(3) ## [1] &quot;3 is awesome!&quot; make_it_awesome(c(&quot;dog&quot;, &quot;cat&quot;)) # hmm ## [1] &quot;dog is awesome!&quot; &quot;cat is awesome!&quot; Just like for() and if() statements, function() statements that are more than 1 line need to be enclosed in {} braces in order to tell R that the whole function needs to have all of those lines. In fact, you can do this with even 1-line functions: make_it_awesome &lt;- function(a_thing){ paste(a_thing, &quot;is awesome!&quot;) } make_it_awesome(&quot;FST 5984&quot;) ## [1] &quot;FST 5984 is awesome!&quot; To be more explicit, a function needs to have 3 elements: A name The function(&lt;argument 1&gt;, &lt;argument 2&gt;, ...) structure One or more lines that execute the function inside the {} Let’s recall that, to normalize a set of observations, we first find the mean and sd of that set, then subtract the mean and divide by the sd for each number. This would be tedious and error prone if we did it one at a time (and maybe impossible if our set was large). Let’s do this with some values from the mtcars dataset for fun: mtcars$wt # this is the weight of each car in 1000s of lbs. What does &quot;$&quot; do? ## [1] 2.620 2.875 2.320 3.215 3.440 3.460 3.570 3.190 3.150 3.440 3.440 4.070 ## [13] 3.730 3.780 5.250 5.424 5.345 2.200 1.615 1.835 2.465 3.520 3.435 3.840 ## [25] 3.845 1.935 2.140 1.513 3.170 2.770 3.570 2.780 mean(mtcars$wt) ## [1] 3.21725 sd(mtcars$wt) ## [1] 0.9784574 # Now for the first car: (mtcars$wt[1] - mean(mtcars$wt)) / sd(mtcars$wt) ## [1] -0.6103996 We could write a for() loop to step through every row and do this, but that seems like a lot of work. What if we write a function that takes any numeric vector and normalizes it? make_it_normalized &lt;- function(our_data){ our_mean &lt;- mean(our_data) our_sd &lt;- sd(our_data) (our_data - our_mean) / our_sd # remember that R will do vector math with scalars, so this &quot;recycles&quot; the scalars to the length of our_data } make_it_normalized(mtcars$wt) # well that&#39;s pretty cool ## [1] -0.610399567 -0.349785269 -0.917004624 -0.002299538 0.227654255 ## [6] 0.248094592 0.360516446 -0.027849959 -0.068730634 0.227654255 ## [11] 0.227654255 0.871524874 0.524039143 0.575139986 2.077504765 ## [16] 2.255335698 2.174596366 -1.039646647 -1.637526508 -1.412682800 ## [21] -0.768812180 0.309415603 0.222544170 0.636460997 0.641571082 ## [26] -1.310481114 -1.100967659 -1.741772228 -0.048290296 -0.457097039 ## [31] 0.360516446 -0.446876870 What’s even cooler is that this function can now be used anywhere else we want. Another famous dataset is the iris dataset, which observes several physical characteristics on a set of Iris flowers: str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... make_it_normalized(iris$Sepal.Width) # let&#39;s normalize those petal widths! ## [1] 1.01560199 -0.13153881 0.32731751 0.09788935 1.24503015 1.93331463 ## [7] 0.78617383 0.78617383 -0.36096697 0.09788935 1.47445831 0.78617383 ## [13] -0.13153881 -0.13153881 2.16274279 3.08045544 1.93331463 1.01560199 ## [19] 1.70388647 1.70388647 0.78617383 1.47445831 1.24503015 0.55674567 ## [25] 0.78617383 -0.13153881 0.78617383 1.01560199 0.78617383 0.32731751 ## [31] 0.09788935 0.78617383 2.39217095 2.62159911 0.09788935 0.32731751 ## [37] 1.01560199 1.24503015 -0.13153881 0.78617383 1.01560199 -1.73753594 ## [43] 0.32731751 1.01560199 1.70388647 -0.13153881 1.70388647 0.32731751 ## [49] 1.47445831 0.55674567 0.32731751 0.32731751 0.09788935 -1.73753594 ## [55] -0.59039513 -0.59039513 0.55674567 -1.50810778 -0.36096697 -0.81982329 ## [61] -2.42582042 -0.13153881 -1.96696410 -0.36096697 -0.36096697 0.09788935 ## [67] -0.13153881 -0.81982329 -1.96696410 -1.27867961 0.32731751 -0.59039513 ## [73] -1.27867961 -0.59039513 -0.36096697 -0.13153881 -0.59039513 -0.13153881 ## [79] -0.36096697 -1.04925145 -1.50810778 -1.50810778 -0.81982329 -0.81982329 ## [85] -0.13153881 0.78617383 0.09788935 -1.73753594 -0.13153881 -1.27867961 ## [91] -1.04925145 -0.13153881 -1.04925145 -1.73753594 -0.81982329 -0.13153881 ## [97] -0.36096697 -0.36096697 -1.27867961 -0.59039513 0.55674567 -0.81982329 ## [103] -0.13153881 -0.36096697 -0.13153881 -0.13153881 -1.27867961 -0.36096697 ## [109] -1.27867961 1.24503015 0.32731751 -0.81982329 -0.13153881 -1.27867961 ## [115] -0.59039513 0.32731751 -0.13153881 1.70388647 -1.04925145 -1.96696410 ## [121] 0.32731751 -0.59039513 -0.59039513 -0.81982329 0.55674567 0.32731751 ## [127] -0.59039513 -0.13153881 -0.59039513 -0.13153881 -0.59039513 1.70388647 ## [133] -0.59039513 -0.59039513 -1.04925145 -0.13153881 0.78617383 0.09788935 ## [139] -0.13153881 0.09788935 0.09788935 0.09788935 -0.81982329 0.32731751 ## [145] 0.55674567 -0.13153881 -1.27867961 -0.13153881 0.78617383 -0.13153881 Even more importantly, our function is just a bit of script we wrote–we can always go back and see what’s in it, or even edit if we need to: make_it_normalized(c(iris$Sepal.Length, NA)) # oops, if our data has an NA this doesn&#39;t work so well, does it? ## [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [101] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [126] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [151] NA make_it_normalized ## function(our_data){ ## ## our_mean &lt;- mean(our_data) ## our_sd &lt;- sd(our_data) ## (our_data - our_mean) / our_sd # remember that R will do vector math with scalars, so this &quot;recycles&quot; the scalars to the length of our_data ## ## } ## &lt;bytecode: 0x7fcc0228f0f8&gt; make_it_normalized &lt;- function(our_data){ our_mean &lt;- mean(our_data, na.rm = TRUE) # why do I set na.rm? our_sd &lt;- sd(our_data, na.rm = TRUE) # why do I set na.rm? (our_data - our_mean) / our_sd # remember that R will do vector math with scalars, so this &quot;recycles&quot; the scalars to the length of our_data } make_it_normalized(c(iris$Sepal.Length, NA)) # there we go ## [1] -0.89767388 -1.13920048 -1.38072709 -1.50149039 -1.01843718 -0.53538397 ## [7] -1.50149039 -1.01843718 -1.74301699 -1.13920048 -0.53538397 -1.25996379 ## [13] -1.25996379 -1.86378030 -0.05233076 -0.17309407 -0.53538397 -0.89767388 ## [19] -0.17309407 -0.89767388 -0.53538397 -0.89767388 -1.50149039 -0.89767388 ## [25] -1.25996379 -1.01843718 -1.01843718 -0.77691058 -0.77691058 -1.38072709 ## [31] -1.25996379 -0.53538397 -0.77691058 -0.41462067 -1.13920048 -1.01843718 ## [37] -0.41462067 -1.13920048 -1.74301699 -0.89767388 -1.01843718 -1.62225369 ## [43] -1.74301699 -1.01843718 -0.89767388 -1.25996379 -0.89767388 -1.50149039 ## [49] -0.65614727 -1.01843718 1.39682886 0.67224905 1.27606556 -0.41462067 ## [55] 0.79301235 -0.17309407 0.55148575 -1.13920048 0.91377565 -0.77691058 ## [61] -1.01843718 0.06843254 0.18919584 0.30995914 -0.29385737 1.03453895 ## [67] -0.29385737 -0.05233076 0.43072244 -0.29385737 0.06843254 0.30995914 ## [73] 0.55148575 0.30995914 0.67224905 0.91377565 1.15530226 1.03453895 ## [79] 0.18919584 -0.17309407 -0.41462067 -0.41462067 -0.05233076 0.18919584 ## [85] -0.53538397 0.18919584 1.03453895 0.55148575 -0.29385737 -0.41462067 ## [91] -0.41462067 0.30995914 -0.05233076 -1.01843718 -0.29385737 -0.17309407 ## [97] -0.17309407 0.43072244 -0.89767388 -0.17309407 0.55148575 -0.05233076 ## [103] 1.51759216 0.55148575 0.79301235 2.12140867 -1.13920048 1.75911877 ## [109] 1.03453895 1.63835547 0.79301235 0.67224905 1.15530226 -0.17309407 ## [115] -0.05233076 0.67224905 0.79301235 2.24217198 2.24217198 0.18919584 ## [121] 1.27606556 -0.29385737 2.24217198 0.55148575 1.03453895 1.63835547 ## [127] 0.43072244 0.30995914 0.67224905 1.63835547 1.87988207 2.48369858 ## [133] 0.67224905 0.55148575 0.30995914 2.24217198 0.55148575 0.67224905 ## [139] 0.18919584 1.27606556 1.03453895 1.27606556 -0.05233076 1.15530226 ## [145] 1.03453895 1.03453895 0.55148575 0.79301235 0.43072244 0.06843254 ## [151] NA 3.2 Control structure - making your code hustle and flow In order to make use of R’s powerful tools for simulating data (and, later, analyzing it), we need to be able to give R instructions that take advantage ofthe fact that it can quickly and tirelessly do repetitive tasks. The idea of “control flow” is a computer-science concept that, broadly, comprises the idea of anticipating all the things you want your program to do, and writing explicitly how to handle them. This is because, if you’ll recall, the computer is very good at tedious, difficult tasks, but also requires completely precise instructions. Control flow can be visualized as the kind of decision chart that you’re very familiar with: A simple decision chart with conditional statements 3.2.1 Write-what-you-want: Pseudo Code Before we start getting into the technical details of how to build these control flows and tell your R program what to do, I want to give you 3 tools for thinking about programs that I’ve gathered from various colleagues over the years, and together help to build a non-mystical, non-technical approach to building your programs: The very first thing you need to do when you sit down to write any program (or do anything, really), is define your goal. Literally. At the top of your R Markdown, R Script, text file, Word Document, Whiteboard, Joplin notebook, whatever, write the following phrase: “This program needs to do _____.” Then fill in the blank. Once you’ve defined what you are trying to do, figuring out how to do it is a much simpler task. When you run into technically difficult problems (“I need R to draw a random number from 1 to 1000 and then access a matrix at that index for the p-value.”), put them into plain language, like you’re explaining what you’re trying to do to your friend who knows nothing about your project or programming: “Ok, first I need to draw a number from a hat, then tell the program that that number is the row I need, and then select the column which the p-value is stored in, which is the 5th column.” This is often called “rubber duck debugging”–keep a rubbber duck on your desk and explain to it. Finally, use pseudocode–a lot. Pseudocode really combines the first two steps here. You write, in plain but structured (bulleted or indented) language the steps you need your program to do. Any step that is complex probably needs to be broken down into substeps. Look at the pseudocode–if you know how to do each step in actual code (in R), then you can do it! Otherwise, keep breaking down the steps, using ?, searching Stack Exchange, etc, until you can put together each step. Then write the code! For reference, here is the pseudocode for the random number example from above. Let’s actually write this into real code. create: data frame called &quot;results&quot; repeat 100 times: roll a 6-sided die: if it&#39;s 1-2 write &quot;dog&quot; into the results data frame if it&#39;s 3-6 write &quot;cat&quot; into the data frame print: how many times &quot;cat&quot; is in results # Create a data frame called &quot;results&quot; results &lt;- data.frame(animal = character(100)) # Roll a six-sided die 100 times, and if result is in 1-2, write &quot;dog&quot;, otherwise write &quot;cat&quot; for(i in 1:100) results$animal[i] &lt;- ifelse(sample(1:6, 1) &lt; 3, &quot;dog&quot;, &quot;cat&quot;) # Count how many times &quot;cat&quot; appears length(results[results$animal == &quot;cat&quot;, ]) ## [1] 65 With that in mind, let’s formally learn about control flow! 3.2.2 Logical conditions We learned last week about the logical data type: TRUE and FALSE are the only contents of this kind of data, and they are reserved words in R (i.e., you can’t write somethig like TRUE &lt;- \"cat\"). The reason for this is these are used for boolean programming logic–control flow. Typically, these types of data aren’t something we store directly (we’d be more likely to store binary data as 0/1 instead)–rather, they are outcomes of logical conditions or tests that we ask about our data. The most common of these that you will be using all the time are binary comparison operators, which ask about the relationship between two variables: x &lt; y, x &gt; y - is x less (greater) than y? x &lt;= y, x &gt;= y - is x less (greater) than or equal to y? x == y - does x equal y. NB: note the double = here. Writing x = y as a logical test is a common novice R mistake. x != y - does x not equal y? Given the following value assignments, which of the logical statements above will return TRUE? x &lt;- 1 y &lt;- 10 A very useful binary operator that isn’t a “classic” one is %in%. This awkward-looking command asks whether the left-hand side is in the right hand side: &quot;cats&quot; %in% c(&quot;cats&quot;, &quot;dogs&quot;, &quot;giraffes&quot;) # is &quot;cats&quot; in this set of words? ## [1] TRUE 1 %in% 2:100 # is 1 in the sequence from 2 to 100? ## [1] FALSE A second category of important logical conditions are the is.*() functions, where * stands in for a data type, for example is.character(). These return TRUE when the class() of the variable is evaluated to match the * in the name. What? That statement was simpler than it sounded: is.character(&quot;dog&quot;) ## [1] TRUE is.numeric(&quot;dog&quot;) ## [1] FALSE There are two specific is.*() functions that are especially useful ask about two, special, reserved value/vector types in R: NULL and NA/Inf/NaN. NULL is a value that means the absence of a vector. The other 3 arise from division errors, from import errors/missing data, and from other processing errors. The key problem with all of these special values is that the above binary comparison operators (like ==) do not work with these. empty_value &lt;- NULL wrong_value &lt;- NA empty_value == NULL # this is wrong, and returns a 0-length logical vector ## logical(0) is.null(empty_value) # this works ## [1] TRUE wrong_value == NA # this is wrong, and will return NA (not TRUE) ## [1] NA is.na(wrong_value) # this works ## [1] TRUE 3.2.3 Conditional statements Conditional statements are, essentially, choices that R has to make. They evaluate a logical condition, like those above (and often more complicated ones) and then execute some arbitrary blocks of code based on the logical condition. An alternative way to think about these are as branching paths–R looks at the branch in the path, chooses the first one that meets the stated condition, and then goes that way. We encounter these all the time in decision trees. XKCD presents decision charts. 3.2.4 IF statements All functional programming language statements have some equivalent of R’s if(&lt;conditional statement&gt;). Very simply, the conditional statement is evaluated, and if it is true, whatever comes after the if() is executed–otherwise R will just skip over it without trying to run it. if(TRUE) print(&quot;this will always print&quot;) # why will this always print? ## [1] &quot;this will always print&quot; With if() statements, as well as with the others we’ll go over below, we’re going to introduce “curly braces”: {}. We introduce these to allow our if() statements to control whether multiple lines get executed. Note that above we had a single print() follow the if(). What happens if we write: if(FALSE) print(&quot;this will never print&quot;) print(&quot;because the if() is always FALSE&quot;) ## [1] &quot;because the if() is always FALSE&quot; Oops, we didn’t want either of those to print. How do we control both with the if()? We enclose everything with curly braces: if(FALSE){ print(&quot;this will never print&quot;) print(&quot;because the if() is always FALSE&quot;) } Now this works right. Notice that RStudio will helpfully indent the lines in the curly braces. This is a programming convention that shows you the order of precendence of the control flow–I personally think of this as all of the print() functions being under the if(). We can string together multiple if() statements: # Shall we play a game? my_die_roll &lt;- sample(1:6, 1) # roll a die for me your_die_roll &lt;- sample(1:6, 1) # and one for you if(my_die_roll &gt; your_die_roll) print(&quot;I win&quot;) if(your_die_roll &gt; my_die_roll) print(&quot;You win&quot;) ## [1] &quot;You win&quot; if(my_die_roll == your_die_roll) print(&quot;A tie!&quot;) Note that if two sequential if() statements are both true, they will both be evaluated. In general, your if() statements need to be disjoint: only one should be able to be true (at a given level in your decision tree). if(my_die_roll &gt; 0) print(&quot;Nice try&quot;) ## [1] &quot;Nice try&quot; if(your_die_roll &gt; 0) print(&quot;You too&quot;) ## [1] &quot;You too&quot; 3.2.4.1 ELSE statement The else statement has to follow an if() statement. Notice that I don’t put any parentheses after the else, because it is not evaluating a condition. Instead, else executes the statement(s) after it if the preceding if() is FALSE: if(sample(c(0, 1), 1) == 1) print(&quot;heads&quot;) else print(&quot;tails&quot;) # another game of coin flips ## [1] &quot;tails&quot; An oddity with R else statements is that they must be on the same line as the end of the if() statement. This is somewhat in contradiction to the rest of R syntax, which makes it hard to remember. If you get errors using else, try Checking your line spacing Using {} to surround both your if() and else blocks 3.2.4.2 Other conditionals R provides a number of more complicated conditionals to make large sets of IF-ELSE statements easier to write and read. The most common is ifelse(), which let’s you combine the two statements in one: ifelse(sample(c(0, 1), 1) == 1, &quot;heads&quot;, &quot;tails&quot;) # this is a silly way to use this! ## [1] &quot;tails&quot; More complicated are the base R switch() and the dplyr::case_when() functions, which evalaute a set of IF statements and automatically have a final ELSE statement. I will not give examples here, but they are worth examining when you start running into a lot of branching if() statements in your code. 3.2.5 Loops While conditional statements (if()) create forking paths in our code, loops execute some section of code repeatedly, often for a set number of times or until some condition is met. This is where R’s ability to do something an arbitrary number of times, very quickly, without making mistakes or getting bored is employed. We actually saw some for() loops in the example of random walks on the football field already, when we wanted to simulate 1000 different people making 16 different paths. We will often use loops to iterate through some data structure. For example, we might want to run some kind of analysis once for every row of a dataframe. The relevant pseudocode which might help make this clearer might look something like this: Goal: Get the average of a set of 6 instrumental measurements of total sugars in a data frame of 1000 observations (1000 x 6 dataframe) for every row in dataframe: get the 6 observations average the 6 observations store the average in a new vector end In fact, the for... end structure is very similar to how R writes the main type of loop you’ll encounter and use. 3.2.5.1 FOR loops The structure of a for() loop looks kind of like an if() statement, but within the parentheses is a sequence instead of a conditional statement. We’ve already learned about sequences today. The syntax looks like for(counter in sequence), where counter is a variable that the for() loop increments on each iteration of the loop. The simplest example might be: for(i in 1:10) print(i) # why does this look different than print(1:10)? ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 This prototypical example has several key elements that are important: The counter variable is defined in the for() statement, not with a &lt;- assignment as per normal Statements within the for loop have access to the counter variable Just like an if(), a single command can follow a for(), but otherwise we’ll need to enclose multiple statements with {} Let’s look at a slightly more complicated for() statement to get a better intuition of what’s happening. Here, we’ll create a 100 x 10 matrix of random (normal) data, and use a for() loop to get the average of each row. some_data &lt;- matrix(rnorm(1000, mean = 0, sd = 1), nrow = 100, ncol = 10) for(i in 1:100){ row_mean &lt;- mean(some_data[i, ]) } row_mean ## [1] -0.04234701 Huh? Shouldn’t we have 100 means, 1 for each row? Well, we did calculate those means, but we are writing a new value to row_mean each time our loop executes. If we want to store them, we will have to create some place for them to go. some_data &lt;- matrix(rnorm(1000, mean = 0, sd = 1), nrow = 100, ncol = 10) row_mean_storage &lt;- numeric(100) # create a 100-space long numeric vector to hold our means for(i in 1:100){ row_mean_storage[i] &lt;- mean(some_data[i, ]) # put the i-th mean into the i-th slot in row_mean_storage } str(row_mean_storage) # I don&#39;t want to print all 100 values, so let&#39;s look at the structure of the new vector to see if this worked ## num [1:100] 0.0505 0.4107 0.3348 0.3465 -0.154 ... hist(row_mean_storage) This illustrates two more important aspects of for() loops: The “current” state of the loop is not saved in between loops unless you tell R to do so explicitly The commands under the for() loop can access and modify variables outside the loop, but the reverse is not true (because of point 1, above) In actual R syntax, we need to recall what we’ve learned about for() loops to make sure they run correctly. Here is a nested for loop that makes this explicit. # We&#39;re going to use brackets to be really clear about what the SCOPE of each for() loop is for(number in 1:3){ # we can call our indicator variable (like &#39;i&#39;) anything -- using descriptive names is clearer for(letter in c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)){ # recall that &#39;letters&#39; is a built in vector of a-z combination &lt;- paste(number, letter, sep = &quot; and &quot;) # paste() is a utility function that PASTES a series of strings together print(combination) } } ## [1] &quot;1 and a&quot; ## [1] &quot;1 and b&quot; ## [1] &quot;1 and c&quot; ## [1] &quot;1 and d&quot; ## [1] &quot;2 and a&quot; ## [1] &quot;2 and b&quot; ## [1] &quot;2 and c&quot; ## [1] &quot;2 and d&quot; ## [1] &quot;3 and a&quot; ## [1] &quot;3 and b&quot; ## [1] &quot;3 and c&quot; ## [1] &quot;3 and d&quot; print(combination) ## [1] &quot;3 and d&quot; There are two things to note here: Each internal loop needs to fully complete before the external loop goes again - we have \"1 and a\", \"1 and b\", etc. At the end of the entire loop, recall that our stored variable, because it was all the way inside the loop, is just the very last value: \"3 and d\" 3.2.5.1.1 Thinking about for loops in context for() loops can be used inside each other, and are often used to “walk through” a data frame. Here’s a real type of example: say you have a matrix in which each row is a sample of a different cider, and each column is a cider-chemistry quality parameter (TA, RS, etc). You may reasonably want to know, for each variable, which samples are above the average reading, and which are below. While there are a number of more efficient ways to do this, you can use a set of for() loops to do this. In pseudocode: Goal: create an indicator matrix that, for every reading in your data, tells you whether that reading is higher than the column average create indicator_matrix that is the same shape and has the same row and column names as your data for column in columns of your data store column average in col_average for row in rows of your data store your_data[row, column] &gt; col_average in indicator_matrix[row, column] end for end for 3.2.5.2 WHILE loops Other programming languages prioritize a different type of loop that can also be used in R: the while() loop. The way one uses a while() loop is by writing while(&lt;conditional statement&gt;). In this way, these are sort of like a looping if() statement: they will keep executing the code they enclose until the conditional statement becomes FALSE. # Create a die and set it to 3 die &lt;- 3 # Roll the die until you get a 1 while(die &gt; 1) { die &lt;- sample(x = 1:6, size = 1) if(die &gt; 1) print(paste0(&quot;You rolled a &quot;, die, &quot;, so you win!&quot;)) else print(&quot;You rolled a 1, so you lose!&quot;) } ## [1] &quot;You rolled a 3, so you win!&quot; ## [1] &quot;You rolled a 1, so you lose!&quot; I said that these kinds of loops are less common in R, and there is a good historical reason for that. R, while a complete programming language in a technical sense, is really built for data analysis. for() loops work with a known precondition: we set up a number of iterations (often based on the shape of our data) and set them running. while() loops tend to be more useful when we know less about our data–which is more the case when we’re looking for user input, or accepting data of an unknown type/shape/quantity. So for() loops, which are a little easier to think about and have a functionality that is closer to data analysis are preferred. 3.2.5.3 Future programming: apply() and map() families When reading Stack Exchange or Stack Overflow, you will see a lot of discussion of functions like apply() and map(). The term “vectorized” is often applied to these. The short summary of this idea is that, unlike loops, these functions take R vectors (remember that all R objects are vectors at heart) and apply functions to them in a very efficient way. These functions (and, really, programming paradigms) are more efficient and faster than for() and while() loops. This is true. It is also much harder to understand them off the bat, and for your needs isn’t really necessary. We will be returning to some of these functions in the following weeks, but right now I am 100% certain that loops and conditionals are enough to keep you properly occupied for a while (ha!). As my mentor told me once: for research, first write the code so it works, then make it efficient… and often you can skip the second part, if you’re never going to run the code again! 3.3 Simulating and sampling data in R Woof, even writing these notes I feel like we’ve gotten through a lot today. But we’re not quite done, because I want to spend a bit of time to start putting these tools. Like many of my terms in the notes for this class, the terms “simulation” and “sampling” are used in extremely inexact ways that might horrify a statistician or computer scientist. But I think that the way I am using them will reflect how you’ll encounter them in the wild. So, to give some broad and inaccurate definitions (that nevertheless correspond to my internal conceptions): “Simulation” - creating data points (these could be single values or whole “cases” of data) that have random properties distributed according to the parameters you specify. For example, you might want to simulate 500 values drawn from the Normal distribution (the classic bell curve). To simulate, you must know something about the “population” of values you are trying to simulate–this prior knowledge is often the barrier we as scientists face in constructing simulations. “Sampling” - to draw a subset of observations from an existing dataset, usually at random Sampling can be an alternative to simulation if we assume that the population resembles our existing dataset–this is called bootstrapping Sampling solves the problem for the scientist of making decisions about the prior properties of the population 3.3.1 Why simulation is important Simulation can sound like something exotic that is done in a fancy science-fiction-esque lab by people with intense computer skills. This can make it seem unapproachable or unrealistic for the average scientist, and it discourages us from making it part of our analytical practice. But the real truth is that the field of statistics and probability developed in the absence of computational power that enables simulation. It is an oversimplication (that I will make) to say that statistical theory was developed to fill this absence, but it is helpful to think this way. Here is an illustration: in your basic stats class you probably learned about the binomial distribution, which describes the probability of a set number k of “successes” in N trials when the probability of a success is binary, with success probability p and failure probability 1 - p. Oof, that’s a long way to say “the probability of getting k heads when you flip a coin N times,” but here we are. But the reason we need the binomial distribution in the first place is because, when it was developed by Jacob Bernoulli in the 17th century, no-one wanted to flip thousands of coins to see what happened. So traditional statistics developed a rich toolbox to allow for the estimation and interpretation of the probabilities associated with common types of events. Jacob Bernoulli, deriver of the binomial distribution, from Wikipedia 3.3.1.1 A quick diversion: data science vs traditional statistics Let’s take a minute to pause and remember the point of this class: to teach you to code and manage data with R in service of research. According to Andrew Gelman, a well-known statistician, statistics is probably a subfield of data science, which adds to statistical analysis a broader concern with, on the one hand, data management and cleaning (we’ll see this shortly) and, on the other, a focus on computational, algorithmic, or programmatic thinking for problem solving (Shah 2020). So, according to this definition (which seems to echo the thinking of other prominent writers Bruce and Bruce 2017; Wickham and Grolemund 2017), what we are largely focusing on in this class is data science. I will go one further and particularly point out the textbook by Bruce and Bruce (2017), in which the authors emphasize the idea of exploratory data analysis as being key to data science. I think I endorse this idea–much of the traditional statistics that you might learn, say in STAT 5615, emphasizes the inferential aspect of statistics: being able to say the all important “significantly different at an \\(\\alpha &lt; 0.05\\) level” statement. As we’ll discuss throughout the second and third parts of the class, there are a bunch of problems with this approach, but I think that one that isn’t talked about as much is the lack of emphasis on exploration of data. This is for two reasons (as I see them): We don’t teach good methods for really understanding the data we have, which leads to us blindly running statistical “tests” and claiming we understand the data Scientists are going to explore the data anyway, but do so unsystematically–this leads to all kinds of bad outcomes like “p-hacking”. So we need to teach safe data exploration! Anyhow, this is a somewhat roundabout way of saying that “traditional statistics” as it’s taught–usually a progression from probability to parameter definition to binomial testing to z- and t-tests to ANOVA/regression–does a disservice to scientists, who increasingly work with data. We should start with tools that let us manipulate and explore data and build models from those (which, again, we will discuss in the latter parts of this course), rather than learning models first and then trying to understand how they fit to data. One more attempt to rephrase these thoughts in order to make sure that I’ve clarified them: too many times I and my students have had the experience of understanding the traditional, statistical model/approach when it is taught in the classroom, and then being completely unable to flexibly and rigorously apply the model to our data. If we start from a place of data exploration and take a programmatic approach, we can instead understand how to go from our data to a model of our data without feeling so lost. 3.3.2 Predictable sequences PHEW. That was a lot of feelings. Let’s talk about numbers, instead! We started this section by talking about tools for generating data, randomly or nonrandomly. R has lots of utilities to help you do this built in, and more you can install from packages. We’re going to learn a few of the basics today, and we’ll get to more in the coming weeks. The first set of tools we’re going to learn are tools for generating predictable sequences or sets of numbers. We’re going to learn about: The : operator The rep() function The seq() function 3.3.2.1 The : operator The first and in many ways friendliest operator for getting a sequence of numbers in R is super simple: :. Specifically, &lt;from&gt;:&lt;to&gt; will generate a vector of integers between from and to, inclusive. 1:17 # print the numbers 1 to 17 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 This is extremely useful shorthand, but isn’t super flexible. It’s great for indexing from sets: letters # &quot;letters&quot; is a built in vector in R of the English alphabet ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; letters[5:12] # get the 5-12 letters of the alphabet ## [1] &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; We can use negative numbers at the start or stop, but the behavior is slightly unpredictable, so be careful to test what you’re doing before you include it as part of a larger function or analysis. -1:-10 # gives the negative numbers from -1 to -10 ## [1] -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 -1:10 # gives all numbers between -1 and 10 ## [1] -1 0 1 2 3 4 5 6 7 8 9 10 1:-10 # counts DOWN from 1 to -10 ## [1] 1 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 The : always counts by 1, but it will count non-integer numbers if you want it to (not recommended). 1.2:5 # Count from 1.2 to 5 (note, DOES NOT include 5 b/c that is &lt; 1 from 4.2) ## [1] 1.2 2.2 3.2 4.2 1:4.5 # Count from 1 to 4, will not include 4.5 b/c it is &lt; 1 from 4 ## [1] 1 2 3 4 We can’t skip numbers or make sequences that are at an interval different from 1 between sequential numbers. 3.3.2.2 The seq() function So if you want to do any of the above, the seq() (for “sequence”) function is your friend. This function will still take a start and end point, but by setting arguments in the function you can have much better flexibility with what you get. help(&quot;seq&quot;) # Run this command yourself to see the display The help file for seq() tells us that there are several arguments we will need to set: from =, the number we want to start with to =, the number we want to end on by =, the increment we want each step to take So we might want to go from 0 to 10 by increments of 0.2: seq(from = 0, to = 10, by = 0.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 ## [16] 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 ## [31] 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 ## [46] 9.0 9.2 9.4 9.6 9.8 10.0 Alternatively, we can set length.out = to the number of steps we want to take (rather than the increment). So say we want to go from 0 to 10 in 25 steps: seq(from = 0, to = 10, length.out = 25) ## [1] 0.0000000 0.4166667 0.8333333 1.2500000 1.6666667 2.0833333 ## [7] 2.5000000 2.9166667 3.3333333 3.7500000 4.1666667 4.5833333 ## [13] 5.0000000 5.4166667 5.8333333 6.2500000 6.6666667 7.0833333 ## [19] 7.5000000 7.9166667 8.3333333 8.7500000 9.1666667 9.5833333 ## [25] 10.0000000 3.3.2.3 The rep() function Many times we actually want to generate multiples of the same value. This is where the rep() (for repeat) function comes in: rep(x = &quot;cool&quot;, times = 10) # get a vector of 10 &quot;cool&quot;s ## [1] &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; &quot;cool&quot; rep(x = c(1, 3, 5), times = 2) # repeat the vector (1, 3, 5) twice ## [1] 1 3 5 1 3 5 rep(x = c(1, 3, 5), each = 2) # now, repeat EACH ELEMENT of (1, 3, 5) twice ## [1] 1 1 3 3 5 5 What happens if we run rep(c(1, 3, 5), each = 2, times = 2)? Does it matter if instead we write it with the order of “each” and “times” reversed? OK, this is fun and all, but what’s the point? We frequently want to be able to use these kinds of sequences as ways to index data frames, or to create a new variable (like, for example, say we have 10 “treatment” and 10 “control” samples - how would you make a vector that has 10 “treatment” followed by 10 “control”?) 3.3.3 “Random” numbers Above we’ve learned how to generate defined sequences of variables. But many times we want to generate random variables. For example, we might want to write a program that looked something like this (this is pseudocode and will not execute): create: data frame called &quot;results&quot; repeat 100 times: roll a 6-sided die: if it&#39;s 1-2 write &quot;dog&quot; into the results data frame if it&#39;s 3-6 write &quot;cat&quot; into the data frame print: how many times &quot;cat&quot; is in results This is a trivial example, but I want to look carefully at the line roll a 6-sided die. Does a computer actually do that? Of course not. Instead, we would use some code combined with the software’s built in “Random Number Generator” (RNG) to generate a number in c(1, 2, 3, 4, 5, 6). The ability to do this is key to both the statistical analysis we’re going to do later and the idea of “simulating” data, which we’ll discuss below. 3.3.3.1 Pseudorandomness and set.seed() Most computer systems, including R, don’t actually generate truly random numbers: they make pseudorandom numbers by using algorithms that provide sequences of numbers that have many of the qualities of statistical randomness (beyond the scope of this class), but that are not truly nonrandom. Information about R’s RNG can be found by using ?RNG. It turns out there are a lot of options here beyond the scope of this class (and beyond my knowledge!), but more info on the “Mersenne Twister”, which is the default generator in R, can be found in Wikipedia. Cool fact: Mersenne Primes, which these are based on, are of the form \\(2^{N}-1\\), where \\(N\\) is also a prime number. The main point to know about pseudorandom number generators is that, because they are in fact deterministic in terms of sequence generation, the user of a pseudo-RNG can make reproducible results by starting the sequence at the same place. In many R scripts, you will see something like set.seed(&lt;number&gt;). What this does is set the “seed” generator for the pseudo-RNG to &lt;number&gt;. That means that results should be exactly reproducible (to the limits of computational precision) if they are started with the same seed. So, to run reproducible code, we’ll set a seed here: set.seed(1234) # this will not return anything This is especially useful when we are sampling from an existing distribution and want to be able to share our results with collaborators or in the future. 3.3.3.2 Random, uniform numbers We are not going to be dealing with formal probability distributions much in this class. Just to get it out of the way, a probability distribution \\(p(x)\\) has the following properties: \\(\\forall x, p(x) ≥ 0\\) \\(\\int{p(x)dx} = 1\\) While I personally don’t find calculus too icky, I am not a mathematician and I don’t think naturally in a lot of these terms. So all these are telling us is that The probability of any event \\(x\\) is either 0 or positive–no such thing as negative probability The total probability of anything happening is 1–something’s gotta happen With that out of the way, we can talk about the uniform probability distribution, which is a formal version of our intuition of a “random number”. The uniform distribution between A and B is just a way of saying that any outcome (number) between A and B is equally probable. So, if we have a uniform distribution between 0 and 1, it means any number in the \\([0, 1]\\) interval is equally likely. This is a long way to go to introduce the function in R for generating such numbers: runif(). This function has 3 arguments n = is the number of random numbers we want to generate min = is A in our example - the lower bound max = is B in our example - the upper bound runif(n = 1, min = 0, max = 1) # generate a single random number between 0 and 1 ## [1] 0.1137034 runif(n = 10, min = 0, max = 100) # generate 10 random numbers between 0 and 100 ## [1] 62.2299405 60.9274733 62.3379442 86.0915384 64.0310605 0.9495756 ## [7] 23.2550506 66.6083758 51.4251141 69.3591292 Intuitively, we know that if we generate a lot of numbers from the uniform distribution they shouldn’t cluster together: they should be random. Let’s check this by plotting a histogram of 1e4 (10,000) random numbers between 0 and 100. hist(runif(n = 1e4, min = 0, max = 100)) Looks pretty ok! There isn’t any pattern in our histogram’s “bins”, which indicates that each one is as likely to be drawn as any other. If we actually want to look at the “distribution” the standard visualization is the “density” curve, which gives the amount of probability at each point. curve(dunif(x, min = 0, max = 100), from = -10, to = 110) …that’s just a mesa! Which is good news, of course. Every point in \\([0, 100]\\) has equal probability density, and everything else has 0. We’re good. So we can use runif() to generate draws of random numbers in an interval when we don’t have any reason to think any one number is more likely than another. This describes a lot of situations we tend to describe–for example, the outcome of a dice roll, coin flip, etc. But this isn’t always the most realistic description. 3.3.3.3 The normal distribution The normal distribution is the other famous distribution we all know: it is a bell-shaped curve, which, in its standard form, is centered at 0. It has two “parameters”: mean and standard deviation. It is also super ugly in its canonical form–a combinaton of an exponential and a quadratic function. I am not going to type it out, because you’ve seen it and not memorized it, just like me. Instead, let’s talk about why we care about it–and why it’s even called the “normal” distribution. It looks like this: curve(dnorm(x, mean = 0, sd = 1), from = -3, to = 3) First off, the key properties of the normal distribution are that: It is centered at the mean It is symmetrical Most of the probability density is near the mean More formally, ~2/3 of the probability density is within ±1 standard deviation of the mean But let’s try to be a little less formal and more useful: the normal distribution describes a situation in which we expect most of our observations to be near to the average, and the probability of seeing an observation very far from the mean is quite small. So when we expect our observations to tightly cluster around some average, the normal distribution will describe this situation well. We can generate observations from the normal distribution using the rnorm() function. Notice the similarity in name? R has a bunch of random number generators that all start with “r”, followed by an indicator of the distribution they are sampling from (e.g., rt(), rbinom(), rf()). rnorm() takes the following arguments: n = just like runif(), the number of numbers you want mean = the mean you expect to see most of your numbers near sd = the standard deviation of how far on average you want your numbers to go Let’s try it out with 1e4 observations again from a normal with mean = 20 and sd = 35: hist(rnorm(n = 1e4, mean = 20, sd = 35)) That looks about right–our bins aren’t suspicious. We have a bell shape. And you’ll notice most of the observations cluster around the mean value we specified, with spread decreasing rapidly as we move more than 35 units from the center in either direction. We can look at some of the variations in normal distributions to see how changing these parameters might work: curve(dnorm(x, mean = 0, sd = 1), from = -5, to = 5, col = &quot;blue&quot;, lwd = 2) # the &quot;standard&quot; normal curve(dnorm(x, mean = 0, sd = 2), from = -5, to = 5, col = &quot;red&quot;, lty = 2, add = TRUE) # a much more &quot;diffuse&quot; (higher sd) normal curve(dnorm(x, mean = 3, sd = 1), from = -5, to = 5, col = &quot;orange&quot;, lty = 3, add = TRUE) # a standard normal with mean shifted curve(dnorm(x, mean = -1.2, sd = 0.90), from = -5, to = 5, col = &quot;purple&quot;, lty = 4, add = TRUE) # a normal shifted and with less spread (lower sd) It’s called the “normal” distribution because there are many ways that these distributions crop up in “nature”. According to statistician Richard McElreath (2020), the normal distribution occurs whenever the underlying process that is generating the data add up random fluctuations will end up in a normal distribution, because the positive and negative large distributions will tend to cancel each other out in the long run, ending up with the familiar, bell-shaped curve clustered around the mean. Here’s the example from McElreath (2020): imagine your 1000 acquaintances all line up on a football field on the halfway line, and, 16 times each, flip a coin and take 1 step forward if it’s heads and 1 step backwards if it’s tails. You’ll end up with a normal distribution of people around the halfway line, with a spread of about ±5 yards. But why do I bring up this (undoubtedly interesting) example? Because it’s actually really easy to simulate this example in R: steps &lt;- matrix(0, nrow = 1000, ncol = 16) # we make a nice matrix to store our results colnames(steps) &lt;- paste0(&quot;step_&quot;, 1:16) # each row is a person, each column is their step position for(i in 1:1000){ # for each person, do the following steps for(j in 1:16){ # for each step, flip a coin steps[i, j] &lt;- sample(c(-1, 1), 1) %&gt;% rnorm(n = 1, mean = ., sd = 1) # if the result is heads, step forward (+1) or backward (-1) with gaussian step size } } # Now we have a matrix that shows what happens at each step. We WANT a matrix # that shows WHERE each person is at each step. To do this we will use the # cumsum() function, which stands for &quot;cumulative sum&quot;, meaning that at each # step it will add all previous steps positions &lt;- matrix(0, nrow = 1000, ncol = 16) colnames(positions) &lt;- paste0(&quot;yards_&quot;, 1:16) for(i in 1:1000) positions[i, ] &lt;- cumsum(steps[i, ]) # for each person, set their yardage to the sum of their steps on all previous steps steps[1, ] # here are the steps that person 1 makes ## step_1 step_2 step_3 step_4 step_5 step_6 step_7 ## -0.8024172 2.4660827 -0.1602998 -0.3775922 -1.5542999 0.8668254 -0.2922849 ## step_8 step_9 step_10 step_11 step_12 step_13 step_14 ## 1.9071436 -2.3753898 -0.8606970 1.4902076 0.9353355 0.9484532 -0.3266756 ## step_15 step_16 ## 0.5931884 -0.3117176 positions[1, ] # and here is where they are in terms of yardage ## yards_1 yards_2 yards_3 yards_4 yards_5 yards_6 yards_7 ## -0.8024172 1.6636655 1.5033657 1.1257735 -0.4285264 0.4382990 0.1460141 ## yards_8 yards_9 yards_10 yards_11 yards_12 yards_13 yards_14 ## 2.0531577 -0.3222321 -1.1829290 0.3072786 1.2426140 2.1910672 1.8643916 ## yards_15 yards_16 ## 2.4575800 2.1458625 So we’d ideally like to see WHERE those people are. We’re going to use a set of packages called tidyverse and the patchwork package to easily plot this. library(tidyverse) library(patchwork) step_plot &lt;- positions %&gt;% as_tibble() %&gt;% slice_head(n = 200) %&gt;% mutate(person = row_number(), yards_0 = 0) %&gt;% pivot_longer(names_to = &quot;step&quot;, values_to = &quot;yardage&quot;, -person) %&gt;% mutate(step = str_remove(step, &quot;yards_&quot;) %&gt;% as.numeric()) %&gt;% ggplot(aes(x = step, y = yardage, group = person)) + geom_line(data = . %&gt;% filter(person == 1), aes(x = step, y = yardage), color = &quot;red&quot;, inherit.aes = FALSE, size = 1.5) + geom_line(alpha = 0.2) + theme_classic() step_hist &lt;- positions %&gt;% as_tibble() %&gt;% mutate(person = row_number()) %&gt;% pivot_longer(names_to = &quot;step&quot;, values_to = &quot;yardage&quot;, -person) %&gt;% mutate(step = str_remove(step, &quot;yards_&quot;) %&gt;% as.numeric()) %&gt;% ggplot(aes(x = yardage)) + geom_histogram(bins = 24) + facet_wrap(~step, nrow = 4) + theme_classic() step_plot + step_hist This doesn’t mean, McElreath goes on, that normal distributions are “true” or “inevitable”. Rather, he writes that assuming our data is normal (and either generating data through rnorm() or using simple models that use the normal distribution as our “null” knowledge) represents a “particular state of ignorance” (McElreath 2020, 75): we are saying we don’t know anything except that we think our data has a particular mean (“center”) and standard deviation (“tendency to vary from the mean”)–we don’t have any idea about why it’s this way, only that probabilistic distributions tends to “regress” to this distribution, and so we’re not comfortable giving any more info. 3.3.3.4 Summary To quickly summarize, we can simulate using runif() when we want numbers drawn without any bias: only from a given range. When we think that our data has a tendency to cluster around a center, we can use rnorm() to simulate random numbers. 3.3.4 Sampling from existing data But what if we already have data? We then may want to draw random samples from that data for a couple of reasons: Our dataset is large and we want to get a representative subsample We want to use our data for simulations R has a very simple, very powerful function for that: sample(). The sample() function is at the heart of many simulations. We’ll be getting to them in the later 2/3 of this class, so right now we’ll just look at how it works. Try using ?sample to get some details. sample(x = 1:6, size = 1) # here we simulate rolling a six-sided die 1 time ## [1] 3 sample(x = 1:6, size = 1000, replace = TRUE) # and here we simulate rolling that same die 1000 times ## [1] 5 5 4 2 1 6 6 1 1 1 5 4 5 6 4 4 3 2 4 2 5 4 1 2 2 1 3 5 6 2 4 1 3 2 4 2 2 ## [38] 3 5 5 2 5 6 3 5 2 1 1 5 5 3 1 3 3 2 3 5 5 6 5 2 3 5 5 1 1 2 2 1 4 5 4 4 1 ## [75] 3 2 4 3 1 6 5 5 1 6 6 2 4 5 6 5 1 6 4 4 4 4 2 5 2 5 6 2 4 1 4 4 6 3 1 1 5 ## [112] 4 5 4 4 2 4 5 6 3 6 4 3 4 6 4 1 2 4 6 2 5 4 6 2 5 5 4 1 1 6 1 6 1 6 5 3 3 ## [149] 4 4 2 3 6 1 2 5 6 1 5 6 4 4 5 2 2 5 1 6 6 3 2 4 3 6 1 1 5 4 5 5 4 5 2 4 3 ## [186] 1 5 5 1 2 1 4 4 1 6 2 1 6 3 6 2 1 2 1 3 5 6 3 5 3 1 2 5 4 3 5 6 1 1 6 6 4 ## [223] 5 2 3 6 1 5 4 5 3 6 4 3 2 4 3 1 1 4 3 2 1 5 6 3 2 4 3 4 5 5 5 1 4 1 1 1 5 ## [260] 1 3 5 1 3 2 3 5 2 1 5 1 3 3 6 4 3 3 2 5 6 6 4 2 1 3 2 2 5 2 1 4 3 6 3 6 2 ## [297] 5 5 5 3 4 6 2 5 5 2 6 1 5 1 6 2 2 1 1 1 2 3 6 1 1 2 2 2 6 6 3 5 2 2 6 3 4 ## [334] 6 1 2 3 5 1 4 1 5 5 3 2 2 2 5 6 1 6 1 2 6 3 6 2 2 4 1 2 1 1 3 3 4 5 1 5 1 ## [371] 5 4 5 4 3 4 4 6 2 5 3 1 2 4 2 3 2 6 2 6 3 4 3 6 3 4 2 4 3 3 1 5 5 2 1 1 3 ## [408] 3 4 1 3 3 3 2 1 4 6 6 1 4 5 6 5 3 5 5 3 1 5 4 3 3 6 3 6 6 4 1 5 3 4 4 1 1 ## [445] 1 2 2 6 6 1 6 5 6 3 4 2 1 3 2 3 5 3 4 1 4 5 6 5 2 4 5 3 6 2 4 1 1 2 3 4 5 ## [482] 4 5 5 4 2 1 5 3 1 3 5 3 4 6 1 4 6 3 1 3 1 4 5 2 5 1 2 3 4 1 6 5 3 2 6 3 5 ## [519] 5 3 2 4 4 3 5 1 1 6 2 5 2 4 6 6 1 1 3 1 3 1 5 6 3 3 1 1 6 4 1 3 3 1 2 2 6 ## [556] 6 2 3 4 1 1 1 3 1 6 6 1 5 5 1 1 2 3 1 2 1 3 6 5 2 6 1 6 5 3 5 2 5 5 2 2 1 ## [593] 2 2 3 4 4 4 6 6 5 5 5 2 4 6 6 1 6 4 1 6 3 2 4 3 1 6 5 3 3 4 3 6 5 6 6 1 3 ## [630] 4 5 5 3 2 3 3 1 3 3 1 5 6 2 2 3 4 1 6 6 5 3 1 2 2 4 5 5 3 1 1 1 2 5 3 3 5 ## [667] 6 5 1 6 6 6 2 4 3 1 5 4 2 6 1 4 3 3 3 1 3 6 4 4 2 1 4 3 6 2 3 3 1 2 6 2 2 ## [704] 6 5 4 6 1 3 2 6 4 5 4 3 3 6 6 6 5 2 5 2 1 2 5 6 2 1 1 4 2 1 1 3 1 2 4 3 6 ## [741] 5 3 5 1 2 1 1 6 5 4 1 5 1 5 3 6 5 5 2 6 2 5 1 3 4 5 3 6 4 6 2 5 3 4 2 3 3 ## [778] 6 6 3 2 5 3 2 2 2 5 4 5 6 2 4 5 4 4 5 2 4 1 5 2 3 6 2 5 1 1 5 4 4 2 1 5 3 ## [815] 3 6 2 4 4 1 6 1 3 2 1 2 3 1 3 4 6 1 4 2 4 6 6 1 2 3 4 2 2 1 1 2 5 2 6 4 4 ## [852] 3 4 1 2 5 3 5 5 2 3 1 1 1 1 4 4 2 6 6 2 3 5 6 2 6 6 3 3 4 5 1 5 6 3 3 4 5 ## [889] 5 2 6 6 4 2 4 3 5 1 6 5 3 2 5 3 2 2 4 6 2 1 1 2 1 6 4 5 6 5 4 2 2 6 5 1 5 ## [926] 2 6 6 4 1 1 6 5 6 4 6 4 3 3 2 3 6 5 1 2 4 6 3 2 4 5 5 5 6 4 2 6 2 2 2 5 5 ## [963] 5 4 3 5 6 5 5 3 1 6 1 2 2 3 1 2 6 1 3 3 5 4 5 5 6 2 5 2 1 4 3 4 5 6 2 2 1 ## [1000] 3 What’s happening here? Well, the sample() function has the following arguments: x = this is the thing you want to sample from. Usually it’s a vector of numbers. size = how big a sample do you want? replace = this is a TRUE/FALSE variable; if TRUE, then x is “refreshed” for every draw in size prob = is a probability distribution with a probability for for each possible item in x The first two arguments are simple: what are you sampling from, and how many things do you want in your sample? The second two are a little more complicated. We’ll come back to each later in the class, but they are both worth giving a little more intuition to right now. The idea of sampling without replacement is like drawing numbers in Bingo. Once you draw a number in Bingo, it can’t be called again. It is not replaced. This means that our probabilities change after each draw–if we are sampling with replacement from \\(\\{1, 2, 3\\}\\) and we draw \\(3\\) on the first time, we only have two options for the next draw: \\(\\{1, 2\\}\\). So the probability of drawing 1 is different in that second draw! On the other hand, sampling with replacement means that we put the number back in the Bingo tumbler… if we draw \\(3\\) on the first draw in our toy example, in the second draw we still have the same probability of drawing 3 again. The idea of giving a probability distribution to your possible results is really just the same thing as weighting your results. Going back to our example of a die, if we think it’s weighted to come up “6” 50% of the time, we can use this to set prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5). Now our sample() function will have a 50% chance (probability) of rolling a 6, and we’ve told it the rest of the probability (the remaining 50%) is evenly distributed among the other 5 sides of the die. Some comprehension questions: Why do we use replace = TRUE in the 1000-roll die example above? What would happen if we used prob = c(0, 0, 0, 1, 0, 0) in the 1000-roll die example above? We can use sample to get us samples from data. Let’s look at, for example, the mtcars dataset. Say we want to only have 10 cars to examine. We can do the following to pull a random sample: nrow(mtcars) # there are 32 rows (observations) in mtcars ## [1] 32 our_sample_indices &lt;- sample(x = 1:32, size = 10, replace = FALSE) # get 10 random numbers in [1:32] mtcars[our_sample_indices, ] # use the indexing functions to get the rows that correspond to our random numbers ## mpg cyl disp hp drat wt qsec vs am gear carb ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 3.3.5 Summary - sampling and simulation To sum up, we’ve gone over methods to produce both predictable sequences of data and to sample randomly, both from theoretical distributions (the normal and uniform distributions) and from data we already have. In later classes, we are going to combine these techniques with the control structure and function techniques from the rest of this week to let us sample and simulate real data, as well as to do statistical inference. References References "],["wrangling-your-data.html", "4 Wrangling your data 4.1 Getting your data into R 4.2 Making your life in R easier with the tidyverse 4.3 Review of functions References", " 4 Wrangling your data We are now in the fourth week of the Spring 2022 semester. One quarter of the way through. Into February. Congratulations, y’all! Today we’re going to start moving a little bit from abstract “how to” do things in R (but of course we’ll still have plenty of that) and into specific tasks that are relevant to food and ag scientists who need to do data science work. The focus today is going to be on Reading data into R (as well as a little discussion of how we store data) Learning to use tidyverse, as set of R packages that makes R code easier and more intuitive (most of the time) (If we have time), returning to functions, including how to make them (and your scripts) readable and useful We’re going to have “the talk” about file extensions and how your computer stores data. Just like my discussion of data types in Week 1 (or, let’s be real, anything I do), this will be an informal and probably not 100% technically correct discussion. But learning to see these things properly will help you become a more capable user of scientific data. What file extensions can we trust? From XKCD. 4.1 Getting your data into R One of the main challenges my students and others face in using R is getting data into the damn thing. I personally don’t think that most stats classes, or most classes on R, spend enough time on how to store your data, how to get it into a shape that R won’t struggle with too much, and how to actally get it into R (and what happens when you do). I think one of the reasons for this is that typical software like SPSS or Minitab (or JMP) gives you a spreadsheet (Excel-like) interface to deal with your data, which you can either copy-paste in from Excel or input directly. Something like this: JMP’s interface directly imitates a spreadsheet, although it is not one. In R, we have to use tools that are similar to the command line to deal with files. With some exceptions we will go over briefly, reading data into R follows the same rules we’ve learned about (in painful detail) so far: it is fast and powerful, but it also requires that we be very precise about how our data is stored already and how we want to bring the data into R. These details seem arcane and pointless at first, and so we’re going to start by talking about how we store data in general. 4.1.1 Data storage outside R Who here stores their lab notebook data in Excel? Raise your hand. Me too! That’s because there’s nothing wrong with doing this (except that Excel is kind of a monster of a program, but that’s another story). Where we start running into problems is the intersection between data storage and data manipulation. Here is an example: Michael Wesolowski’s thesis (2018) dataset uses Excel both to organize and to analyze data. The problems with this approach are several-fold. First off, the analyses run in here (the average log calculation, for example), rely on the spatial alignment of the data. If the data are copy-pasted incorrectly, or a line is deleted, the analysis will become incorrect without giving any warning. Second, the visual annotation (the colored fields, the bolding, the outlines around cells) are not directly machine-readable (technically they are metadata that can be accessed with enough coding, but they are not directly attached to the data in a way that R or other programs can easily process). But the worst thing about this well-organized lab notebook is that the data are organized in multiple columns and rows within the same sheet, and that there are informational headers and other notes throughout the sheet. The reason this is bad has to do with how computers (and R in particular) process files. In general, R is going to be expecting a file with consistent structure on each line. Computers like structured data. Humans… may not. When we look at the above Excel file, our brains (which are pattern-finding engines) quickly intuit the patterns. We look to see where the colors are defined in the top lines, then check those out in the file. We understand that the first couple lines are “metadata”. But computers can’t know this (unless we build complex workflows for them to follow that encode that knowledge). So, instead, computers prefer data where the structure is predefined and there is no need for complicated rules. The typical kind of data we will deal with in this class is delimited data. Non-rigorously defined, delimited data is data stored where each field is delimited by the same character. So, in the Excel file above, imagine that each cell value (A1, etc) is followed by the same character. This character could be anything, technically, but there are some conventional choices: comma-separated, tab-separated, or space-separated (least common). A comma-separated file would look like this: # Comma-separated data cat_acquisition_order,name,weight\\n 1,Nick,9\\n 2,Margot,10\\n 3,Little Guy,13\\n This might look garbage-y, but let’s get in there. On each line of the file, we have three fields, each separated by a comma, and each line is ended (explicitly for the example) with the newline character: \\n (the newline character is what your computer reads as “go to the next line”, like when you hit enter on your keyboard to go to a new line, you are explicitly entering a \\n into the document). One of the lines in this file looks like column names, whereas the rest look like a consistent pattern of data. So we can think of this as a way to represent tabular data in a plain-text file. In fact, comma-separated value files have their own, common file extensions: .csv. And they are one of the most common ways to read data into R, but it is by no means the only way–we will go over a few others below in the Why Data Storage is Hard section. So how do we actually get these data into R? When we use JMP or SPSS or Minitab we can just open our .csv (or word file or whatever) and copy-paste the data into something that looks a lot like Excel. What happens if we try to copy-paste the cat example above into R? In general, our workflow for R is the following: Store data in delimited format (usually .csv, but we will be talking about how to deal with existing Excel (.xls, .xlsx) files today as well.) Organize your file directories SOMEHOW Name your files usefully–if you’re not going to use git, it is a good idea to name files with dates (I like YYYYMMDD format). Use a function like read.table() to read your data into R. It makes sense to store your files (or a copy of your files) in the same folder as your R scripts If you want to use R to make changes to your files, use write.table() or other functions to output your data back into the same folder. THIS IS VERY IMPORTANT: Do not store data in R’s workspace. Your scripts/R Markdown files should always read in a data file, and make all changes to that file. This is why we clear changes on exit. We can store data in an R format using .RData files and the save() function, but this makes the most sense only if we’re producing big or complex output we don’t want to rerun, and if we annotate the file well. 4.1.1.1 Example files We are going to be working with several files in today’s example, which are all stored in the &lt;class directory&gt;/Files/Week 4/ directory on Canvas. You should store them in a similar format, although on your file system you’ll want to replace &lt;class directory&gt; with your actual directory structure. We’ll talk about directory structure very briefly later today. The first file is polyphenol testing.csv, and it contains data from Sihui Ma’s dissertation (2019) research into polyphenols in apples. In this particular dataset, Dr. Ma was interested in the effect of readings of polyphenol content in response to different ratios of constituent polyphenols, because she suspected that tests were differentially responsive to different polyphenols (she was correct). The second file is soup pilot data.csv. This is from research I conducted in 2016 at Drexel University on the effect of course order on acceptance for parts of a meal. This file, however, is just pilot data–we needed to develop different soups with different levels of base liking (outside of a meal), and so these are data from tasting of 4 soups, 2 minestrone and 2 hot and sour. To start with, both of these files are well-structured. Here are some non-exhaustive attributes of that well-structuring: Each line is an observation. In the polyphenol file, that means each line is a single observation (this is long data). In the soup file, each line is a subject, with 4 observations (each soup) on the same line (this is wide data). There is no formatting or other non-readable information. The format of each file is 1 header line (names of variables) followed by each line as an observation that fits the pattern. If your files are not in this format, it can sometimes make your life easier to get them into this format before you try to put them into R, although I will show at least one other option later. 4.1.2 read.table() The most basic utitlity in R for reading in delimited data is the read.table() function. If you run ?read.table you’ll see that this is a function with a ton of options, because it is made to be very flexible and not assume a lot about the file you’re trying to read. read.table(file = &quot;Files/Week 4/polyphenol testing.csv&quot;, header = TRUE, sep = &quot;,&quot;) # Note that this line tells read.table() how to break up your file - what happens if we exclude it with this file? ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.73 ## 5 0.2 1 200 58.19 ## 6 0.2 1 200 69.24 ## 7 0.2 1 500 150.31 ## 8 0.2 1 500 161.70 ## 9 0.2 1 500 161.13 ## 10 0.2 30 5 15.17 ## 11 0.2 30 5 15.27 ## 12 0.2 30 5 14.01 ## 13 0.2 30 200 63.54 ## 14 0.2 30 200 64.00 ## 15 0.2 30 200 62.40 ## 16 0.2 30 500 168.25 ## 17 0.2 30 500 159.71 ## 18 0.2 30 500 136.65 ## 19 0.2 100 5 46.57 ## 20 0.2 100 5 46.12 ## 21 0.2 100 5 50.90 ## 22 0.2 100 200 95.32 ## 23 0.2 100 200 92.36 ## 24 0.2 100 200 84.16 ## 25 0.2 100 500 148.32 ## 26 0.2 100 500 158.29 ## 27 0.2 100 500 157.43 ## 28 30.0 1 5 14.01 ## 29 30.0 1 5 13.80 ## 30 30.0 1 5 13.87 ## 31 30.0 1 200 60.01 ## 32 30.0 1 200 51.92 ## 33 30.0 1 200 65.14 ## 34 30.0 1 500 170.24 ## 35 30.0 1 500 173.38 ## 36 30.0 1 500 170.53 ## 37 30.0 30 5 21.93 ## 38 30.0 30 5 17.97 ## 39 30.0 30 5 17.32 ## 40 30.0 30 200 57.28 ## 41 30.0 30 200 52.04 ## 42 30.0 30 200 50.33 ## 43 30.0 30 500 155.44 ## 44 30.0 30 500 153.16 ## 45 30.0 30 500 158.00 ## 46 30.0 100 5 52.38 ## 47 30.0 100 5 51.47 ## 48 30.0 100 5 52.61 ## 49 30.0 100 200 115.58 ## 50 30.0 100 200 117.57 ## 51 30.0 100 200 116.43 ## 52 30.0 100 500 119.56 ## 53 30.0 100 500 130.38 ## 54 30.0 100 500 118.71 ## 55 100.0 1 5 44.75 ## 56 100.0 1 5 45.32 ## 57 100.0 1 5 44.98 ## 58 100.0 1 200 56.37 ## 59 100.0 1 200 56.14 ## 60 100.0 1 200 57.39 ## 61 100.0 1 500 131.52 ## 62 100.0 1 500 138.64 ## 63 100.0 1 500 136.36 ## 64 100.0 30 5 50.67 ## 65 100.0 30 5 51.92 ## 66 100.0 30 5 50.79 ## 67 100.0 30 200 56.82 ## 68 100.0 30 200 63.43 ## 69 100.0 30 200 66.85 ## 70 100.0 30 500 143.19 ## 71 100.0 30 500 153.45 ## 72 100.0 30 500 143.19 ## 73 100.0 100 5 56.03 ## 74 100.0 100 5 56.94 ## 75 100.0 100 5 56.14 ## 76 100.0 100 200 121.55 ## 77 100.0 100 200 115.58 ## 78 100.0 100 200 118.71 ## 79 100.0 100 500 151.17 ## 80 100.0 100 500 145.19 ## 81 100.0 100 500 151.45 The most important line here is the header = TRUE, which tells R that the first line in our file is a set of column names. Other options that are important are specifying the separator between columns (sep = \",\" tells this example that this is a .csv), and asis = TRUE, which will stop R from converting character data into factor data (the default behavior of read.table()). We won’t necessarily be using read.table() very much, because more specific options like read.csv() or more general options like scan() will almost always be able to do what we want. But it is good to see the basic function in action. You’ll see read.table() come up a lot in older R scripts and references. 4.1.3 read.csv() When we already know that the kind of data we’ll want to deal with is comma-separated, it’s much easier to use read.csv(), which does exactly what it sounds like. It is a wrapper function for read.table()–when you see something called a “wrapper”, it means it is actually just that function set up with some useful defaults. We can see that here: read.csv ## function (file, header = TRUE, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, ## fill = TRUE, comment.char = &quot;&quot;, ...) ## read.table(file = file, header = header, sep = sep, quote = quote, ## dec = dec, fill = fill, comment.char = comment.char, ...) ## &lt;bytecode: 0x7fdd2776b4e0&gt; ## &lt;environment: namespace:utils&gt; So read.csv() just tells read.table() a bunch of useful defaults so we don’t have to set them. Let’s try it on both of our files. read.csv(&quot;Files/Week 4/polyphenol testing.csv&quot;) ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.73 ## 5 0.2 1 200 58.19 ## 6 0.2 1 200 69.24 ## 7 0.2 1 500 150.31 ## 8 0.2 1 500 161.70 ## 9 0.2 1 500 161.13 ## 10 0.2 30 5 15.17 ## 11 0.2 30 5 15.27 ## 12 0.2 30 5 14.01 ## 13 0.2 30 200 63.54 ## 14 0.2 30 200 64.00 ## 15 0.2 30 200 62.40 ## 16 0.2 30 500 168.25 ## 17 0.2 30 500 159.71 ## 18 0.2 30 500 136.65 ## 19 0.2 100 5 46.57 ## 20 0.2 100 5 46.12 ## 21 0.2 100 5 50.90 ## 22 0.2 100 200 95.32 ## 23 0.2 100 200 92.36 ## 24 0.2 100 200 84.16 ## 25 0.2 100 500 148.32 ## 26 0.2 100 500 158.29 ## 27 0.2 100 500 157.43 ## 28 30.0 1 5 14.01 ## 29 30.0 1 5 13.80 ## 30 30.0 1 5 13.87 ## 31 30.0 1 200 60.01 ## 32 30.0 1 200 51.92 ## 33 30.0 1 200 65.14 ## 34 30.0 1 500 170.24 ## 35 30.0 1 500 173.38 ## 36 30.0 1 500 170.53 ## 37 30.0 30 5 21.93 ## 38 30.0 30 5 17.97 ## 39 30.0 30 5 17.32 ## 40 30.0 30 200 57.28 ## 41 30.0 30 200 52.04 ## 42 30.0 30 200 50.33 ## 43 30.0 30 500 155.44 ## 44 30.0 30 500 153.16 ## 45 30.0 30 500 158.00 ## 46 30.0 100 5 52.38 ## 47 30.0 100 5 51.47 ## 48 30.0 100 5 52.61 ## 49 30.0 100 200 115.58 ## 50 30.0 100 200 117.57 ## 51 30.0 100 200 116.43 ## 52 30.0 100 500 119.56 ## 53 30.0 100 500 130.38 ## 54 30.0 100 500 118.71 ## 55 100.0 1 5 44.75 ## 56 100.0 1 5 45.32 ## 57 100.0 1 5 44.98 ## 58 100.0 1 200 56.37 ## 59 100.0 1 200 56.14 ## 60 100.0 1 200 57.39 ## 61 100.0 1 500 131.52 ## 62 100.0 1 500 138.64 ## 63 100.0 1 500 136.36 ## 64 100.0 30 5 50.67 ## 65 100.0 30 5 51.92 ## 66 100.0 30 5 50.79 ## 67 100.0 30 200 56.82 ## 68 100.0 30 200 63.43 ## 69 100.0 30 200 66.85 ## 70 100.0 30 500 143.19 ## 71 100.0 30 500 153.45 ## 72 100.0 30 500 143.19 ## 73 100.0 100 5 56.03 ## 74 100.0 100 5 56.94 ## 75 100.0 100 5 56.14 ## 76 100.0 100 200 121.55 ## 77 100.0 100 200 115.58 ## 78 100.0 100 200 118.71 ## 79 100.0 100 500 151.17 ## 80 100.0 100 500 145.19 ## 81 100.0 100 500 151.45 read.csv(&quot;Files/Week 4/soup pilot data.csv&quot;) ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 ## 11 11 -50 75 -15 50 ## 12 12 75 25 -100 -100 ## 13 13 -25 50 -75 -10 ## 14 14 20 40 -75 60 ## 15 15 70 90 -25 -60 ## 16 16 20 90 -50 30 ## 17 17 15 90 30 50 ## 18 18 40 90 -60 -15 ## 19 19 55 85 -50 -60 ## 20 20 20 60 -40 0 ## 21 21 20 100 -100 60 ## 22 22 20 60 -60 -10 ## 23 23 10 100 -100 85 ## 24 24 10 75 35 55 It’s been a little while since we did this, but what happens if we want that data? polyphenol testing # We read the data and just threw it away! ## Error: &lt;text&gt;:1:12: unexpected symbol ## 1: polyphenol testing ## ^ Remember, we have to store R objects using &lt;-. polyphenols &lt;- read.csv(&quot;Files/Week 4/polyphenol testing.csv&quot;) soups &lt;- read.csv(&quot;Files/Week 4/soup pilot data.csv&quot;) Now we can access these objects and learn about them. class(polyphenols) # the read.*() functions create data.frame objects ## [1] &quot;data.frame&quot; head(polyphenols, n = 5) # display the first n lines of an object--defaults to n = 6 ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.73 ## 5 0.2 1 200 58.19 soups$minestrone1 # header = TRUE in read.csv() means that all the columns are named properly ## [1] 0 70 80 20 65 30 75 70 40 15 -50 75 -25 20 70 20 15 40 55 ## [20] 20 20 20 10 10 And we can start to do basic data analysis. Let’s look at a boxplot of soup rating: boxplot(soups[, -1]) # We use the [, -1] syntax to remove the first column of soups--why? 4.1.4 Why data storage is hard OK, so we’ve pretty quickly gotten to getting data into R. That didn’t seem so bad. But let’s try importing a third file, which contains data from my lab from J’Nai Kessinger’s thesis (2019), in which she had subjects sort 18 ciders into groups according to their own perceptions. The file is called cider sorting.csv, but it has two problems. It is not stored in the standardized location I set up for this R Markdown It is well-formatted for human reading, but not for computers. 4.1.4.1 Where files live in your computer The first problem above is one of file organization. Who here uses a set of structured folders to organize their files? But it seems like this is becoming less standard practice, according to this article in the Verge from Fall 2021. Both Windows and macOS have introduced more powerful search functions, so that if you have an idea of what your file is called you can kind of get away with just typing it into the search engine in the OS and having it return you the file. Unfortunately, this plays pretty poorly with coding for research. We have a lot of data files that are often versioned–with the same file name but different dates. We also tend to have similar file names: “Cider ANOVA”, “Polyphenol ANOVA”, etc. It takes longer and longer file names to specify these, and it also requires more and better memory. On the other hand, folder structure, of the type I show above, is sort of like “preserved memory”–I don’t actually remember teaching FST 3024 in 2018 very well, but I know if I go into that folder I will find files in descriptively labeled folders. Therefore for this class and for analysis projects, I recommend a simple file structure that can be expanded upon, rather than descriptively naming files. You should create a directory (a “folder”) for the class, and have sub-directories within that directory for each week. You will find this much easier to manage. 4.1.4.2 Dealing with other ways of storing data So once we find the file we can read it, right? str(read.csv(&quot;Files/cider sorting.csv&quot;)) # The file is very large, so I am not going to display it, but instead look at its structure via str() ## &#39;data.frame&#39;: 3817 obs. of 7 variables: ## $ Test.Name : chr &quot;102918_VA Cider Sort&quot; &quot;Section Name&quot; &quot;Sample Number&quot; &quot;1&quot; ... ## $ Description : chr &quot;New Test&quot; &quot;1. First Section: Sorting Task&quot; &quot;Sample Name&quot; &quot;Big Fish Cider Co.&quot; ... ## $ Test.Status : chr &quot;Ready To Run&quot; &quot;&quot; &quot;Blinding Code&quot; &quot;188&quot; ... ## $ Test.Owner : chr &quot;J&#39;Nai Phillips&quot; &quot;&quot; &quot;Sample Type&quot; &quot;Sample&quot; ... ## $ Saved.for : chr &quot;Virginia Tech Research&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Test.Language: chr &quot;English&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Test.Expiry : chr &quot;11/15/2018&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... The second problem with this file is that it is, like Michael Wesolowski’s thesis data, really mixing data organization and data analysis. In the file, if we open it, we’ll see that it looks like there are actually multiple data tables crammed into one file. This is a clue that this isn’t really a data file–it’s a directory. What we should do is restructure this file into multiple .csv files, all in one directory called “cider sorting”. For example, we’d want to create a cider samples.csv file with just the sample data, as well as .csv files for the sorting and the questionnaire data. This is just one example of how the same file type can contain multiple (and sometimes pathological) ways of storing data. There are many other ways of Structuring data. There is no need for the data to be tabular, for example. A common form of structured data is a JSON file–JavaScript Object Notation. These files look like this (example from Wikipedia): # This is a JSON object { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 27, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [], &quot;spouse&quot;: null } This is a very different kind of format than the .csv files we’ve been looking at, but you can see there are a number of “key-value” pairs here that give it structure, as well as being able to support a nested (recursive) structure, like that in the “address” field. This can be parsed by a computer easily because of its consistent structure–in fact, the jsonlite::fromJSON() function will happily do this. Can you run this and see what R thinks this is? Note two advantages of this kind of data storage over tabular (.csv) files: It allows for nested/structured subfields, like the “address” field (which is itself a tiny JSON) It allows for fields with multiple/arbitrary entries Neither of these are possible in tabular data or in R data frames–but these are both features of R’s list structure. And you’ll notice the JSON imports as a list object. As we will see in the rest of the course, we often encounter lists when we need flexibility. 4.2 Making your life in R easier with the tidyverse Having talked about getting data into R, we’re going to take a detour and talk about how to start to make your life easier, especially when dealing with these kinds of data.frame objects. The tidyverse is a shortcut package that you should already be familiar with from your reading in R for Data Science. It loads a number of individual packages, including two that make many of the R operations we’ve been learning about for the first month of class more natural and comfortable. You can learn about all of them at the tidyverse package page. Remember that, if you haven’t already, you will need to install and load the tidyverse package to make this work. install.packages(&quot;tidyverse&quot;) library(tidyverse) 4.2.1 Naming: long (“tidy”) vs wide data The name of the “tidyverse” comes from the core concept of “tidy” data. According to Wickham and Grolemund (2017), tidy data has the following properties: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. The result of this is the style of data we see in the polyphenols dataset, which we will use the tibble() function to make clearer: tibble(polyphenols) ## # A tibble: 81 × 4 ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.7 ## 5 0.2 1 200 58.2 ## 6 0.2 1 200 69.2 ## 7 0.2 1 500 150. ## 8 0.2 1 500 162. ## 9 0.2 1 500 161. ## 10 0.2 30 5 15.2 ## # … with 71 more rows ## # ℹ Use `print(n = ...)` to see more rows “Tidy” data is a synonym for “long” or “tall” data. This is in contrast to the “wide” data of the soups dataset: tibble(soups) ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows In soups, each row represents a case–the subject–and then there are multiple columns that represent different observations on a particular soup. The type-of-soup variable, meanwhile, has no column of its own. This is a common format for data storage, because it feels natural. But it may not always be the easiest format to access and manipulate data. In later classes we’ll learn about some easy functions to switch between these data formats using ideas from database software, but for now we’re just going to note their existence. 4.2.1.1 syntax conventions: unquoted arguments Before we dive into some of the howtos of the tidyverse, one other key difference from much of R is worth noting. In many of the “core” tidyverse functions like those we are going to learn about here, arguments to functions can be left unquoted. That means that, for example, if we want to use the select() function to pick the minestrone1 column from soups (see below), we would merely need to write select(soups, minestrone1). Contrast this with base R, in which (using the [] syntax), we’d need to write soups[, \"minestrone1\"]. This is a programming choice based on how R uses names and symbols in the environment (see ?as.name for some confusing terminology). It is not critical to know the why of this, only to know that, for many functions in tidyverse, you supply plain variable names, or even vectors (using c() of plain variable names). If you are really curious about how all this works, the Advanced R book may be of interest (but then I would wonder why you were taking this class). 4.2.2 One pipe to rule them all: %&gt;% One of the main tools that makes the tidyverse compelling is the pipe: %&gt;%. This garbage-looking set of symbols is actually your best friend, you just don’t know it yet. I use this tool constantly in my R programming, but I’ve been avoiding it up to this point because it’s not part of base R (in fact that’s no longer strictly true, but it is kind of complicated at the moment). OK, enough background, what the heck is a pipe? The term “pipe” comes from what it does: like a pipe, %&gt;% let’s whatever is on it’s left side flow through to the right hand side. Let’s try this out in some pseudocode load soups data %&gt;% # Load the soups data into R AND THEN select minestrone columns %&gt;% # Pick only the columns to do with minestrone AND THEN filter first 10 subjects %&gt;% # Get only the first 10 subjects to participate AND THEN run a t-test to compare soups # Run the statistical test we wanted In this example, each place there is a %&gt;% I’ve added a comment saying “AND THEN”. This is because that’s exactly what the pipe does: it passes whatever happened in the previous step to the next function. Without the pipe, we’d end up doing something like this: soups &lt;- load soups data # Create an object with our data soups_2 &lt;- select soups minestrone columns # Create a second object for our selected columns soups_3 &lt;- filter soups_2 first 10 subjects # Create a third object for our filtered data t-test(soups_3) # Run our t_test on the third data set This is messy, harder to read, and means that we have to run every line again if we mess up, because if we go back and, for example, change soups (say we forgot to use read.table() with header = TRUE), the other objects won’t be automatically updated. But we don’t care about those intermediate objects–they’re just by-products of our workflow. The advantage of the %&gt;% pipe is that it gets rid of those, and makes code that is much more readable in the process. There are a couple of points to know about using the pipe, and much more detail is available from the Wickham and Grolemund (2017) chapter on the subject. 4.2.2.1 Pipes require that the lefthand side be a single functional command This means that we can’t directly do something like rewrite sqrt(1 + 2) with %&gt;%: 1 + 2 %&gt;% sqrt # this is instead computing 1 + sqrt(2) ## [1] 2.414214 Instead, if we want to pass binary operationse in a pipe, we need to enclose them in () on the line they are in: (1 + 2) %&gt;% sqrt() # Now this computes sqrt(1 + 2) = sqrt(3) ## [1] 1.732051 More complex piping is possible using the curly braces ({}), which create new R environments, but this is more advanced than you will generally need to be. 4.2.2.2 Pipes always pass the result of the lefthand side to the first argument of the righthand side This sounds like a weird logic puzzle, but it’s not, as we can see if we look at some simple math. Let’s define a function for use in a pipe that computes the difference between two numbers: subtract &lt;- function(a, b) a - b subtract(5, 4) ## [1] 1 If we want to rewrite that as a pipe, we can write: 5 %&gt;% subtract(4) ## [1] 1 But we can’t write 4 %&gt;% subtract(5) # this is actually making subtract(4, 5) ## [1] -1 We can explicitly force the pipe to work the way we want it to by using . as the placeholder for the result of the lefthand side: 4 %&gt;% subtract(5, .) # now this properly computes subtract(5, 4) ## [1] 1 So, when you’re using pipes, make sure that the output of the lefthand side should be going into the first argument of the righthand side–this is often but not always the case, especially with non-tidyverse functions. 4.2.2.3 Pipes are a pain to type Typing %&gt;% is no fun. But, happily, RStudio builds in a shortcut for you: ctrl + shift + M. . .. … ctrl + shift + M = %&gt;% 4.2.3 The tibble() OK, with those bits of syntax out of the way, let’s talk about what tidyverse can get us. We’ll start with something simple, but nice. The augmented version of data.frame that is provided by tidyverse (via the tibble package) is called a “tibble”. soup_tbl &lt;- tibble(soups) class(soup_tbl) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; soup_tbl ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows You’ll notice a few thing about these: They print nicely: delimited number of lines, explicit printing of column types They do have class of data.frame, but also tbl and tbl_df–this means everything that works on data.frame works on them, but they also have additional capabilities Tibbles disallow row names, which is a good habit to get into but can cause some compatibility issues with non-tidyverse functions Tibbles can be created using pretty simple syntax that is parallel but simpler than data frames: tibble(x = 1:10, y = runif(10), z = letters[1:10]) ## # A tibble: 10 × 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 0.732 a ## 2 2 0.394 b ## 3 3 0.799 c ## 4 4 0.0283 d ## 5 5 0.300 e ## 6 6 0.549 f ## 7 7 0.0612 g ## 8 8 0.229 h ## 9 9 0.728 i ## 10 10 0.787 j 4.2.4 Pick columns: select() Here’s where tidyverse starts getting kind of spicy! R’s system for indexing data frames is clear and sensible for those who are used to programming languages, but it is not necessarily easy to read. A common situation in R is wanting to select some rows and some columns of our data–this is called “subsetting” our data. We might want to do it manually (when we are doing exploration) or write a function that will do it automatically (maybe as part of a conditional statement or a loop). We give an example of wanting to do this in our pseudocode example for pipes, above. But this is less easy than it might be for the beginner in R. In our pseudocode example, we wanted to pick all columns in soup that were about “minestrone”. In base R, we can get at these in a number of ways, but the most “foolproof” way is the following: soup_tbl[, grep(pattern = &quot;minestrone&quot;, x = names(soups), fixed = TRUE, value = FALSE)] # I picked the tbl version for printing ## # A tibble: 24 × 2 ## minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; ## 1 0 50 ## 2 70 80 ## 3 80 97 ## 4 20 60 ## 5 65 96 ## 6 30 80 ## 7 75 90 ## 8 70 80 ## 9 40 90 ## 10 15 65 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows Yikes! The reason this is “foolproof” is because it doesn’t rely on these columns being in a particular place, and it does not require the reconstruction of the data frame from constituent parts. (grep() is a function for finding regular expressions, a powerful, compact, difficult way to do character matching.) We could do a less foolproof but easier search in the following ways: Use numeric indexing to get the right numbers of the columns. This relies on the indexes, rather than the names containing minestrone. Extract each minestrone columns using $ and paste them together into a new data frame. 4.2.4.1 Exercise: Indexing the old-fashioned way For the exercises above, use the following chunk to actually do both. # Numeric indexing # Reconstructing from extracted vectors The select() function in tidyverse (actually from the dplyr package) is the smarter, easier way to do this. It works on data frames, and it can be read as “from &lt;data frame&gt;, select the columns that meet the criteria we’ve set.” The simplest way to use select() is just to name the columns you want! select(soup_tbl, minestrone1, minestrone2) # note the lack of quoting on the column names ## # A tibble: 24 × 2 ## minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; ## 1 0 50 ## 2 70 80 ## 3 80 97 ## 4 20 60 ## 5 65 96 ## 6 30 80 ## 7 75 90 ## 8 70 80 ## 9 40 90 ## 10 15 65 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows But select() has a bunch of helper functions that make it easy to do more foolproof selection. Look them up by running, for example, ?one_of. We’re going to use contains(), which is the easy-to-read version of the grep() command I ran above. We’ll also do this with a pipe (%&gt;%) just to practice. soup_tbl %&gt;% select(contains(&quot;minestrone&quot;)) # note that this IS quoted, because I am asking for all columns with the string &quot;minestrone&quot; ## # A tibble: 24 × 2 ## minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; ## 1 0 50 ## 2 70 80 ## 3 80 97 ## 4 20 60 ## 5 65 96 ## 6 30 80 ## 7 75 90 ## 8 70 80 ## 9 40 90 ## 10 15 65 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows We can also use logical negation (!, which we learned about last class) to, for example, get everything that isn’t “hotandsour”. soup_tbl %&gt;% select(!contains(&quot;hotandsour&quot;)) # Note that this ALSO returns the subject #, which we might or might not want ## # A tibble: 24 × 3 ## subject minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 ## 2 2 70 80 ## 3 3 80 97 ## 4 4 20 60 ## 5 5 65 96 ## 6 6 30 80 ## 7 7 75 90 ## 8 8 70 80 ## 9 9 40 90 ## 10 10 15 65 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows soup_tbl %&gt;% select(!contains(&quot;hotandsour&quot;) &amp; !contains(&quot;subject&quot;)) # we can combine logical conditions using &amp; (&quot;and&quot;) or | (&quot;or&quot;) ## # A tibble: 24 × 2 ## minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; ## 1 0 50 ## 2 70 80 ## 3 80 97 ## 4 20 60 ## 5 65 96 ## 6 30 80 ## 7 75 90 ## 8 70 80 ## 9 40 90 ## 10 15 65 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows Besides being easier to write conditions for, select() is code that is much closer to how you or I think about what we’re actually doing, making code that is more human readable. 4.2.5 Pick rows: filter() So select() lets us pick which columns we want. Can we also use it to pick particular observations? No. But for that, there’s filter(). In the pseudocode above, we stated we wanted to only get the first 10 subjects. Just like selecting columns we can do this using base R with a combination of indexing approaches. Here is, again, the most foolproof way to select the first 10 subjects in the soups data: soup_tbl[which(soup_tbl$subject %in% 1:10), ] ## # A tibble: 10 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 This works by (reading inside out), first using %in% to test whether each entry in soup_tbl$subject is in 1:10, then using which(), a function that turns a vector of logical (true/false) data into a set of integers based on the positions of the TRUE readings in the vector, and then finally using that as the index to get the rows of soup_tbl. That’s a lot to think about. There’s another, less foolproof way of doing this: Select only the first 10 rows of soup_tbl. 4.2.5.1 Exercise For the above less foolproof way, write R code to select only the first 10 rows of soup_tbl. Then, tell me why this is a less foolproof way to select the first 10 subjects. # Select only the first 10 rows of soup_tbl The filter() function works to do this in a more human way. Read it as “In , give me only the rows that meet the filter conditions.” So, for our example: filter(soup_tbl, subject %in% 1:10) # once again, notice the lack of quoting ## # A tibble: 10 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 We can combine filter() and select() using pipes (%&gt;%) to get most of our pseudocode done. soup_tbl %&gt;% # pass soup_tbl from left to right select(!contains(&quot;hotandsour&quot;)) %&gt;% # drop the &quot;hotandsour&quot; columns filter(subject %in% 1:10) # filter to only the first 10 subjects ## # A tibble: 10 × 3 ## subject minestrone1 minestrone2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 50 ## 2 2 70 80 ## 3 3 80 97 ## 4 4 20 60 ## 5 5 65 96 ## 6 6 30 80 ## 7 7 75 90 ## 8 8 70 80 ## 9 9 40 90 ## 10 10 15 65 In the code chunk below, right this so that it first selects for “minestrone”, rather than dropping “hotandsour”. What happens? # rewrite the above code chunk to first select for &quot;minestrone&quot;. What happens, and why? These are trivial examples, but they quickly become more powerful. In our tidy polyphenols dataset, we have 3 treatment variables (Catechin, PC.B2, and Chlorogenic.acid). A very common application for filter() would be to get a subset of our data that only matches certain treatment conditions: polyphenol_tbl &lt;- tibble(polyphenols) # for printing purposes polyphenol_tbl %&gt;% filter(Chlorogenic.acid == 5) # get only the observations where Chlorogenic.acid == 5. Note the double &#39;==&#39; for testing equality. ## # A tibble: 27 × 4 ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 30 5 15.2 ## 5 0.2 30 5 15.3 ## 6 0.2 30 5 14.0 ## 7 0.2 100 5 46.6 ## 8 0.2 100 5 46.1 ## 9 0.2 100 5 50.9 ## 10 30 1 5 14.0 ## # … with 17 more rows ## # ℹ Use `print(n = ...)` to see more rows 4.2.6 Make new columns: mutate() You hopefully are starting to be excited by the relative ease of doing some things in R with tidyverse that are otherwise a little bit abstruse. Here’s where I think things get really, really cool. The mutate() function creates a new column in the existing dataset. We can do this easily in base R by setting a new name for a column and using the assign (&lt;-) operator, but this is clumsy. Often, we want to create a new column temporarily, or to combine several existing columns. We can do this using the mutate() function. soup_tbl %&gt;% mutate(minestrone_liker = ifelse(minestrone1 &gt; 0 &amp; minestrone2 &gt; 0, &quot;yes&quot;, &quot;no&quot;)) ## # A tibble: 24 × 6 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 minestrone_liker ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 0 50 -50 10 no ## 2 2 70 80 -20 20 yes ## 3 3 80 97 10 37 yes ## 4 4 20 60 -80 65 yes ## 5 5 65 96 37 99 yes ## 6 6 30 80 -50 20 yes ## 7 7 75 90 40 80 yes ## 8 8 70 80 50 -10 yes ## 9 9 40 90 -60 20 yes ## 10 10 15 65 25 20 yes ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows What does the above function do? mutate() is a very easy way to edit your data mid-pipe. So we might want to do some calculations, create a temporary variable using mutate(), and then continue our pipe. polyphenol_tbl %&gt;% filter(Chlorogenic.acid == 5) %&gt;% mutate(is_outlier = ifelse(equivalents.in.gallic.acid.mg.L &gt; 2 * sd(equivalents.in.gallic.acid.mg.L), TRUE, FALSE)) %&gt;% filter(!is_outlier) ## # A tibble: 12 × 5 ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L is_outlier ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 0.2 1 5 2.79 FALSE ## 2 0.2 1 5 2.73 FALSE ## 3 0.2 1 5 2.74 FALSE ## 4 0.2 30 5 15.2 FALSE ## 5 0.2 30 5 15.3 FALSE ## 6 0.2 30 5 14.0 FALSE ## 7 30 1 5 14.0 FALSE ## 8 30 1 5 13.8 FALSE ## 9 30 1 5 13.9 FALSE ## 10 30 30 5 21.9 FALSE ## 11 30 30 5 18.0 FALSE ## 12 30 30 5 17.3 FALSE What did we do above? Was it a good idea? Most importantly, did we actually change polyphenol_tbl? Check in your work environment to see. 4.2.7 Create quick data summaries: group_by(), count(), and summarize() We are only scratching the surface of what we can do with tidyverse, and you will both be reading more about this and we’ll be returning throughout the semester as we learn more application. But here is an exploratory data analysis application that we will see here and will come back to us frequently. The group_by() function takes a data frame and groups it by whatever variable is specified. It looks for distinct values, so it will work with even numeric variables (although not well, if they are not truly grouping variables). polyphenol_tbl %&gt;% group_by(Catechin) # note no quotes ## # A tibble: 81 × 4 ## # Groups: Catechin [3] ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.7 ## 5 0.2 1 200 58.2 ## 6 0.2 1 200 69.2 ## 7 0.2 1 500 150. ## 8 0.2 1 500 162. ## 9 0.2 1 500 161. ## 10 0.2 30 5 15.2 ## # … with 71 more rows ## # ℹ Use `print(n = ...)` to see more rows By itself, this does not appear to do anything, although if you look closely at the output from printing the tibble you’ll notice that it now lists Groups: Catechin [3]. But this now allows us to use filter(), mutate(), and a host of other tidyverse functions in “grouped” mode, which means when we do all of those they will act WITHIN the group you have defined. So we can see the effect, we will first filter polyphenol_tbl, and then create a new variable with a grouped mutate() that is the average for that group. polyphenol_tbl %&gt;% filter(Chlorogenic.acid == 5) %&gt;% # this is purely for ease of printing this example group_by(Catechin) %&gt;% mutate(average_eq_gc = mean(equivalents.in.gallic.acid.mg.L)) ## # A tibble: 27 × 5 ## # Groups: Catechin [3] ## Catechin PC.B2 Chlorogenic.acid equivalents.in.gallic.acid.mg.L average_eq_gc ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.79 21.8 ## 2 0.2 1 5 2.73 21.8 ## 3 0.2 1 5 2.74 21.8 ## 4 0.2 30 5 15.2 21.8 ## 5 0.2 30 5 15.3 21.8 ## 6 0.2 30 5 14.0 21.8 ## 7 0.2 100 5 46.6 21.8 ## 8 0.2 100 5 46.1 21.8 ## 9 0.2 100 5 50.9 21.8 ## 10 30 1 5 14.0 28.4 ## # … with 17 more rows ## # ℹ Use `print(n = ...)` to see more rows But that’s not all! Now that we have the idea of a group, we can use it for all kinds of useful things. For example, the count() function will just return the number of rows in each group. Let’s use that to check if our polyphenol_tbl was from a balanced design (e.g., has the same number of observations at every treatment level and intersection): polyphenol_tbl %&gt;% group_by(Catechin, Chlorogenic.acid, PC.B2) %&gt;% # note that we can group by multiple variables count() # looks balanced, with triplicates at every treatment factor level ## # A tibble: 27 × 4 ## # Groups: Catechin, Chlorogenic.acid, PC.B2 [27] ## Catechin Chlorogenic.acid PC.B2 n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0.2 5 1 3 ## 2 0.2 5 30 3 ## 3 0.2 5 100 3 ## 4 0.2 200 1 3 ## 5 0.2 200 30 3 ## 6 0.2 200 100 3 ## 7 0.2 500 1 3 ## 8 0.2 500 30 3 ## 9 0.2 500 100 3 ## 10 30 5 1 3 ## # … with 17 more rows ## # ℹ Use `print(n = ...)` to see more rows The count() function gets way more useful when we have large data that is unbalanced, and we want to be able to easily pull counts without knowing what they are beforehand. Much more useful, but slightly more complex is the summarize() function, which applies arbitrary functions to the subsets defined by group_by(). So, say we want to find the average and standard deviation at each of the groups we defined before, instead of the count: polyphenol_tbl %&gt;% group_by(Catechin, PC.B2, Chlorogenic.acid) %&gt;% summarize(mean_eq_gc = mean(equivalents.in.gallic.acid.mg.L), sd_eq_gc = sd(equivalents.in.gallic.acid.mg.L)) ## # A tibble: 27 × 5 ## # Groups: Catechin, PC.B2 [9] ## Catechin PC.B2 Chlorogenic.acid mean_eq_gc sd_eq_gc ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.75 0.0321 ## 2 0.2 1 200 61.7 6.52 ## 3 0.2 1 500 158. 6.42 ## 4 0.2 30 5 14.8 0.700 ## 5 0.2 30 200 63.3 0.824 ## 6 0.2 30 500 155. 16.3 ## 7 0.2 100 5 47.9 2.64 ## 8 0.2 100 200 90.6 5.78 ## 9 0.2 100 500 155. 5.52 ## 10 30 1 5 13.9 0.107 ## # … with 17 more rows ## # ℹ Use `print(n = ...)` to see more rows Even I think that’s pretty cool, and I wrote the code! This can be a bit tricky to get your head around, but it shows the power of Having your data in a tidy format so it can be easily filtered Using pipes to define an analysis flow NB: One final tip: you can always run part of a piped data flow by selecting just that part and hitting “run” in the right top corner of your RStudio coding panel or hitting ctrl/cmd + enter. This is useful for testing your code flow. 4.2.8 Utilities for data management Honestly, the amount of power in tidyverse is way more than we can cover today, and is covered more comprehensively (obviously) by Wickham and Grolemund (2017), including your reading for this week. Instead, I want to draw attention to a few “quality of life” functions that tidyverse also provides that make tasks that are overly difficult in R a little easier. 4.2.8.1 rename() A common need in data analysis is renaming your columns, because they are called something clear but not machine-readable/easily typable in your data file, and they turn into a mess in R. We have that problem in our polyphenol_tbl dataset: names(polyphenol_tbl) ## [1] &quot;Catechin&quot; &quot;PC.B2&quot; ## [3] &quot;Chlorogenic.acid&quot; &quot;equivalents.in.gallic.acid.mg.L&quot; Even with tab-completion, those are a pain to deal with. Renaming these in base R is a huge pain, in fact. You have to copy them to a new column by using the &lt;- operator, then remove the original column using the [] operator with negative indices. In tidyverse, the rename() function is there to help. It works in two, equally useful ways. # New names go on the left hand side, old names on the right polyphenol_tbl %&gt;% rename(chlor = Chlorogenic.acid, pcb2 = PC.B2, cat = Catechin, eq_ga = equivalents.in.gallic.acid.mg.L) ## # A tibble: 81 × 4 ## cat pcb2 chlor eq_ga ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.2 1 5 2.79 ## 2 0.2 1 5 2.73 ## 3 0.2 1 5 2.74 ## 4 0.2 1 200 57.7 ## 5 0.2 1 200 58.2 ## 6 0.2 1 200 69.2 ## 7 0.2 1 500 150. ## 8 0.2 1 500 162. ## 9 0.2 1 500 161. ## 10 0.2 30 5 15.2 ## # … with 71 more rows ## # ℹ Use `print(n = ...)` to see more rows # Alternatively, we can rename by INDEX, with new name on the left and column # # on the right. This is easier for programming, but is less safe because you # have to be certain you&#39;re matching columns correctly. polyphenol_tbl &lt;- polyphenol_tbl %&gt;% # don&#39;t forget we have to actually save the changes if we want them to stick rename(cat = 1, pcb2 = 2, chlor = 3, eq_ga = 4) 4.2.8.2 arrange() Another thing we often want to do is sort the rows of a data table according to some criterion. Doing this in base R requires the use of sort(), which works only on vectors, not on data frames, and so requires a similar approach to the base R filtering approach we saw above. The tidyverse version is very simple: arrange() just requires that you tell it a data frame and one or more columns to sort by. soup_tbl %&gt;% arrange(hotandsour1) # show us the data in ascending order of liking for hotandsour1 ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 12 75 25 -100 -100 ## 2 21 20 100 -100 60 ## 3 23 10 100 -100 85 ## 4 4 20 60 -80 65 ## 5 13 -25 50 -75 -10 ## 6 14 20 40 -75 60 ## 7 9 40 90 -60 20 ## 8 18 40 90 -60 -15 ## 9 22 20 60 -60 -10 ## 10 1 0 50 -50 10 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows soup_tbl %&gt;% arrange(-hotandsour1) # writing &quot;-&quot; before the sorting column goes in descending order ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 8 70 80 50 -10 ## 2 7 75 90 40 80 ## 3 5 65 96 37 99 ## 4 24 10 75 35 55 ## 5 17 15 90 30 50 ## 6 10 15 65 25 20 ## 7 3 80 97 10 37 ## 8 11 -50 75 -15 50 ## 9 2 70 80 -20 20 ## 10 15 70 90 -25 -60 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows soup_tbl %&gt;% arrange(hotandsour1, -hotandsour2) # this sorts first ascending by hotandsour1, then within that descending by hotandsour2 ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 23 10 100 -100 85 ## 2 21 20 100 -100 60 ## 3 12 75 25 -100 -100 ## 4 4 20 60 -80 65 ## 5 14 20 40 -75 60 ## 6 13 -25 50 -75 -10 ## 7 9 40 90 -60 20 ## 8 22 20 60 -60 -10 ## 9 18 40 90 -60 -15 ## 10 16 20 90 -50 30 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows 4.2.8.3 relocate() R doesn’t really care about the order of columns in a data frame, but we might, for example If we have a lot of columns, they might not print properly in the console, requiring some work every time we want to check their values. We might be outputting data to a knitr::kable() or other display mode We can reorder columns using similar syntax to our base R select operations above, but this requires many steps. Again, the simple tidyverse solution is a single function: relocate(): soup_tbl %&gt;% relocate(hotandsour2) # by default the column(s) are moved to the &quot;front&quot; of the table ## # A tibble: 24 × 5 ## hotandsour2 subject minestrone1 minestrone2 hotandsour1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 10 1 0 50 -50 ## 2 20 2 70 80 -20 ## 3 37 3 80 97 10 ## 4 65 4 20 60 -80 ## 5 99 5 65 96 37 ## 6 20 6 30 80 -50 ## 7 80 7 75 90 40 ## 8 -10 8 70 80 50 ## 9 20 9 40 90 -60 ## 10 20 10 15 65 25 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows soup_tbl %&gt;% relocate(hotandsour2, .after = subject) # .before and .after (note the &quot;.&quot;) can be used to move to specific spots ## # A tibble: 24 × 5 ## subject hotandsour2 minestrone1 minestrone2 hotandsour1 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 10 0 50 -50 ## 2 2 20 70 80 -20 ## 3 3 37 80 97 10 ## 4 4 65 20 60 -80 ## 5 5 99 65 96 37 ## 6 6 20 30 80 -50 ## 7 7 80 75 90 40 ## 8 8 -10 70 80 50 ## 9 9 20 40 90 -60 ## 10 10 20 15 65 25 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows 4.2.9 Reading data tidily: read_*() We started today talking about reading data. The readr package in tidyverse gives a set of tidy options for reading data. They have nice defaults and (usually) require less user input. The most common one is read_csv()–note its similarity to read.csv(), except for the substitution of “_” in the name (this is common in tidyverse equivalent functions). read_csv(file = &quot;Files/Week 4/soup pilot data.csv&quot;) ## Rows: 24 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): subject, minestrone1, minestrone2, hotandsour1, hotandsour2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 24 × 5 ## subject minestrone1 minestrone2 hotandsour1 hotandsour2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 50 -50 10 ## 2 2 70 80 -20 20 ## 3 3 80 97 10 37 ## 4 4 20 60 -80 65 ## 5 5 65 96 37 99 ## 6 6 30 80 -50 20 ## 7 7 75 90 40 80 ## 8 8 70 80 50 -10 ## 9 9 40 90 -60 20 ## 10 10 15 65 25 20 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows Note that the function automatically makes a tibble, not just a data frame, which may or may not be desired. By default, they also look for headers and treat text normally. In general, I tend to use these functions as my default. 4.2.9.1 Solving (some) messy spreadsheet problems: readxl package However, I promised we’d solve some file problems before the end of the day. Even though Excel can read .csv files, they are not the same thing as spreadsheet files. Specifically, they have no real notion of “index” in the way that Excel files do (the whole “A1” scheme for referencing cells). But the author of the readxl package for R saw the value in those indices for dealing with messy data storage. So here we are going to use readxl::read_excel() to read in Michael Wesolowski’s thesis data WITHOUT having to edit the underlying file: # Remember to run this if you haven&#39;t installed readxl install.packages(&quot;readxl&quot;) library(readxl) read_excel(&quot;Files/Week 4/Thesis Data Mastersheet.xls&quot;, sheet = &quot;Tomato&quot;, range = &quot;A6:J78&quot;) ## # A tibble: 72 × 10 ## Time …¹ Trial…² Repli…³ Plate…⁴ Count Addit…⁵ Total…⁶ Total…⁷ Log C…⁸ Avera…⁹ ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 A 1 352 0.01 1.10e-4 3.20e6 6.51 6.60 ## 2 &lt;NA&gt; 1 A 2 51 0.001 1.10e-5 4.64e6 6.67 NA ## 3 &lt;NA&gt; 1 B 1 52 0.001 1.10e-5 4.73e6 6.68 NA ## 4 &lt;NA&gt; 1 B 2 75 0.001 1.10e-5 6.82e6 6.83 NA ## 5 &lt;NA&gt; 1 C 1 53 0.001 1.10e-5 4.82e6 6.68 NA ## 6 &lt;NA&gt; 1 C 2 55 0.001 1.10e-5 5.00e6 6.70 NA ## 7 &lt;NA&gt; 2 A 1 62 0.001 1.10e-5 5.64e6 6.75 NA ## 8 &lt;NA&gt; 2 A 2 66 0.001 1.10e-5 6.01e6 6.78 NA ## 9 &lt;NA&gt; 2 B 1 251.5 0.01 1.10e-4 2.29e6 6.36 NA ## 10 &lt;NA&gt; 2 B 2 267.5 0.01 1.10e-4 2.43e6 6.39 NA ## # … with 62 more rows, and abbreviated variable names ¹​`Time (sec)`, ## # ²​`Trial #`, ³​Replicate, ⁴​`Plate #`, ⁵​`Additional Dilution`, ## # ⁶​`Total Dilution`, ⁷​`Total CFU/mL`, ⁸​`Log CFU/mL`, ⁹​`Average Log` ## # ℹ Use `print(n = ...)` to see more rows That’s pretty cool! We still have problems with some of the empty cells, and in fact we will be asking you to fix that as part of the assignment this week using the fill() function from tidyverse. Should be a good time! 4.3 Review of functions Finally, I want to leave some time today, if we have any, to discuss the material from last week on functions. This is a huge topic, and I am sure you still have some questions. Let’s have some answers! References References "],["exploring-your-data-visually.html", "5 Exploring your data visually 5.1 Reasons for developing competency with data visualization 5.2 Plotting in base R 5.3 The ‘grammar of graphics’: ggplot2 5.4 Return to data wrangling - long vs wide References", " 5 Exploring your data visually One of the major strengths of R is that it provides a very strong platform for data visualization or “dataviz”, if you’re a little bit hurried and willing to be obnoxious. Data visualization, in the last several years, has been an increasing subject of emphasis in statistical and data science classes, a focus of businesses (e.g., the development of specific platforms like Tableau), and a general requirement for working with data. As sociologist Kieran Healy (2019) writes: You should look at your data. Graphs and charts let you explore and learn about the structure of the information you collect. Good data visualizations also make it easier to communicate your ideas and findings to other people. Beyond that, producing effective plots from your own data is the best way to develop a good eye for reading and understanding graphs—good and bad—made by others, whether presented in research articles, business slide decks, public policy advocacy, or media reports. Healy is mirroring advice from one of the main influences in modern, effective statistics and data science: John Tukey (inventor of, among many things, the box plot). As historians Michael Friendly and Howard Wainer write (Friendly and Wainer 2021): John Tukey… revolution[ized] the field of statistics with the idea that the purpose of data analysis was insight, not just numbers, and that insight–seeing the unexpected–more often came from drawing pictures than from proving theorems or deriving equations. John Tukey, from Wikipedia Thus, while dataviz as a subject of increasing concentration is new, the discipline itself isn’t. Famous examples of effective and persuasive data visualization can be traced even farther back than Tukey (unsurprisingly). One of the most famous, in fact, is Florence Nightingale’s work with “rose charts” to document causes of death in the Crimean War in the 1850s, which were credited with causing the British army to modernize sanitation standards. The story (which is fascinating) is described very well in an episode of the 99% Invisible podcast: Florence Nightingale’s famous “rose charts”, describing causes of mortality in the Crimean theater in the 1850s. We can go back even further (almost to prehistory, according to Friendly and Wainer 2021), but we’ll end with just one more example: what data visualization guru Edward Tufte has called the greatest single graphic in history. The visualization, by Charles Joseph Minard, shows Napoleon’s disastrous invasion of Russia in 1812, in which he lost the majority of his army to the Russian winter. Charles Joseph Minard’s data visualization of Napoleon’s invasion of Russia, detailing army size during invasion and retreat, temperature, and geography and time. These are classic examples of data visualization that are frequently cited in histories of the subject. But they have survived for a number of reasons: they are clear visualizations of the underlying data, they are aesthetically engaging enough to draw in the reader, and they use the combination of aesthetics and information to make arguments. These are what we want to do when we make data visualizations, which should ideally have these attributes. As a note, data visualization is related to but not identical with recently popular tools for easily making “infographics”–the latter tend to combine dataviz with design elements to accomplish some of the same points, but are usually meant to be complete documents or other communicative tools on their own. For our purposes we are going to focus on data visualization as a way to make sense of our data. XKCD on infographics. This week we’re going to focus on the how of plotting data in R. This means I’m not going to try to teach principles for effective dataviz design (a topic I am only marginally competent with), but rather how to turn your data into dataviz. 5.1 Reasons for developing competency with data visualization There are several key reasons we might want to develop competency with data visualization, and before we get started with the how I think it’s worth mentioning these motivating factors. 5.1.1 To help you explore your data As the Healey quote above implies, one of the main reasons to “look at your data” is to make sure you understand what you’re doing. Looking at your data is a great quick check to make sure that, for example, the linear model (regression or ANOVA) you’re fitting is actually sensible for the data you’ve observed. This is why, for example, examining QQ-plots or residuals is such common practice taught in every statistics class. But looking at your data can be even more valuable: our ability to find patterns in visual information is much stronger than our ability to find similar patterns in written information (like tables of data). This can be a disadvantage–we will happily find patterns in truly random data–but it is of great use to us as researchers and scientists. Looking at data is an invaluable way for us to come up with strategies for data analysis, new hypotheses, and even new research questions. For example, what patterns can we see in this table of the data we just plotted (a subset, obviously)? x y1 y2 0.0000000 -1.3214703 -2.5353760 0.1010101 -0.8269770 -7.5286794 0.2020202 0.4996656 -1.2341072 0.3030303 0.5917504 5.1983819 0.4040404 0.7527332 -2.2728104 0.5050505 -0.1505266 8.3169781 0.6060606 1.1242934 5.6965722 0.7070707 -1.9397261 7.8641251 0.8080808 0.4597118 9.6065223 0.9090909 0.9153580 0.9753971 When visualizing data for yourself, the goal is to quickly and accurately be able to make plots that you want to see, so that you’re not fiddling with options or struggling to actually see the data. You aren’t looking for “publication quality” visualization–you want to be able to look at your data clearly and quickly. 5.1.2 For communication to stakeholders This is the main reason we mostly think about doing dataviz: we want to create plots for use in presentations or publications, so that stakeholders–your committee, reveiwers at a journal, peers at a scientific conference–can understand your data quickly and clearly. Below, we can quickly clean up the fake data I made to demonstrate dataviz for yourself to be something we’d be somewhat happier putting into a journal. In this case, you care a lot more about the aesthetics. You only get a limited number of plots shown to the audience’s eyes, so you want to be both impactful and attractive. While your readings (in particular Healy 2019) give some advice on this, developing expertise in what is the most effective is a matter of craft–you will see examples as you read papers, and learn what works and doesn’t. 5.1.3 For argumentation This might more accurately be a subcategory of communicating to stakeholders, but often you will want to use your data visualization to effectively make a point. This is where things get a little tricky, and we will be tackling questions of data literacy and ethics a little later in the class, although if you’re interested at all in this topic it may be worth investigating some of the readings in Calling Bullshit (Bergstrom and West 2021) and Data Feminism (D’Ignazio and Klein 2020). Briefly, we want to make visualizations that, like Florence Nightingale’s, make our argument effectively–we want the interpretation that others to take away from them to be the interpretation we ourselves have made. But we have to be cautious to not be fooling ourselves and others with bad, misleading, or even unethical choices about how we select our data and visualize it in order to advance our agenda. There are several examples of this given in the references above, as well as a number in Kieran Healy’s work. 5.2 Plotting in base R With all that in mind, let’s get into how to plot data in R. To explore this, we’re going to use some data from Dr. Sihui Ma’s dissertation research, in which she quantified the amino acid composition of a number of different varieties of apples in Virginia. The abstract for the paper in which these data were published (Ma et al. 2018) is given here: Amino acids and ammonium ions constitute the yeast assimilable nitrogen naturally present in apple juice, with free amino acids being the major constituent. Little information is available on the extent to which free amino acid composition in apple (Malus × domestica Borkh.) juice varies among juices used for fermentation. Twenty amino acids were quantified by UPLC-PDA in juices from 13 apple cultivars grown in Virginia with potential use in cider making. The relative amino acid profile was significantly different among the apple juices evaluated. The total amino acid concentration ranged from 18 mg/L in Blacktwig juice to 57 mg/L in Enterprise juice. l-Asparagine, l-aspartic acid and l-glutamine are the principal amino acids observed in most apple juices. Our results indicate that the relative concentration of amino acids in apples is different from that found in Vitis vinifera wine grapes, which are rich in l-proline and l-arginine. The impact of these differences on fermentation of apple juice by wine yeast strains warrants further research, as the amino acid composition of grape juice impacts wine quality both directly and indirectly via yeast metabolism. These results will inform future research on yeast metabolism and nitrogen management during cider fermentation. And here is a sample of what the data look like (without cleaning or wrangling): apples &lt;- read_csv(&quot;Files/Week 5/apple varieties.csv&quot;) head(apples) %&gt;% knitr::kable() Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala GABA Pro Cys Lys Tyr Met Val Ile Leu Phe Arkansas Black Rep_1 0.62 10.42 0.68 1.60 0.36 0.09 2.92 2.37 0.20 0.72 0.35 0.26 0.22 0.43 0.67 0.98 0.55 1.59 1.24 3.25 Blacktwig Rep_1 0.77 2.32 0.54 0.00 0.24 0.00 1.96 1.03 0.18 0.47 0.15 0.27 0.28 0.52 0.83 1.15 0.62 1.64 1.51 3.70 Empire Rep_1 0.46 21.20 0.43 1.01 0.27 0.09 5.88 1.49 0.28 1.92 0.51 0.67 0.21 0.44 0.65 1.74 0.52 1.50 1.13 3.20 Enterprise Rep_1 0.52 34.75 0.90 2.78 0.24 0.08 5.18 2.49 0.33 1.71 0.29 0.37 0.22 0.41 0.66 0.84 0.63 1.39 1.18 2.66 Field Red Rep_1 0.85 4.02 0.29 0.00 0.41 0.00 1.87 1.29 0.19 0.33 0.15 0.00 0.40 0.55 1.03 1.09 1.72 1.82 1.67 13.37 Golden Delicious Rep_1 0.56 1.01 0.00 0.00 0.34 0.00 1.76 0.98 0.32 0.26 0.00 0.00 0.33 0.49 0.76 0.92 1.50 1.59 1.44 11.83 For reference, the numerical data represent concentrations of amino acids measured as mg/L. We are going to start with “one-line” plotting commands: simple, base R commands that produce commonly needed data visualizations. We’re going to speed through these in base R and move on to the much more powerful, flexible, and easier (although initially bewildering) ggplot2. 5.2.1 Histograms with hist() One of the most common types of data visualizations is a histogram. Histograms, briefly, are a common type of data visualization that show the approximate distribution of the data. In plain English, histograms in general try to show how many times a particular range of values are observed in your data. The recipe for making a histogram is as follows: Pick the value you want to visualize, call it \\(x\\) Define “bins” of (usually) equal width to count data “in”: for example, if your total range of values that \\(x\\) can take is \\([1,100]\\), you might define 10 bins as \\(1-10, 11-20, ..., 91-100\\). Note that this step is something we will almost always let software do for us–we’ll just pick the number of bins we want. Count the number of times that \\(x\\) is in each bin, and draw a bar graph with that count, with your bins on the x-axis and your counts on the y-axis. Histograms are common because they are a good way to see how our data is distributed. We might be interested to know, for example, whether the data is skewed (whether it is mostly to one or the other side of the histogram) and what counts are most common. To draw histograms in R, we can use the hist() function. By default, hist() just requires a single vector of your data (\\(x\\) in the example above). So, if we are interested in the distribution of alanine concentrations across all of our apple species, we can write the following command: apples %&gt;% # start with our data .$Ala %&gt;% # get the alanine concentrations hist(x = .) # plot the histogram; note that I don&#39;t have to write &quot;x = .&quot; We can see immediately that most apples have very little or no alanine, with a few varieties expressing quite a bit more. Note that we didn’t define our bins–R picked those for us automatically. But we might prefer to set them differently. In this case, we can use the breaks = argument in the hist() function. Go ahead and use ?hist to see what options there are for this. You’ll see there are a few ways to do this, but the easiest is just to give it a single number for the number of breaks we want, and let R figure out the bins. apples %&gt;% # start with our data .$Ala %&gt;% # get the alanine concentrations hist(x = ., breaks = 4) # here we&#39;re reducing the number of breaks, which is probably bad Now, it might be nice to see the distribution of concentrations of all amino acids across apples. We will be returning to the subject of tidy data later today, but for now here’s an easy example: tidy_apples &lt;- apples %&gt;% pivot_longer(names_to = &quot;amino_acid&quot;, values_to = &quot;concentration&quot;, -c(Variety, Rep)) # This makes a single row for each combination of apple, rep, and AA head(tidy_apples, 10) %&gt;% # head() returns just the first n rows of a data frame so as not to overwhelm the console knitr::kable() Variety Rep amino_acid concentration Arkansas Black Rep_1 His 0.62 Arkansas Black Rep_1 Asn 10.42 Arkansas Black Rep_1 Ser 0.68 Arkansas Black Rep_1 Gln 1.60 Arkansas Black Rep_1 Arg 0.36 Arkansas Black Rep_1 Gly 0.09 Arkansas Black Rep_1 Asp 2.92 Arkansas Black Rep_1 Glu 2.37 Arkansas Black Rep_1 Thr 0.20 Arkansas Black Rep_1 Ala 0.72 tidy_apples %&gt;% .$concentration %&gt;% # here we select the new &quot;concentration&quot; variable we made hist(breaks = 15) Finally, note that the title of our plot is pretty wonky: “Histogram of .”. THis is because the pipe–%&gt;%–doesn’t pass the column name in a way that works for hist(), just the data. For purpose #1 of dataviz, this is fine–we know what we just did. But if we wanted to share this with someone, we might want to give it a better title. We can do that with the main = argument in hist(). tidy_apples %&gt;% .$concentration %&gt;% hist(breaks = 15, main = &quot;Histogram of amino acids (mg/L) in 13 apple varieties&quot;) 5.2.2 Boxplots with boxplot() Often, though, we want to know more about our variables than simply the frequency with which they take on certain values. One of the simplest but most effective tools for this task is the boxplot, invented by none other than John Tukey. Boxplots concisely communicate much (although not all) of the same information as histograms, but can much more easily accommodate displaying multiple variables at once. It will be helpful to look at some boxplots first while we talk about what they’re displaying: tidy_apples %&gt;% boxplot(concentration ~ amino_acid, data = .) # Note that here we have to explicitly write data = ., because data is NOT the first argument Boxplots display a few different statistics (points of information) about the data they describe: The median, represented by a center line. The 25th and 75th percentiles of the data, represented by the upper and lower boxes. The lower and upper “plausible” bounds of the data, represented by the upper and lower “whiskers” (the lines outside the boxes). In base R, these are defined as \\(1.5 \\cdot IQR\\) above or below the upper and lower bounds (respectively), where the Interquartile Range is defined as the difference between the 25th and 75th percentiles, as above. Outliers, which are defined as points with values outside the whiskers. Thus, boxplots give a quick picture of how the data are distributed, including information about skewness (the data are skewed if one or the other box and or whisker are much larger). Boxplots also have the advantage that they allow easy comparison: in the figure above we show boxplots for all amino acids measured in the study, across all apple varieties–that is, if we look at alanine, the values being included in the boxplot include all 13 varieties of apple (and their replicates). We can see from this kind of side-by-side comparison that it appears apples have relatively large amounts of asparagine and phenylalanine, as well as aspartate (unsurprisingly), but lower levels of all other amino acids. We do still see a fair amount of variation, however. 5.2.2.1 Formulas! You might have noticed that we introduced some new notation into our plot: concentration ~ amino_acid. This is an example of an R formula object. You can learn more by reading the help page at ?formula, but it’s honestly kind of abstruse. Essentially, we can read ~ in R as meaning “depends on” or “is related to”. Formulas are always directional–we talk about the left-hand side (LHS) and right-hand side (RHS). Formulas get used a lot in R statistical modeling, and we’ll also see them pop up all over the place, such as in the function xtable() (for plotting contingency tables). Right now, you just need to read them as specified above: concentration ~ amino_acid is saying that the concentration (LHS) is dependent on the specific amino_acid (RHS), wich is a categorical variable telling us which amino acid is which. This also implies we might use boxplots to explore (slice?) the data in different ways. For example, we could ask “Is there apparent variation in total amino acid content by apple variety?” tidy_apples %&gt;% boxplot(concentration ~ Variety, data = .) # note the new formula: concentration ~ Variety While there are a large number of outliers here, it seems like on a first glance that the total amino acid content per apple isn’t all that different. We might still want to investigate specific amino acid distributions for each type of apple: tidy_apples %&gt;% filter(amino_acid %in% c(&quot;Asp&quot;, &quot;Asn&quot;, &quot;Phe&quot;)) %&gt;% # let&#39;s look at the most abundant AAs boxplot(concentration ~ Variety, data = .) This starts to look like we might be seeing more variation. But boxplots may not be the best tool to drill deeply into this. As a closing note, we can always set options for title, axis labels, etc just as we did with hist(): tidy_apples %&gt;% filter(amino_acid %in% c(&quot;Gln&quot;, &quot;Gly&quot;, &quot;Pro&quot;)) %&gt;% boxplot(concentration ~ Variety, data = ., main = &quot;Concentration of glycine, glutamine, and proline in 13 apple varieties&quot;, xlab = &quot;Apple variety&quot;, ylab = &quot;Concentration (mg/L) of glycine, glutamine, or proline&quot;) 5.2.3 Generic plot() Both hist() and boxplot() are functional shortcuts for the generic R plot() method. In a very general sense, plot() initializes a graphical output and then accepts new objects to put into it. hist(), for example, has a bunch of default options designed to draw columns, set the range on the x- and y-axes correctly, etc. We can use plot() directly in two ways. The most common is that you’ll call it directly on the results of some other function or analysis. For example, if we might run an ANOVA on our apple data in order to ask if, for phenylalanine, concentrations are dependent on apple variety: tidy_apples %&gt;% filter(amino_acid == &quot;Phe&quot;) %&gt;% aov(concentration ~ Variety, data = .) %&gt;% anova() %&gt;% plot() …this doesn’t get us what we want, at all, but it does get us a plot! In fact, what it’s trying to do here is to plot every variable against every other variable in what is known commonly as “pair-plot”. But you can see that here we just let R decide on the plot it wants to make with the object we give it. While this may occasionally be what you want to do, generally we will use plot() to systematically build up a plot. For example, let’s say we want to see if there’s some kind of relationship between aspartate and asparagine: plot(x = apples$Asn, y = apples$Asp) # Note that here we can&#39;t use the %&gt;% easily, because we need &quot;x = &quot; AND &quot;y = &quot;. This is pretty bare bones, but it looks like there is a relationship. Maybe we think it would be prettier if we added lines connecting each (x, y) pair to the next one in order to see if there’s a trend: plot(x = apples$Asn, y = apples$Asp, type = &quot;b&quot;) # &quot;b&quot; = BOTH a line and the points Yikes! That looks terrible! Unfortunately, that’s kind of common for any kind of computer program, because our first rule here applies: this is powerful and fast, but also does exactly what we ask, not what we want. In this particular case, that’s because our pairs aren’t ordered: apples %&gt;% select(Variety, Asn, Asp) %&gt;% head(10) %&gt;% knitr::kable() Variety Asn Asp Arkansas Black 10.42 2.92 Blacktwig 2.32 1.96 Empire 21.20 5.88 Enterprise 34.75 5.18 Field Red 4.02 1.87 Golden Delicious 1.01 1.76 Granny Smith 8.76 1.41 Newtown Pippin 26.13 5.14 Northern Spy 2.52 1.60 Rome 1.57 1.23 If we really want that kind of plot (perhaps our lines are meant to show the trend clearly?) we need to order our points into the order we expect them to show up on the plot: apples %&gt;% select(Variety, Asn, Asp) %&gt;% arrange(Asn, Asp) %&gt;% # we can sort our data by Asn, then by Asp with(., plot(x = Asn, y = Asp, type = &quot;b&quot;)) # the with() function creates a new environment and lets us use %&gt;% here Just as with hist() and boxplot(), we can also easily change titles and axis labels for clarity: plot(x = apples$Asn, y = apples$Asp, main = &quot;Relationship of Asn and Asp in 13 apple varieties&quot;, xlab = &quot;Asparagine (mg/L)&quot;, ylab = &quot;Aspartate (mg/L)&quot;) 5.2.3.1 Adding points() But beyond just changing labels, we can do a lot of construction on our generic visualizations from plot(). R uses an additive approach to plotting. Once we have a plot, we have the option of adding the add = TRUE argument to any plotting functions to (attempt to) plot our new elements right onto the previous plot. We might want to do this to: Add some new data to the plot (like points we hadn’t plotted before) Add a fitted model (like a curve representing a regression) to the plot Add some annotations to the plot We will go over the first two use cases here, but because we’re going to be emphasizing tidy plotting using ggplot2 for most of the class, we won’t be concentrating on every possible way to implement plot options in base R–there are just too many! But know that you can do many things that are very powerful in R plotting, even without learning ggplot2 (which is easier, anyway). Now, imagine we wanted to plot the relationship between asparagine and aspartate and phenylalanine. The plot() function only accepts one x = and one y = argument, so how can you plot both? Assuming we want to use asparagine as our x variable, and plot it against aspartate and against phenylalanine, we might take an approach like the following plot(x = apples$Asn, y = apples$Asp, xlab = &quot;Asparagine (mg/L)&quot;, ylab = &quot;Concentration (mg/L)&quot;) # make our previous plot points(x = apples$Asn, y = apples$Phe, col = &quot;red&quot;) # add points with y-values from the phenylalanine concentrations legend(x = &quot;bottomright&quot;, legend = c(&quot;aspartate&quot;, &quot;phenylalanine&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = 1) Now, I will be the first to agree that this isn’t a particularly pretty or useful plot, but it did do what we asked. We can see that the relationship between phenylalanine concetration and asparagine concentration seems much weaker than between asparagine and aspartate (which makes sense). You might also be noting the legend() function here–this is one base R way of adding annotation to a plot. While the syntax is pretty arcane, I think it will illustrate some of the ways that R thinks if I describe what’s happening: x is a placeholder for a text string to say where to put the legend The legend = argument asks what entries you want in the legend col = lets you set the color of each legend entry, in the same order they are in legend = pch = lets you set the shape of your legend entries, in the same order they are in legend =. Because we didn’t make different shapes in this plot, both are shape “1”, which is equivalent to the open circle. You can learn more about the shapes and colors (and other attributes, like size) that you can set for points in R by looking at ?points. 5.2.3.2 Adding lines with abline() Let’s continue with this example. We want to know whether there is a relationship between asparagine concentration and both aspartate and phenylalanine. It appears there is some kind of relationship between asparagine and aspartate: higher values of asparagine concentration generally are associated with higher values of aspartate. The same does not appear to be true for phenylalanine. As you know from stats class, this is a classic linear regression question. We’ll learn more about this in later classes, but the way we run linear regression in R is using the lm() function. We are going to ask for simple linear regressions describing our research question, and then plot the resulting regression lines onto our plot–we are going to add lines to this plot. asp_model &lt;- lm(Asp ~ Asn, data = apples) # we will save our model. Note the formula syntax again! phe_model &lt;- lm(Phe ~ Asn, data = apples) # What is different about this line? asp_model ## ## Call: ## lm(formula = Asp ~ Asn, data = apples) ## ## Coefficients: ## (Intercept) Asn ## 1.6214 0.1289 phe_model ## ## Call: ## lm(formula = Phe ~ Asn, data = apples) ## ## Coefficients: ## (Intercept) Asn ## 9.3494 -0.2581 These models are complex R objects (what happens if you use typeof(), str(), and class() on asp_model?), but their default print value is to give you the coefficients of the regression model you’ve fitted. For example, the linear model fit is \\(\\hat{Asp} = 0.1288706 \\cdot Asn + 1.6213577\\). We can directly access these using the coefficients() function, if we want to, and also get some more info using the summary() function. But we are most interested in plotting! The abline() function adds a line to any plot in the \\(y = a + b \\cdot x\\) format common to algebra, and because R is built around statistics it turns out that abline() can directly plot linear models. # First we will recreate our previous plot plot(x = apples$Asn, y = apples$Asp, xlab = &quot;Asparagine (mg/L)&quot;, ylab = &quot;Concentration (mg/L)&quot;) # make our previous plot points(x = apples$Asn, y = apples$Phe, col = &quot;red&quot;) # add points with y-values from the phenylalanine concentrations legend(x = &quot;bottomright&quot;, legend = c(&quot;aspartate&quot;, &quot;phenylalanine&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = 1) # And now we can use abline() to add our two model fits abline(asp_model, col = &quot;black&quot;) # make sure the color matches the points! abline(phe_model, col = &quot;red&quot;) Oh no! That looks terrible for two reasons. The phenylalanine fit goes over the legend. This tells us something about how R is building these plots: it puts each layer on top of the previous ones. So, we can reorganize the order in which we add elements by calling legend() last in order to fix this. The phenylalanine fit doesn’t look like it fits our plot very well, does it? But in fact, it does! This is a lesson, once again, in how R does what you ask, not what you want. What happened? Well, the we never actually looked at just a plot of phenylalanine ~ asparagine (in R formula syntax): with(apples, plot(x = Asn, y = Phe, col = &quot;red&quot;)) # I am using the with() syntax as a demonstration again When we look at this plot it is clear that a cluster of points was outside of the range of our original plot. This is because, in base R, plot() draws a “canvas” or “area” based on the original data–all subsequent additions are fit into (or chopped out of) this plot. So R happily made us a plot of aspartate ~ asparagine and then, when we asked to add phenylalanine, it just didn’t put in the points outside that area! Here’s what happens if we reverse the whole sequence: plot(x = apples$Asn, y = apples$Phe, col = &quot;red&quot;, xlab = &quot;Asparagine (mg/L)&quot;, ylab = &quot;Concentration (mg/L)&quot;) points(x = apples$Asn, y = apples$Asp, col = &quot;black&quot;) abline(asp_model, col = &quot;black&quot;) abline(phe_model, col = &quot;red&quot;) legend(x = &quot;bottomright&quot;, legend = c(&quot;aspartate&quot;, &quot;phenylalanine&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = 1, lty = 1) Now we see the whole story. Of course, part of that story is that a linear fit is probably not sensible for this particular model, but now we at least can see why these fits are being produced. In many ways this workflow is a good demonstration of the use of dataviz in order to explore your own data. Step by step we encountered unexpected things and used our tools in R to understand where they came from. Of course, at the same time we learned about how to render plots in R. 5.2.4 Saving plots to file Once you have a plot in R you might want to know how to save it. Of course, in R Markdown the plot gets rendered into your knitted file, but how can you save the file to disk, perhaps to use in a presentation or a paper? One way that you should NOT do this is by using the RStudio Export menu in the Plot pane: RStudio image export interface. For whatever reason, this is one of the weakest utilities built into RStudio. It rarely outputs in high enough resolution for modern usage, and gives you very little control over size and aspect ratio. Don’t use it. Instead, unfortunately, you will have to use R’s somewhat arcane (to non-programmers) graphic device interface. By default, RStudio outputs to the Plot Pane as the “graphic device”. Instead, if you want to write an image to disk, you need to tell it to use a new graphic device by telling R to do so explicitly, using one of the device commands: jpeg(), png(), and less commonly tiff() or bmp(). Generally you will want to use the PNG format for modern software. To do so, you follow the following recipe: Create a new graphic device by calling one of the functions (like jpeg()) with the appropriate options. This makes an empty image file on your disk. Run the plotting functions you normally want to run. Use the function dev.off() to close the graphic device, stop editing the image file, and revert to plotting in R. So, for example, the following code will write our plot to a PNG with the name “asn_vs_asp_and_phe.png”. # Open a new graphic device png(filename = &quot;asn_vs_asp_and_phe.png&quot;, width = 8, height = 6, units = &quot;in&quot;, res = 300) # All of this code is just our plot from before plot(x = apples$Asn, y = apples$Phe, col = &quot;red&quot;, xlab = &quot;Asparagine (mg/L)&quot;, ylab = &quot;Concentration (mg/L)&quot;) points(x = apples$Asn, y = apples$Asp, col = &quot;black&quot;) abline(asp_model, col = &quot;black&quot;) abline(phe_model, col = &quot;red&quot;) legend(x = &quot;bottomright&quot;, legend = c(&quot;aspartate&quot;, &quot;phenylalanine&quot;), col = c(&quot;black&quot;, &quot;red&quot;), pch = 1, lty = 1) # And this code turns off that connection and returns us to the RStudio Plot Pane dev.off() ## quartz_off_screen ## 2 You’ll see that, unlike our previous code chunks, this one doesn’t print anything in the Markdown. That’s because it has instead redirected the plotting output to the file we specified. If we forgot to run dev.off(), we’d end up continuing to send out plotting output to that same file–definitely not something we want to do! While that seems like a lot of work, in general once you understand the steps it becomes somewhat natural. All of the arguments for png() have to do with the size and resolution of the plot–you can learn about them by reading ?png. 5.3 The ‘grammar of graphics’: ggplot2 We’ve now learned a bit about how base R treats plotting, although we’ve really only scratched the surface. By using plot() on many different outputs from different R modeling functions (like lm(), etc), you will be able to automatically make many default plots. But as you can see the default, while very customizable, is also somewhat arcane and difficult to use. it also doesn’t have great default settings–you’ll notice that our plots, while clear, are not particularly compelling looking. This is where ggplot2 comes in. The name for this package, which is part of the tidyverse, comes from the “Grammar of Graphics”, a conceptual framework for theorizing the elements of a visualization. These are explained in some detail in Chapter 3 and Chapter 24 of R for Data Science (Wickham and Grolemund 2017), and Kieran Healy’s book (Healy 2019), while not going into the theory, gives a detailed explanation. In my typical, vague and hand-wave-y fashion, here is my summary of the innovation that ggplot2 encodes and makes accessible to the average R user: We can break down the elements of a data visualization systematically. Mappings, which connect data to visual elements, like coordinates on a graph, colors, or other aesthetic elements of a plot Geoms, which dictate how to take the mapped elements and render them into visual elements, such as a boxplot, points, or lines Visual aspects of the plot which are unrelated to data, such as the overall theme, background color, font choice, etc Treating data visualizations as cousins to data frames or tibbles lets us easily manipulate them without having to restart them from scratch when we make mistakes or discover new ways we want to treat the plot or data. Breaking apart these elements allows us to swap parts in and out without redoing all our previous work, for example we could change the x-axis scale without changing anything else, or change the color of our points or our title. 5.3.1 Anatomy of a ggplot() To use ggplot2, you’ll have to load the package, which you have already done if you ran library(tidyverse) at the start of this R Markdown. The basic command structure looks more complicated than the plot() functions we’ve been using so far: # The ggplot() function creates your plotting environment. We usually save it to a variable in R so that we can use the plug-n-play functionality of ggplot without retyping a bunch of nonsense p &lt;- ggplot(mapping = aes(x = &lt;a variable&gt;, y = &lt;another variable&gt;, ...), data = &lt;your data&gt;) # Then, you can add various ways of plotting data to make different visualizations. p + geom_&lt;your chosen way of plotting&gt;(...) + theme_&lt;your chosen theme&gt; + ... In graphical form, the following diagram (from VT Professor JP Gannon) gives an intuition of what is happening: Basic ggplot mappings. Color boxes indicate where the elements go in the function and in the plot. To use ggplot(), your data will usually have to be in “tidy” format: that is, long rather than wide. We’ve already talked about Wickham’s (2017) definition, and Kieran Healy (2019) takes another crack at it here. In our example, the original apples data frame is not tidy: head(apples) %&gt;% knitr::kable() Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala GABA Pro Cys Lys Tyr Met Val Ile Leu Phe Arkansas Black Rep_1 0.62 10.42 0.68 1.60 0.36 0.09 2.92 2.37 0.20 0.72 0.35 0.26 0.22 0.43 0.67 0.98 0.55 1.59 1.24 3.25 Blacktwig Rep_1 0.77 2.32 0.54 0.00 0.24 0.00 1.96 1.03 0.18 0.47 0.15 0.27 0.28 0.52 0.83 1.15 0.62 1.64 1.51 3.70 Empire Rep_1 0.46 21.20 0.43 1.01 0.27 0.09 5.88 1.49 0.28 1.92 0.51 0.67 0.21 0.44 0.65 1.74 0.52 1.50 1.13 3.20 Enterprise Rep_1 0.52 34.75 0.90 2.78 0.24 0.08 5.18 2.49 0.33 1.71 0.29 0.37 0.22 0.41 0.66 0.84 0.63 1.39 1.18 2.66 Field Red Rep_1 0.85 4.02 0.29 0.00 0.41 0.00 1.87 1.29 0.19 0.33 0.15 0.00 0.40 0.55 1.03 1.09 1.72 1.82 1.67 13.37 Golden Delicious Rep_1 0.56 1.01 0.00 0.00 0.34 0.00 1.76 0.98 0.32 0.26 0.00 0.00 0.33 0.49 0.76 0.92 1.50 1.59 1.44 11.83 Each row contains data about 1 apple variety in 1 analytical rep, but there are multiple amino acid observations in each row–we can say that the columns are an unrecorded variable–amino_acid–which is spread “wide” across those columns. In contrast, our tidy_apples dataset has only one observation per row: head(tidy_apples) %&gt;% knitr::kable() Variety Rep amino_acid concentration Arkansas Black Rep_1 His 0.62 Arkansas Black Rep_1 Asn 10.42 Arkansas Black Rep_1 Ser 0.68 Arkansas Black Rep_1 Gln 1.60 Arkansas Black Rep_1 Arg 0.36 Arkansas Black Rep_1 Gly 0.09 For example, in row 3 of this dataset, we have a single observation: the concentration of amino_acid = Ser for Rep = 1 of Variety = Arkansas Black is 0.68 mg/L. This kind of data is easier to do operations like filtering, selecting, and otherwise manipulating, and, as we’ll see, it is easier to plot, especially ing ggplot2. However, this doesn’t mean wide data is “bad” or “wrong”–it is often much more convenient for humans to read and for a number of multivariate analyses. We’ll see below how we can easily go between wide/long data using pivot_*() functions, which will let us get data easily into the formats we want. To return to our example, let’s take a look at recreating our histogram of all apple varieties and all amino acids. # First we can create a ggplot() object that stores what variable we want to plot through the &quot;mapping&quot; of an aesthetic function p &lt;- tidy_apples %&gt;% ggplot(data = ., # the &quot;data = .&quot; argument is written out for clarity mapping = aes(x = concentration)) # If we try to plot p, we will see only the blank canvas because we haven&#39;t defined any geoms plot(p) # Then we add elements to the plot p2 &lt;- p + geom_histogram(color = &quot;black&quot;, fill = &quot;grey&quot;) + theme_classic() + # This changes the theme to a nicer, &quot;R style&quot; plot labs(x = &quot;Amino acid concentration (mg/L)&quot;) # This relabels the x-axis with a more descriptive label # We can actually plot ggplots by just calling them directly p2 While that doesn’t seem so impressive compared to the ease of a one-stop function like hist(), you should be able to start to see how we can add and subtract elements to change our plots around, as well as being able to save them and print them again and again at will. This lets us do several things: We can build up plots step-by-step in order to make sure we’ve got everything right, avoiding mistakes like our scale problem in our scatterplot above. We can do complex things like, for example, breaking out our plots by some kind of grouping variable (“faceting”): p2 + facet_wrap(~amino_acid) Now that’s pretty cool. It also points to the importance of tidy data for this kind of plotting–we are able to essentially tell ggplot() “pull out all the data that is tagged with distinct values of amino_acid in that column and plot them individually.” 5.3.1.1 WHAT THE HECK IS + doing ??? Ok, let’s start unpicking this. The first thing you have probably noticed about ggplot() is the way that you build the plots: by using + to literally add new elements or themes or other things to the plot. This is contrary to both everything in base R (there we just ran plotting functions sequentially) and tidyverse conventions (which generally use the %&gt;% to do something similar). The short story is that, in fact, it would be better if ggplot2 used %&gt;%, and someday it will. But it is older than the rest of the tidyverse, and so was programmed using both different syntax and slightly different underlying functional programming, so it doesn’t work with %&gt;%. I personally think it’s easiest to think of + in ggplot2 as literal. When you see it, read it as “and then add &lt;this element&gt;…”, rather than “pass what’s on the left hand side to the right hand side”. So it works a lot like the pipe, but is not interchangeable. 5.3.1.2 The aes() function/argument The ggplot() function takes two arguments that are essential, as well as some others you’ll rarely use. The first, data =, is straightforward, and you’ll usually be passing data to the function at the end of some pipeline using %&gt;% The second, mapping =, is less clear. This argument requires the aes() function, which can be read as the “aesthetic” function. The way that this function works is quite complex, and really not worth digging into here, but I understand it in my head as telling ggplot() what part of my data is going to connect to what part of the plot. So, if we write aes(x = concentration), we can read this in our heads as “the values of x will be mapped from the ‘concentration’ column”. This sentence tells us the other important thing about ggplot() and the aes() mappings: mapped variables each have to be in their own column. This is another reason that ggplot() requires tidy data. Let’s look again at the original apples data: apples %&gt;% head(10) %&gt;% knitr::kable() Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala GABA Pro Cys Lys Tyr Met Val Ile Leu Phe Arkansas Black Rep_1 0.62 10.42 0.68 1.60 0.36 0.09 2.92 2.37 0.20 0.72 0.35 0.26 0.22 0.43 0.67 0.98 0.55 1.59 1.24 3.25 Blacktwig Rep_1 0.77 2.32 0.54 0.00 0.24 0.00 1.96 1.03 0.18 0.47 0.15 0.27 0.28 0.52 0.83 1.15 0.62 1.64 1.51 3.70 Empire Rep_1 0.46 21.20 0.43 1.01 0.27 0.09 5.88 1.49 0.28 1.92 0.51 0.67 0.21 0.44 0.65 1.74 0.52 1.50 1.13 3.20 Enterprise Rep_1 0.52 34.75 0.90 2.78 0.24 0.08 5.18 2.49 0.33 1.71 0.29 0.37 0.22 0.41 0.66 0.84 0.63 1.39 1.18 2.66 Field Red Rep_1 0.85 4.02 0.29 0.00 0.41 0.00 1.87 1.29 0.19 0.33 0.15 0.00 0.40 0.55 1.03 1.09 1.72 1.82 1.67 13.37 Golden Delicious Rep_1 0.56 1.01 0.00 0.00 0.34 0.00 1.76 0.98 0.32 0.26 0.00 0.00 0.33 0.49 0.76 0.92 1.50 1.59 1.44 11.83 Granny Smith Rep_1 0.61 8.76 0.34 0.00 0.31 0.10 1.41 0.58 0.38 0.42 0.00 0.23 0.37 0.56 1.49 1.28 1.66 2.00 1.62 3.79 Newtown Pippin Rep_1 0.33 26.13 1.01 1.36 0.67 0.13 5.14 3.23 0.57 1.03 0.54 0.28 0.36 0.60 0.94 1.38 0.85 2.12 1.39 4.50 Northern Spy Rep_1 0.51 2.52 0.17 0.00 0.23 0.00 1.60 1.05 0.35 0.34 0.00 0.00 0.35 0.53 1.24 1.34 1.52 2.04 1.25 12.47 Rome Rep_1 0.36 1.57 0.20 0.00 0.43 0.08 1.23 0.91 0.34 0.23 0.00 0.00 0.33 0.54 0.80 1.02 1.63 1.92 1.46 12.68 Here we have a data frame that is cleaned and well organized, but it is wide. The variable that is used for faceting above to make the previous plot–the name of the amino acid being measured in each column–is spread out over the column names, and so ggplot() can’t easily find it in order to make this nice kind of faceted plot. It is possible to do this kind of faceting in base R (with or without tidy data) using control-flow and the par() function–I leave it to you as an exercise to try it, as a way to develop programming skills. In our example, we used aes() to map the x = concentration in the original call to ggplot(), and then added elements to that original, blank canvas. You’ll note, in particular, that we didn’t tell geom_histogram() that the x-values it was counting were coming from conentration. This is one of the key rules of ggplot(): each additional element of a plot inherits the mappings of the plot from the base function. p3 &lt;- p + geom_histogram(aes(fill = Variety), color = &quot;black&quot;, lwd = 0.1) + theme_classic() p3 So, in this plot, we’ve colored each column in the same histogram as before with the number of counts that come from different types of apples. This isn’t super useful as a visualization, but it does demonstrate the principle: geom_histogram() inherited the aesthetic mapping of x = concentration from the previous statement (ggplot()). We didn’t have to set it. It also demonstrates a second principle: each subsequent addition to a plot can add new aesthetics. In our case, within geom_histogram() we set mapping = aes(fill = Variety), which told ggplot() to (automatically) assign a different color to each unique value of Variety (why is it “fill” and not “color”?). You’ll also note that ggplot() then automatically made a legend for us in the plot, showing us the choices it had made. 5.3.1.3 Settings outside of aes() You may also have noticed in the previous plot that we did also set a color = argument in the geom_histogram() function–but outside the whole mapping = aes(&lt;stuff&gt;) argument. This is what we might call a “set” variable, as opposed to a “mapped” variable (which are all within aes()). We wrote color = \"black\", which tells ggplot() and specifically the geom_histogram() part of ggplot() to not look for a variable to map to color, but just to set it to \"black\". This is a shortcut to one of the colors that R knows about–you can get a full list of these by running colors(). In general, in ggplot() if we want the data visualization to programmatically assign some aesthetic aspect of the plot from a variable in our data, we will want to map that aesthetic using the mapping = aes() argument construction, either in ggplot() or in one of the subsequent layer functions. If we want to set an aesthetic aspect to be constant, we set it in the function argument outside of aes() to be a constant. Based on this principle, what is going wrong in this plot? This is one of the most common problems in ggplot(). tidy_apples %&gt;% ggplot(aes(x = concentration, fill = &quot;blue&quot;)) + geom_histogram(color = &quot;black&quot;, lwd = 0.1) + theme_classic() 5.3.1.4 Themes with theme_*() I’ve ended all of my plots using ggplot() with the addition of theme_classic(). In ggplot(), themes encompass everything about the plot that doesn’t depend on data at all. Things like the way the legend is printed, whether or not there is a coordinate grid displayed on the plot, the background color, etc–these are all related to themes. ggplot2 comes with a bunch of shortcut themes that you can apply, like theme_classic(), theme_minimal(), theme_bw(), etc. p2 + theme_dark() If you need to manipulate individual aspects of a plot, you may need to use the general theme() function. This allows you to very precisely manipulate aspects of the theme, but it is not very user friendly. This is one of the points where a developed sense of “how do I search Stack Exchange for this?” will be your friend! p2 + theme(axis.title.x = element_text(face = &quot;italic&quot;), axis.title.y = element_blank()) 5.3.1.5 All these damn scale_*() functions Finally, say we didn’t like the default color set for the filled bars in our histogram. How can we manipulate the colors that are plotted? The way in which mapped, aesthetic variables are assigned to visual elements is controlled by the scale_*() functions. In my experience, the most frequently encountered scales are those for color: either scale_fill_*() for solid objects (like the bars in a histogram) or scale_color_*() for lines and points (like the outlines of the histogram bars). Scale functions work by telling ggplot() how to map aesthetic variables to visual elements. So, in the case of our histogram, we have mapped different values of Variety to different fill colors, but we haven’t actually told ggplot() which colors we want. We can do this with scale_fill_*(). p3 + scale_fill_viridis_d() # this is just one option - use tab-completion to see some others ggplot2 has a broad range of built-in options for scales, but there are many others available in add-on packages that build on top of it. You can also build your own scales using the scale_*_manual() functions, in which you give a vector of the same length as your mapped aesthetic variable in order to set up the visual assignment. That sounds jargon-y, so here is an example: # We&#39;ll pick 14 random colors from the colors R knows about random_colors &lt;- print(colors()[sample(x = 1:length(colors()), size = 14)]) ## [1] &quot;palegoldenrod&quot; &quot;deepskyblue2&quot; &quot;purple1&quot; &quot;goldenrod1&quot; ## [5] &quot;orchid1&quot; &quot;chocolate&quot; &quot;seagreen2&quot; &quot;mediumpurple&quot; ## [9] &quot;sienna&quot; &quot;orchid2&quot; &quot;mediumorchid1&quot; &quot;steelblue3&quot; ## [13] &quot;olivedrab2&quot; &quot;lightsalmon3&quot; p3 + scale_fill_manual(values = random_colors) Obviously this doesn’t produce good results, but it illustrates how you can use this function to set colors the way you’d like. 5.3.1.6 Faceting (making multiple plots) The final bit of recipe we’ve seen so far is the facet_wrap() function, which–still almost magically, to me–splits our plot into multiple facets according to the specified variable(s). The classic interface for this uses R expressions–the ~ notation–but there are now other options, which are detailed if we inspect the ?facet_wrap documentation. p3 + facet_wrap(~Rep) The only difference is that we must write a “one-sided” function: leave the left-hand side (LHS) of the expression empty. We can actually facet by multiple variables. Remember that every combination of variables will make one facet in the array of plots, so you want to be cautious or all of your plots will be illegible. That’s why I am going to filter our data and show this off with just a few apple varieties: tidy_apples %&gt;% filter(Variety %in% c(&quot;Arkansas Black&quot;, &quot;Granny Smith&quot;, &quot;Northern Spy&quot;)) %&gt;% ggplot(mapping = aes(x = concentration)) + geom_histogram(aes(fill = Variety), color = &quot;black&quot;, lwd = 0.2) + facet_wrap(~Variety + Rep) + theme_classic() We can also set up the arrangement of the grid by manipulating the nrow = and ncol = arguments, just like when we make a matrix(). There is a companion function for facet_wrap() named facet_grid(). Test out your ability to read help files to figure out what the difference between the two are. 5.3.2 Common recipes for basic tasks Now that we have the start of a grasp on how ggplot2 works, let’s look at a few of the common visualizations that you’d want to use for your own data exploration or for publication–the main tasks of dataviz. 5.3.2.1 Histograms with geom_histogram()/geom_density() We’ve already thoroughly explored how to make histograms from our apple data with geom_histogram(). A related option that gives a little more information, especially for large, continuous dataset is called a density plot. These visualize an empirical probability density based on your dataset. For our apples, let’s look at the density plot instead of the histogram: p + geom_density() + theme_classic() If you squint, you can see the plotted line describes a similar shape to our histogram. We can use ggplot()s layering ability to just put them both on the same plot: p + geom_histogram(aes(y = after_stat(count / sum(count))), fill = &quot;white&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;, lwd = 1) + theme_classic() FYI, a similar approach can be achieved with geom_freqpoly(), which literally draws a line between the peak of each bar in a histogram: p + geom_histogram(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_freqpoly(color = &quot;red&quot;, lwd = 1) + theme_classic() These are not exact duplicates because density is based on a smoothed estimate, whereas both histogram and freqpoly are just empirical counts. 5.3.2.2 Boxplots with geom_boxplot() / geom_violin() I agree with John Tukey that boxplots are some of the best visualizations for quickly understanding the distribution of different categorical variables. ggplot() does a great job with boxplots, especially when combined with facets. As you might imagine, geom_boxplot() will give you boxplots. tidy_apples %&gt;% ggplot(mapping = aes(x = amino_acid, y = concentration)) + geom_boxplot() + theme_classic() Notice that, unlike in our examples with histogram, in ggplot() we now need to map both x and y aesthetic variables. In English, you might say “we are interested in the values of concentration (y) at each type of amino acid (x).” A related visualization is the “violin” plot–instead of a straight box, violin plots show the distribution of observations around the IQR: tidy_apples %&gt;% ggplot(mapping = aes(x = Rep, y = concentration)) + geom_violin() + theme_classic() Because our data doesn’t have a lot of variation in most of the categories of amino_acid, I demonstrated this plot using the Rep variable, which unsurprisingly (and reassuringly) doesn’t look much different across the 3 reps. 5.3.2.3 Scatterplots with geom_point() We haven’t really talked about types of data explicitly in this class from a measurement theory perspective. Who can tell me the definition of these four types of data: Nominal Ordinal Interval Ratio For our purposes in R, we can largely think about these two categories instead: Categorical Continuous How do these relate? So far, in all of our ggplot() examples, we’ve been plotting either a single continuous variable (concentration), or one continuous variable against a categorical variable (e.g., concentration ~ Variety). But in our base plotting we were using the most common kind of continuous vs. continuous plot: a scatterplot. Scatterplots are the visualizations that we’re all most comfortable with: basic Cartesian (x, y) plots. In ggplot2, we use geom_point() to make scatter plots. Let’s recreate our base R scatterplot: # Note that here I am using apples instead of tidy_apples - why? apples %&gt;% ggplot(aes(x = Asn, y = Asp)) + geom_point(shape = 1) + theme_classic() One advantage of ggplot() visualization is that it takes care of scaling for us. If we want to add the phenylalanine points, ggplot() is smart enough to resize the plot to include the cluster of points we previously missed. apples %&gt;% ggplot(aes(x = Asn, y = Asp)) + geom_point(shape = 1) + geom_point(aes(y = Phe), color = &quot;red&quot;, shape = 1) + # note that this still INHERITS the x = Asn mapping theme_classic() However, note that ggplot() isn’t magic. Confusingly, our y-axis is now labeled “Asp”, even though it is really plotting “Concentration” of both Asp (black) and Phe (red). apples %&gt;% ggplot(aes(x = Asn, y = Asp)) + geom_point(shape = 1, color = &quot;black&quot;) + geom_point(aes(y = Phe), color = &quot;red&quot;, shape = 1) + theme_classic() + labs(x = &quot;Asparagine (mg/L)&quot;, y = &quot;Concentration (mg/L)&quot;) But you’ll notice there still isn’t a legend. It turns out that is because we’re using ggplot() in a nonstandard way. As homework, I will be asking you to find a way to create a similar kind of plot but to do it using tidy data, which will produce a legend properly. 5.3.2.3.1 Correlation fit plots with geom_smooth() To continue recreating our previous example, we might want to add linear fits to these two sets of points. The function that fits a model to points in ggplot2 is called geom_smooth()–it adds a “smoothed” model fit. apples %&gt;% ggplot(aes(x = Asn, y = Asp)) + geom_point(shape = 1, color = &quot;black&quot;) + geom_point(aes(y = Phe), color = &quot;red&quot;, shape = 1) + geom_smooth(color = &quot;black&quot;, method = &quot;lm&quot;) + geom_smooth(aes(y = Phe), color = &quot;red&quot;, fill = &quot;red&quot;, method = &quot;lm&quot;) + theme_classic() + labs(x = &quot;Asparagine (mg/L)&quot;, y = &quot;Concentration (mg/L)&quot;) Notice that we had to add two calls to geom_smooth()–one that inherited our initial mapping of y = Asp, and one that overrides that to be y = Phe, just as we do for the geom_point() calls. The method = \"lm\" argument tells geom_smooth() to use the lm() function, just like we did with our base R abline() call. This is not the default for geom_smooth()–it has built in math to try to choose the best model. Let’s see what it does if we let it choose it’s own fit: apples %&gt;% ggplot(aes(x = Asn, y = Asp)) + geom_point(shape = 1, color = &quot;black&quot;) + geom_point(aes(y = Phe), color = &quot;red&quot;, shape = 1) + geom_smooth(color = &quot;black&quot;) + geom_smooth(aes(y = Phe), color = &quot;red&quot;, fill = &quot;red&quot;) + theme_classic() + labs(x = &quot;Asparagine (mg/L)&quot;, y = &quot;Concentration (mg/L)&quot;) Based on its internal tests, geom_smooth() chooses what’s called a “spline” model–this is a way of fitting a wiggly curve to a set of points. It is a better way of describing the points we see, but may make worse predictions for points outside our x (Asn) value range. 5.4 Return to data wrangling - long vs wide Finally, we’re going to briefly return to the idea of tidy data. We’ve talked about this today and in previous classes, but hopefully experimenting with ggplot2 has given some motivation to why we might want to use tidy data in many cases. We also showed some basic examples of moving from wide to narrow data by “tidying” our apples data frame into tidy_apples. 5.4.1 Making data tidy In that operation we did the following: Figure out what variable is spread across the columns: the amino acid names. Check to make sure the data in all of those columns is the same: Yes! It is all “concentration of amino acid X” data. Create two new columns to reorganize our data into: amino_acids. which has the name of the amino acid being measured, coming from the column names concentration, which has the concentration of the named amino acid, coming from the former column entries We did all this using the function pivot_longer(), which takes wide data and makes it long (and tidy). The anatomy of pivot_wider() is as follows: names_to = \"&lt;where the former column names will go&gt;\": in our case, we wrote names_to = \"amino_acid\". values_to = \"&lt;where the former column entries will go&gt;\": in our case, we wrote values_to = \"concentration\" -c(&lt;excluded columns&gt;): a vector of the columns that we don’t want to make longer, but instead want to keep track of. We wrote -c(Variety, Rep). So the overall recipe was apples %&gt;% pivot_longer(names_to = &quot;amino_acid&quot;, values_to = &quot;concentration&quot;, -c(Variety, Rep)) ## # A tibble: 840 × 4 ## Variety Rep amino_acid concentration ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Arkansas Black Rep_1 His 0.62 ## 2 Arkansas Black Rep_1 Asn 10.4 ## 3 Arkansas Black Rep_1 Ser 0.68 ## 4 Arkansas Black Rep_1 Gln 1.6 ## 5 Arkansas Black Rep_1 Arg 0.36 ## 6 Arkansas Black Rep_1 Gly 0.09 ## 7 Arkansas Black Rep_1 Asp 2.92 ## 8 Arkansas Black Rep_1 Glu 2.37 ## 9 Arkansas Black Rep_1 Thr 0.2 ## 10 Arkansas Black Rep_1 Ala 0.72 ## # … with 830 more rows ## # ℹ Use `print(n = ...)` to see more rows 5.4.2 Making data wide We can also reverse the process, by using the pivot_wider() function. This literally reverses the operation. We need to specify two main arguments: names_from = \"&lt;current column&gt;\": where the names for the new, wide columns will come from. values_from = \"&lt;current column&gt;\": where the values that will go into the cells in the new columns will come from So to get back to apples, we can write: tidy_apples %&gt;% pivot_wider(names_from = &quot;amino_acid&quot;, values_from = &quot;concentration&quot;) ## # A tibble: 42 × 22 ## Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arkansas B… Rep_1 0.62 10.4 0.68 1.6 0.36 0.09 2.92 2.37 0.2 0.72 ## 2 Blacktwig Rep_1 0.77 2.32 0.54 0 0.24 0 1.96 1.03 0.18 0.47 ## 3 Empire Rep_1 0.46 21.2 0.43 1.01 0.27 0.09 5.88 1.49 0.28 1.92 ## 4 Enterprise Rep_1 0.52 34.8 0.9 2.78 0.24 0.08 5.18 2.49 0.33 1.71 ## 5 Field Red Rep_1 0.85 4.02 0.29 0 0.41 0 1.87 1.29 0.19 0.33 ## 6 Golden Del… Rep_1 0.56 1.01 0 0 0.34 0 1.76 0.98 0.32 0.26 ## 7 Granny Smi… Rep_1 0.61 8.76 0.34 0 0.31 0.1 1.41 0.58 0.38 0.42 ## 8 Newtown Pi… Rep_1 0.33 26.1 1.01 1.36 0.67 0.13 5.14 3.23 0.57 1.03 ## 9 Northern S… Rep_1 0.51 2.52 0.17 0 0.23 0 1.6 1.05 0.35 0.34 ## 10 Rome Rep_1 0.36 1.57 0.2 0 0.43 0.08 1.23 0.91 0.34 0.23 ## # … with 32 more rows, and 10 more variables: GABA &lt;dbl&gt;, Pro &lt;dbl&gt;, Cys &lt;dbl&gt;, ## # Lys &lt;dbl&gt;, Tyr &lt;dbl&gt;, Met &lt;dbl&gt;, Val &lt;dbl&gt;, Ile &lt;dbl&gt;, Leu &lt;dbl&gt;, Phe &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Note that, if your remaining variables (in this case, Variety and Rep) do not have unique combinations for each new column, pivot_wider() will give you a warning telling you that you’re trying to assign multiple variables to a single cell. This is a common error when you’re starting out: tidy_apples %&gt;% select(-Rep) %&gt;% # we get rid of Rep to make row IDs non-unique pivot_wider(names_from = &quot;amino_acid&quot;, values_from = &quot;concentration&quot;) ## Warning: Values from `concentration` are not uniquely identified; output will contain list-cols. ## * Use `values_fn = list` to suppress this warning. ## * Use `values_fn = {summary_fun}` to summarise duplicates. ## * Use the following dplyr code to identify duplicates. ## {data} %&gt;% ## dplyr::group_by(Variety, amino_acid) %&gt;% ## dplyr::summarise(n = dplyr::n(), .groups = &quot;drop&quot;) %&gt;% ## dplyr::filter(n &gt; 1L) ## # A tibble: 14 × 21 ## Variety His Asn Ser Gln Arg Gly Asp Glu Thr Ala GABA ## &lt;chr&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; &lt;lis&gt; ## 1 Arkansas B… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 2 Blacktwig &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 3 Empire &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 4 Enterprise &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 5 Field Red &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 6 Golden Del… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 7 Granny Smi… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 8 Newtown Pi… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 9 Northern S… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 10 Rome &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 11 Shenandoah &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 12 Virginia G… &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 13 Winesap &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 14 York &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## # … with 9 more variables: Pro &lt;list&gt;, Cys &lt;list&gt;, Lys &lt;list&gt;, Tyr &lt;list&gt;, ## # Met &lt;list&gt;, Val &lt;list&gt;, Ile &lt;list&gt;, Leu &lt;list&gt;, Phe &lt;list&gt; ## # ℹ Use `colnames()` to see all variable names This can either tell you you’ve incorrectly set up your pivot table or that you haven’t properly understood your data. The warning message produced has useful suggestions to figure out which of these might be the case. References References "],["describing-your-data.html", "6 Describing your data 6.1 Datasets 6.2 Review exercise: How did I import these datasets? 6.3 What kind of data do you have? 6.4 Examining big data frames 6.5 Generating data summaries 6.6 Statistics review References", " 6 Describing your data Last week we devoted a lot of time to an introduction to dataviz, using the example of a dataset of amino-acid measurements from Sihui Ma’s dissertation research (Ma et al. 2018). We mostly explored the descriptive and exploratory potentials of data visualization, rather than the inferential capabilities of this approach. This week we will be doing something similar: we will be learning how to use R’s tools for describing and summarizing data, as well as introducing our first review of statistical concepts. However, we will be generally staying far away from the world of statistical inference, even though that’s the ultimate goal of much of science and of this class. Briefly, it is worth it to describe the key differences between the tasks of description and inference. In description, we are interested in providing accurate summaries and insights that have to do with our sample. We are not trying to make the step of generalizing to the population of similar samples (or indeed the broader world)–we are interested in giving insightful, concise insights into the mess of data we have in front of us. In general, description doesn’t require that we assume or theorize about generating mechanisms (actual or statistical) for our data. In inference, we would like to be able to generalize from our sample to the broader world of other samples, or entire populations. We may also be interested in prediction of future outcomes. Either of these tasks require us to make some hypotheses or assumptions about how our data were generated in order for us to predict future events. This is generally much harder and less certain than the task of describing just the data we have. We’ll talk about how to approach inference (from an agnostic, critical perspective) starting after Spring Break. XKCD weather forecasts using different models. However, it is worth noting that the distinction between description and inference in scientific thinking and writing is often somewhat muddy. Often, we apply statistics that are appropriate for description to our samples and then, without ill-intention, use what we know about our sample to generalize to a larger setting, which means we are performing inference. This can cause all sorts of problems! For an introduction to this problem, I recommend the chapter(s) on inference from Statistical Rethinking (McElreath 2020). 6.1 Datasets Today we’re going to be looking at a set of pilot data that Elizabeth Cole and Martha Calvert collected as part of a class project for FST 5014. The abstract for the project follows: Hard cider is a fermented, alcoholic beverage often compared to wine and beer that is growing in popularity throughout the United States. However, the growing popularity of cider has been accompanied by inconsistent communication regarding consumers’ perception of dryness and sweetness in hard cider. To date, there is no conclusive methodology to measure cider dryness and sweetness in a way that is relevant and accurate for most ciders and cider-drinkers. At the same time, there is no industry standard for the serving temperature of hard cider, although temperature is known to impact the sensory experience and particularly the apparent dryness of other alcoholic beverages. Recently, the New York Cider Association developed the Merlyn Dryness Scale as a way to scale cider dryness using basic cider chemistry (pH, RS, TA, polyphenol content, malic acid, and CO2) , but this approach has not been validated in sensory experiments. Thus, the aim of this research is to validate the Merlyn Dryness Scale by comparing the chemical analyses of various cider samples to the sensory evaluations of those cider samples, and to assess the effect of serving temperature on cider dryness perception. Chemical analyses performed for the research include determining the pH, RS, TA, and total polyphenol content. This dataset contains two different data tables. The first–cider_chem–is a table of chemistry data for the ciders, as described above. knitr::kable(cider_chem) Cider pH TA (g/L) CO2 TRS (g/L) MAL(g/L) Polyphenols (ppm) Merlyn Rating Eden 3.69 4.27 4.54 18.67 3.62 203.56 4.374 Buskey 3.94 3.10 4.53 15.67 2.05 245.57 5.055 1911 3.57 4.97 4.54 21.00 5.63 256.86 4.225 The second–cider_dryness–is a (much larger) table of subjects’ sensory judgments of the same ciders. slice_sample(cider_dryness, n = 10) %&gt;% knitr::kable() Sample_Name Panelist_Code Panelist_Name Panelist_Display_Name Panelist_Email Dryness Liking 1911 Chilled lahne_vtu Jacob Lahne Jacob jlahne@vt.edu 2.8 6 Buskey Chilled akyle99_vtu Kyle Adie Kyle akyle99@vt.edu 2.5 7 1911 RT klbyrwa_vtu NA NA klbyrwa@vt.edu 4.4 6 1911 RT kaylinf_vtu Kaylin Fitzgerald Kaylin kaylinf@vt.edu 4.0 8 Buskey Chilled Rmawn_vtu Rachel Mawn Rachel Rmawn@vt.edu 3.3 7 Eden RT taybenn_vtu NA NA taybenn@vt.edu 3.0 8 1911 RT sok_vtu Sean O’Keefe Sean sok@vt.edu 3.3 7 Eden Chilled peggylaynepe_vtu Peggy Layne Peggy peggylaynepe@gmail.com 2.5 4 Buskey Chilled ann.sandbrook@vt.edu Ann Sandbrook Ann ann.sandbrook@vt.edu 2.0 6 Buskey Chilled gerberm_vtu NA NA gerberm@vt.edu 3.7 7 6.2 Review exercise: How did I import these datasets? # Use this R chunk to write code that reads the appropriate files into R. We may also make recourse to the polyphenol data from last week to give us some data in a different format to play with. As a reminder, those data describe amino-acid contents in different apple varieties, and look like: slice_sample(apples, n = 5) %&gt;% knitr::kable() Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala GABA Pro Cys Lys Tyr Met Val Ile Leu Phe Northern Spy Rep_2 0.64 2.47 0.19 0.00 0.25 0.00 1.69 1.10 0.35 0.39 0.00 0.00 0.29 0.45 0.71 0.91 1.56 1.99 1.20 13.11 Newtown Pippin Rep_1 0.33 26.13 1.01 1.36 0.67 0.13 5.14 3.23 0.57 1.03 0.54 0.28 0.36 0.60 0.94 1.38 0.85 2.12 1.39 4.50 Field Red Rep_2 0.66 3.97 0.31 0.00 0.43 0.00 2.07 1.41 0.21 0.35 0.18 0.00 0.41 0.53 1.12 1.02 1.88 1.77 1.80 13.26 Newtown Pippin Rep_3 0.35 26.01 1.00 1.36 0.64 0.12 5.22 3.27 0.63 1.01 0.54 0.29 0.35 0.60 0.94 1.32 0.85 2.10 1.52 4.35 Arkansas Black Rep_2 0.17 9.87 0.64 1.55 0.46 0.09 2.67 2.20 0.20 0.63 0.25 0.20 0.29 0.48 0.82 1.08 0.63 1.47 1.32 3.29 6.3 What kind of data do you have? Before we get into new material, let’s review some basic tools for understanding data. We’re going to start off with a quick review of material from Weeks 1 and 2 of this course, but then quickly get into new, useful concepts and functions for understanding your data. 6.3.1 Basic attributes There are some very basic questions we might have about our data that it’s worth remember. When we import data, we might ask: What kinds of variables are there in the data? Are the data types those we expect? How many rows and columns are there in the data? What shape is the data? If we are dealing with structured data (like data frames and tibbles), what are the names of the variables/columns and the rows? What does the data look like–that is, what does a representative set of rows contain? 6.3.1.1 Types of data and data structures We learned about data types in lesson 1 and 2–you might recall a discussion of integers, characters, numeric, etc vectors, and of complex data structures like matrices, lists, and data frames. In general, it’s worth remembering that vectors and matrices can contain only one kind of data–that is, a matrix can have numeric data in all of its cells, or character data, but it cannot have some numeric and some character data. matrix(letters[1:4], nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; matrix(1:4, nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 The data types of these comparatively simple data objects are going to always be the kind of thing they contain: typeof(&quot;a string&quot;) ## [1] &quot;character&quot; typeof(matrix(letters[1:4], nrow = 2)) ## [1] &quot;character&quot; Lists, on the other hand, can contain any number of different types of objects (including complex data structures): list(a_character = &quot;hi!&quot;, a_number = 3.5, an_integer = 3L, a_logical = TRUE, a_matrix = matrix(1:4, nrow = 2)) ## $a_character ## [1] &quot;hi!&quot; ## ## $a_number ## [1] 3.5 ## ## $an_integer ## [1] 3 ## ## $a_logical ## [1] TRUE ## ## $a_matrix ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Therefore, the type of a list is always just list: typeof(list(a = 1, b = TRUE)) ## [1] &quot;list&quot; Data frames (and tibbles) are special kinds of lists. example_tibble &lt;- tibble(x = 1:2, y = c(TRUE, FALSE), z = list(list(a = 1), list(b = &quot;jazz&quot;))) example_tibble ## # A tibble: 2 × 3 ## x y z ## &lt;int&gt; &lt;lgl&gt; &lt;list&gt; ## 1 1 TRUE &lt;named list [1]&gt; ## 2 2 FALSE &lt;named list [1]&gt; typeof(example_tibble) ## [1] &quot;list&quot; The main things to remember about these structures are: They are “rectangular” - e.g., they enable numerical indexing in the same way that we can with matrices ([]) or using the special data-frame access: $. example_tibble[1, ] # get the first row of our tibble ## # A tibble: 1 × 3 ## x y z ## &lt;int&gt; &lt;lgl&gt; &lt;list&gt; ## 1 1 TRUE &lt;named list [1]&gt; example_tibble$x # get the first column of our example tibble ## [1] 1 2 That means that each column of a data frame must be the same length: example_tibble$new &lt;- 1:5 ## Error: ## ! Assigned data `1:5` must be compatible with existing data. ## ✖ Existing data has 2 rows. ## ✖ Assigned data has 5 rows. ## ℹ Only vectors of size 1 are recycled. All data stored in each column must be of the same type (although as can be seen in our example, we can make a “list of lists” column). typeof(example_tibble$y) ## [1] &quot;logical&quot; 6.3.1.2 Sizes Frequently, we will need to know something about the size of our data sets. For example, we might need to know how many observations we have total in our data set, which may correspond to the number of rows (if our data is in “long” format), we might need to know the number of columns we have so that we can use indexing to access them properly (although using named accessing through select() will generally be more readable), or we might need to know numbers of rows or columns so that we can use for loops or other control-flow tools. For vectors, the most useful function is length(). This returns, as you might expect, the length of the vector in terms of how many things are in it: length(1:100) ## [1] 100 length(letters) ## [1] 26 However, length() is less useful for complex data objects: for data frames, counterintuitively, R treats their length as the number of columns; for lists the length is the number of top-level entries. length(cider_chem) ## [1] 8 length(cider_dryness) ## [1] 7 For matrices, data frames, etc, the nrow() and ncol() functions are more useful. Respectively, they tell us the number of rows and columns in the object: ncol(cider_dryness) ## [1] 7 nrow(cider_chem) ## [1] 3 For “rectangular” data structures, we can also ask about the dimensions all at once using the dim() function: dim(cider_chem) ## [1] 3 8 Less important, for our purposes, is the actual (memory) size of files: how many bytes they take up. Unless you are working with big data (on the scale of at least hundreds of megabytes), R will happily deal with whatever size data you need. 6.3.1.3 Names In most of the use cases we have for R, we will be working with data objects that have names. When we print objects we can see that there are names assigned to different parts of the data: apples # what are the names here? ## # A tibble: 42 × 22 ## Variety Rep His Asn Ser Gln Arg Gly Asp Glu Thr Ala ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Arkansas B… Rep_1 0.62 10.4 0.68 1.6 0.36 0.09 2.92 2.37 0.2 0.72 ## 2 Blacktwig Rep_1 0.77 2.32 0.54 0 0.24 0 1.96 1.03 0.18 0.47 ## 3 Empire Rep_1 0.46 21.2 0.43 1.01 0.27 0.09 5.88 1.49 0.28 1.92 ## 4 Enterprise Rep_1 0.52 34.8 0.9 2.78 0.24 0.08 5.18 2.49 0.33 1.71 ## 5 Field Red Rep_1 0.85 4.02 0.29 0 0.41 0 1.87 1.29 0.19 0.33 ## 6 Golden Del… Rep_1 0.56 1.01 0 0 0.34 0 1.76 0.98 0.32 0.26 ## 7 Granny Smi… Rep_1 0.61 8.76 0.34 0 0.31 0.1 1.41 0.58 0.38 0.42 ## 8 Newtown Pi… Rep_1 0.33 26.1 1.01 1.36 0.67 0.13 5.14 3.23 0.57 1.03 ## 9 Northern S… Rep_1 0.51 2.52 0.17 0 0.23 0 1.6 1.05 0.35 0.34 ## 10 Rome Rep_1 0.36 1.57 0.2 0 0.43 0.08 1.23 0.91 0.34 0.23 ## # … with 32 more rows, and 10 more variables: GABA &lt;dbl&gt;, Pro &lt;dbl&gt;, Cys &lt;dbl&gt;, ## # Lys &lt;dbl&gt;, Tyr &lt;dbl&gt;, Met &lt;dbl&gt;, Val &lt;dbl&gt;, Ile &lt;dbl&gt;, Leu &lt;dbl&gt;, Phe &lt;dbl&gt; ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Simple objects can also have names: simple_names &lt;- c(a = 1, b = 2, c = 10) # what type of object is this? simple_names ## a b c ## 1 2 10 We can get the names of an object through names(): names(simple_names) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; names(apples) ## [1] &quot;Variety&quot; &quot;Rep&quot; &quot;His&quot; &quot;Asn&quot; &quot;Ser&quot; &quot;Gln&quot; &quot;Arg&quot; ## [8] &quot;Gly&quot; &quot;Asp&quot; &quot;Glu&quot; &quot;Thr&quot; &quot;Ala&quot; &quot;GABA&quot; &quot;Pro&quot; ## [15] &quot;Cys&quot; &quot;Lys&quot; &quot;Tyr&quot; &quot;Met&quot; &quot;Val&quot; &quot;Ile&quot; &quot;Leu&quot; ## [22] &quot;Phe&quot; We can also set names in an object using the same function, combined with &lt;-: names(simple_names) &lt;- c(&quot;new 1&quot;, &quot;new 2&quot;, &quot;new 3&quot;) simple_names ## new 1 new 2 new 3 ## 1 2 10 For complex objects, we have equivalents of nrow()/ncol()/dim(): colnames(cider_chem) ## [1] &quot;Cider&quot; &quot;pH&quot; &quot;TA (g/L)&quot; ## [4] &quot;CO2&quot; &quot;TRS (g/L)&quot; &quot;MAL(g/L)&quot; ## [7] &quot;Polyphenols (ppm)&quot; &quot;Merlyn Rating&quot; rownames(cider_chem) # note that tibbles do not support row names ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; dimnames(cider_chem) ## [[1]] ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## ## [[2]] ## [1] &quot;Cider&quot; &quot;pH&quot; &quot;TA (g/L)&quot; ## [4] &quot;CO2&quot; &quot;TRS (g/L)&quot; &quot;MAL(g/L)&quot; ## [7] &quot;Polyphenols (ppm)&quot; &quot;Merlyn Rating&quot; All of these can be combined with &lt;- to modify the existing assignments. Data frames and matrices allow rownames, but these are discouraged in tibbles. Wickham &amp; Grolemund -Wickham and Grolemund (2017) argue that row names are more confusing and less useful than a named column in a dataframe, but a number of non-tidyverse packages in R do rely on them. Therefore, it is useful to know a little bit about them, even though I do agree that they are less useful than a named column. For example, we might want to transform our cider_chem tibble into a data frame with row names corresponding to the sample names cider_chem_df &lt;- as.data.frame(cider_chem) rownames(cider_chem_df) &lt;- cider_chem$Cider cider_chem_df ## Cider pH TA (g/L) CO2 TRS (g/L) MAL(g/L) Polyphenols (ppm) ## Eden Eden 3.69 4.27 4.54 18.67 3.62 203.56 ## Buskey Buskey 3.94 3.10 4.53 15.67 2.05 245.57 ## 1911 1911 3.57 4.97 4.54 21.00 5.63 256.86 ## Merlyn Rating ## Eden 4.374 ## Buskey 5.055 ## 1911 4.225 Row names must be unique, which can be a problem for long data frames (as opposed to wide), because usually the observations in a long data frame are uniquely identified by a combination of columns, rather than a single one–this is one of the reasons that Wickham &amp; Grolemund -Wickham and Grolemund (2017) argue against their use. Rather than the somewhat annoying set of code commands above, tibble does provide a nice one-step function appropriate for piping for transforming a tibble to a data frame with rownames: column_to_rownames(). This also has the nice side effect of deleting the (now redundant) column from the new data frame cider_chem %&gt;% column_to_rownames(&quot;Cider&quot;) ## pH TA (g/L) CO2 TRS (g/L) MAL(g/L) Polyphenols (ppm) Merlyn Rating ## Eden 3.69 4.27 4.54 18.67 3.62 203.56 4.374 ## Buskey 3.94 3.10 4.53 15.67 2.05 245.57 5.055 ## 1911 3.57 4.97 4.54 21.00 5.63 256.86 4.225 For matrices and higher-dimension data, it is more natural to use rownames. These are set with the same commands. 6.4 Examining big data frames For the most part, we are going to be dealing with data frames that have more rows than we can easily inspect in the R Console, or even the viewer. As you may have already found, we can open a simple “spreadsheet” view of dataframes by clicking on a dataset in the Environment pane–what this actually does is run View(&lt;data&gt;) in the Console. Personally, I don’t think this is the most effective way to inspect our data. We’ve already discussed the use of str() as a more effective way to get a summary of the structure of a data frame: str(cider_dryness) ## spec_tbl_df [294 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Sample_Name : chr [1:294] &quot;1911 Chilled&quot; &quot;Buskey Chilled&quot; &quot;Eden Chilled&quot; &quot;1911 RT&quot; ... ## $ Panelist_Code : chr [1:294] &quot;ejcole_vtu&quot; &quot;ejcole_vtu&quot; &quot;ejcole_vtu&quot; &quot;ejcole_vtu&quot; ... ## $ Panelist_Name : chr [1:294] &quot;Elizabeth Cole&quot; &quot;Elizabeth Cole&quot; &quot;Elizabeth Cole&quot; &quot;Elizabeth Cole&quot; ... ## $ Panelist_Display_Name: chr [1:294] &quot;Elizabeth&quot; &quot;Elizabeth&quot; &quot;Elizabeth&quot; &quot;Elizabeth&quot; ... ## $ Panelist_Email : chr [1:294] &quot;ejcole@vt.edu&quot; &quot;ejcole@vt.edu&quot; &quot;ejcole@vt.edu&quot; &quot;ejcole@vt.edu&quot; ... ## $ Dryness : num [1:294] 1 2.4 NA NA NA 3 2.3 3.5 2.8 4.2 ... ## $ Liking : num [1:294] 5 5 NA NA NA 5 6 7 4 2 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Sample_Name = col_character(), ## .. Panelist_Code = col_character(), ## .. Panelist_Name = col_character(), ## .. Panelist_Display_Name = col_character(), ## .. Panelist_Email = col_character(), ## .. Dryness = col_double(), ## .. Liking = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; This tells us: The type of object The structure of the object in terms of list/column structure: what kinds of data is in the object? “Metadata” about the object, which in R can be found in the atrributes. FWIW, these are accessed via the attributes() function (which can be very useful but hard to read). These can be set using the attr() function, but in general this is a level of detail you won’t need to deal with until you get into very advanced applications. 6.4.1 dplyr::glimpse() The dplyr package offers a somewhat beefed up version of str() called glimpse(). It gives us the same info about the size of the object and info about the types of the columns, but leaves out attributes and class information. This can be more readable, but isn’t necessarily that much better: glimpse(cider_dryness) ## Rows: 294 ## Columns: 7 ## $ Sample_Name &lt;chr&gt; &quot;1911 Chilled&quot;, &quot;Buskey Chilled&quot;, &quot;Eden Chilled&quot;… ## $ Panelist_Code &lt;chr&gt; &quot;ejcole_vtu&quot;, &quot;ejcole_vtu&quot;, &quot;ejcole_vtu&quot;, &quot;ejcol… ## $ Panelist_Name &lt;chr&gt; &quot;Elizabeth Cole&quot;, &quot;Elizabeth Cole&quot;, &quot;Elizabeth C… ## $ Panelist_Display_Name &lt;chr&gt; &quot;Elizabeth&quot;, &quot;Elizabeth&quot;, &quot;Elizabeth&quot;, &quot;Elizabet… ## $ Panelist_Email &lt;chr&gt; &quot;ejcole@vt.edu&quot;, &quot;ejcole@vt.edu&quot;, &quot;ejcole@vt.edu… ## $ Dryness &lt;dbl&gt; 1.0, 2.4, NA, NA, NA, 3.0, 2.3, 3.5, 2.8, 4.2, 2… ## $ Liking &lt;dbl&gt; 5, 5, NA, NA, NA, 5, 6, 7, 4, 2, 7, 5, 8, 6, 7, … However, for various reasons we will need different ways to look at parts of our data. One frequent situation I find myself in is the need to make sure that a calculated column (perhaps made via mutate()) is working properly. To do so, we might want to not just look at the top of the data set (printed normally via when we put the name of a tibble into the Console), since that might be systematically different than later observations. We can of course use indexing to access any part of the tibble, but this is going to get tedious fast: # Explain what calculation I am making here. cider_dryness %&gt;% group_by(Panelist_Name) %&gt;% # this has introduced a subtle error, why? mutate(liking_by_subject = mean(Liking, na.rm = TRUE)) %&gt;% # why does this show us the error? ungroup() %&gt;% .[94:103,] ## # A tibble: 10 × 8 ## Sample_Name Panelist_Code Paneli…¹ Panel…² Panel…³ Dryness Liking likin…⁴ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1911 RT blittleson_vtu Brenna … &lt;NA&gt; blittl… 2.3 7 4.83 ## 2 Buskey RT blittleson_vtu Brenna … &lt;NA&gt; blittl… 1.2 3 4.83 ## 3 Eden RT blittleson_vtu Brenna … &lt;NA&gt; blittl… 3.3 6 4.83 ## 4 1911 Chilled taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 2 6 5.40 ## 5 Buskey Chilled taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 1.5 4 5.40 ## 6 Eden Chilled taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 2 7 5.40 ## 7 1911 RT taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 4 8 5.40 ## 8 Buskey RT taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 2.5 7 5.40 ## 9 Eden RT taybenn_vtu &lt;NA&gt; &lt;NA&gt; tayben… 3 8 5.40 ## 10 1911 Chilled sophiedrew_vtu Sophie … Sophie sophie… 3.5 6 5.33 ## # … with abbreviated variable names ¹​Panelist_Name, ²​Panelist_Display_Name, ## # ³​Panelist_Email, ⁴​liking_by_subject As you might expect, we have some better tools to do this kind of quick access. 6.4.2 head()/tail() In base R, there are functions to access the first or last N rows. The default is for n = 6. head(cider_dryness) ## # A tibble: 6 × 7 ## Sample_Name Panelist_Code Panelist_Name Panelist_…¹ Panel…² Dryness Liking ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1911 Chilled ejcole_vtu Elizabeth Cole Elizabeth ejcole… 1 5 ## 2 Buskey Chilled ejcole_vtu Elizabeth Cole Elizabeth ejcole… 2.4 5 ## 3 Eden Chilled ejcole_vtu Elizabeth Cole Elizabeth ejcole… NA NA ## 4 1911 RT ejcole_vtu Elizabeth Cole Elizabeth ejcole… NA NA ## 5 Buskey RT ejcole_vtu Elizabeth Cole Elizabeth ejcole… NA NA ## 6 Eden RT ejcole_vtu Elizabeth Cole Elizabeth ejcole… 3 5 ## # … with abbreviated variable names ¹​Panelist_Display_Name, ²​Panelist_Email tail(cider_dryness, n = 10) ## # A tibble: 10 × 7 ## Sample_Name Panelist_Code Panelist…¹ Panel…² Panel…³ Dryness Liking ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Eden Chilled gail.billingsley_vtu Elizabeth… Elizab… gail.b… 4 2 ## 2 1911 RT gail.billingsley_vtu Elizabeth… Elizab… gail.b… 4.3 8 ## 3 Buskey RT gail.billingsley_vtu Elizabeth… Elizab… gail.b… 3.5 7 ## 4 Eden RT gail.billingsley_vtu Elizabeth… Elizab… gail.b… 1.7 6 ## 5 1911 Chilled kaylinf_vtu Kaylin Fi… Kaylin kaylin… 2.7 8 ## 6 Buskey Chilled kaylinf_vtu Kaylin Fi… Kaylin kaylin… 3.5 4 ## 7 Eden Chilled kaylinf_vtu Kaylin Fi… Kaylin kaylin… 2.4 7 ## 8 1911 RT kaylinf_vtu Kaylin Fi… Kaylin kaylin… 4 8 ## 9 Buskey RT kaylinf_vtu Kaylin Fi… Kaylin kaylin… 3.5 6 ## 10 Eden RT kaylinf_vtu Kaylin Fi… Kaylin kaylin… 3.5 3 ## # … with abbreviated variable names ¹​Panelist_Name, ²​Panelist_Display_Name, ## # ³​Panelist_Email These are useful (I find especially tail()) as a very quick quality check. 6.4.3 dplyr::slice_*() More powerful but slightly more complicated are the slice_*() functions from dplyr (part of the tidyverse). These offer more powerful ways to get a subset of data, and can actually be used in a more powerful, functional workflow. slice_head() and slice_tail() are equivalents to head() and tail(), but they also work with group_by(), so you can get the first/last N rows in each group. A next step up in power are slice_max() and slice_min(), which give you the N rows with the highest/lowest value of a specified variable: cider_dryness %&gt;% slice_max(n = 10, order_by = Dryness) ## # A tibble: 11 × 7 ## Sample_Name Panelist_Code Panelist_Name Panel…¹ Panel…² Dryness Liking ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1911 RT jkess_vtu Julia Kesselri… Julia jkess@… 5 7 ## 2 1911 RT lukes17_vtu Luke Shanholtz Luke lukes1… 5 5 ## 3 Buskey RT rrboyer_vtu Renee Boyer Renee rrboye… 4.9 4 ## 4 Eden Chilled klbyrwa_vtu &lt;NA&gt; &lt;NA&gt; klbyrw… 4.9 4 ## 5 1911 RT Rmawn_vtu Rachel Mawn Rachel Rmawn@… 4.8 6 ## 6 Eden RT Rmawn_vtu Rachel Mawn Rachel Rmawn@… 4.8 2 ## 7 1911 Chilled emilyplunkett_vtu &lt;NA&gt; &lt;NA&gt; emilyp… 4.8 4 ## 8 1911 RT rrboyer_vtu Renee Boyer Renee rrboye… 4.7 6 ## 9 1911 RT indiana22_vtu &lt;NA&gt; &lt;NA&gt; indian… 4.7 7 ## 10 1911 RT ccamryn3_vtu Camryn Grace C… Camryn ccamry… 4.6 8 ## 11 Eden Chilled vturesearch001 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4.6 2 ## # … with abbreviated variable names ¹​Panelist_Display_Name, ²​Panelist_Email Finally, slice_sample() gets a random set of rows from the dataset. This is a great way to pull a representative sample from throughout the dataset: cider_dryness %&gt;% slice_sample(n = 5) ## # A tibble: 5 × 7 ## Sample_Name Panelist_Code Panelist_Name Panel…¹ Panel…² Dryness Liking ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1911 Chilled emilyplunkett_vtu &lt;NA&gt; &lt;NA&gt; emilyp… 4.8 4 ## 2 Buskey RT Rmawn_vtu Rachel Mawn Rachel Rmawn@… 3.5 3 ## 3 Eden Chilled emilyplunkett_vtu &lt;NA&gt; &lt;NA&gt; emilyp… 3.3 7 ## 4 1911 Chilled jkess_vtu Julia Kesselr… Julia jkess@… 2.8 1 ## 5 Buskey Chilled meenak99_vtu &lt;NA&gt; &lt;NA&gt; meenak… 3.2 4 ## # … with abbreviated variable names ¹​Panelist_Display_Name, ²​Panelist_Email All of these also work with group_by(). 6.5 Generating data summaries So far, we’ve largely limited ourselves to just looking at data. This is the equivalent to drawing simple histograms from last week (although we’re not even counting data, yet)–we’re not getting any useful summaries out of our data, just checking for quality and making sure we understand what we have. The next step in descriptive data analysis and statistics is to extract numbers that summarize our data. 6.5.1 summary() The base R summary() function is a generic function in R–this means it will do different things depending on what kind of object we give it as its input. For example, when we give it a dataframe, it will give us information about the average and range of each column when the data is numeric, and not a whole lot when it is character data. It will also tell us a little bit about missing data (NAs). summary(cider_dryness) ## Sample_Name Panelist_Code Panelist_Name Panelist_Display_Name ## Length:294 Length:294 Length:294 Length:294 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## ## Panelist_Email Dryness Liking ## Length:294 Min. :1.000 Min. :1.000 ## Class :character 1st Qu.:2.200 1st Qu.:4.000 ## Mode :character Median :3.000 Median :6.000 ## Mean :2.925 Mean :5.306 ## 3rd Qu.:3.500 3rd Qu.:7.000 ## Max. :5.000 Max. :8.000 ## NA&#39;s :3 NA&#39;s :3 However, if we give it a model object (like that produced from simple linear regression with lm()) we will get very different output: summary(lm(Liking ~ Sample_Name, data = cider_dryness)) ## ## Call: ## lm(formula = Liking ~ Sample_Name, data = cider_dryness) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0612 -1.5306 0.3878 1.4375 3.4694 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.0612 0.2725 22.244 &lt; 2e-16 *** ## Sample_Name1911 RT 0.2304 0.3874 0.595 0.552371 ## Sample_NameBuskey Chilled -0.4490 0.3853 -1.165 0.244944 ## Sample_NameBuskey RT -1.4987 0.3874 -3.869 0.000135 *** ## Sample_NameEden Chilled -1.2904 0.3874 -3.331 0.000978 *** ## Sample_NameEden RT -1.5306 0.3853 -3.972 9.03e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.907 on 285 degrees of freedom ## (3 observations deleted due to missingness) ## Multiple R-squared: 0.1256, Adjusted R-squared: 0.1102 ## F-statistic: 8.187 on 5 and 285 DF, p-value: 3.069e-07 So summary() is a useful but unpredictable first step for understanding data. 6.5.2 skimr::skim() A more powerful approach that requires installing a separate package is the skim() function, from skimr. This is actually one of my favorite R tools, because it tells you so much so easily: skim(cider_dryness) Table 6.1: Data summary Name cider_dryness Number of rows 294 Number of columns 7 _______________________ Column type frequency: character 5 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Sample_Name 0 1.00 7 14 0 6 0 Panelist_Code 0 1.00 7 20 0 49 0 Panelist_Name 96 0.67 9 21 0 33 0 Panelist_Display_Name 108 0.63 3 9 0 29 0 Panelist_Email 18 0.94 10 26 0 46 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Dryness 3 0.99 2.93 0.93 1 2.2 3 3.5 5 ▅▇▇▇▂ Liking 3 0.99 5.31 2.02 1 4.0 6 7.0 8 ▂▂▆▅▇ In one command we get details about the size and shape of our data, as well as column summaries that are customaized for the different types of data (in this case, character and numeric). But skim() is actually even more powerful: for example, it supports group_by() to give us group summaries for some grouping variable. # Let&#39;s do a by-cider summary cider_dryness %&gt;% group_by(Sample_Name) %&gt;% skim() Table 6.2: Data summary Name Piped data Number of rows 294 Number of columns 7 _______________________ Column type frequency: character 4 numeric 2 ________________________ Group variables Sample_Name Variable type: character skim_variable Sample_Name n_missing complete_rate min max empty n_unique whitespace Panelist_Code 1911 Chilled 0 1.00 7 20 0 49 0 Panelist_Code 1911 RT 0 1.00 7 20 0 49 0 Panelist_Code Buskey Chilled 0 1.00 7 20 0 49 0 Panelist_Code Buskey RT 0 1.00 7 20 0 49 0 Panelist_Code Eden Chilled 0 1.00 7 20 0 49 0 Panelist_Code Eden RT 0 1.00 7 20 0 49 0 Panelist_Name 1911 Chilled 16 0.67 9 21 0 33 0 Panelist_Name 1911 RT 16 0.67 9 21 0 33 0 Panelist_Name Buskey Chilled 16 0.67 9 21 0 33 0 Panelist_Name Buskey RT 16 0.67 9 21 0 33 0 Panelist_Name Eden Chilled 16 0.67 9 21 0 33 0 Panelist_Name Eden RT 16 0.67 9 21 0 33 0 Panelist_Display_Name 1911 Chilled 18 0.63 3 9 0 29 0 Panelist_Display_Name 1911 RT 18 0.63 3 9 0 29 0 Panelist_Display_Name Buskey Chilled 18 0.63 3 9 0 29 0 Panelist_Display_Name Buskey RT 18 0.63 3 9 0 29 0 Panelist_Display_Name Eden Chilled 18 0.63 3 9 0 29 0 Panelist_Display_Name Eden RT 18 0.63 3 9 0 29 0 Panelist_Email 1911 Chilled 3 0.94 10 26 0 46 0 Panelist_Email 1911 RT 3 0.94 10 26 0 46 0 Panelist_Email Buskey Chilled 3 0.94 10 26 0 46 0 Panelist_Email Buskey RT 3 0.94 10 26 0 46 0 Panelist_Email Eden Chilled 3 0.94 10 26 0 46 0 Panelist_Email Eden RT 3 0.94 10 26 0 46 0 Variable type: numeric skim_variable Sample_Name n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Dryness 1911 Chilled 0 1.00 2.97 0.85 1.0 2.50 3.00 3.50 4.8 ▂▇▇▇▃ Dryness 1911 RT 1 0.98 3.32 0.99 1.5 2.45 3.25 4.12 5.0 ▆▅▃▇▅ Dryness Buskey Chilled 0 1.00 2.75 0.83 1.3 2.00 2.80 3.50 4.2 ▆▇▆▇▆ Dryness Buskey RT 1 0.98 2.86 0.88 1.2 2.22 2.75 3.50 4.9 ▅▇▆▆▂ Dryness Eden Chilled 1 0.98 2.76 1.00 1.0 2.00 2.65 3.50 4.9 ▅▇▆▅▂ Dryness Eden RT 0 1.00 2.90 0.93 1.0 2.30 3.00 3.50 4.8 ▅▆▆▇▃ Liking 1911 Chilled 0 1.00 6.06 1.75 1.0 5.00 6.00 7.00 8.0 ▂▁▃▅▇ Liking 1911 RT 1 0.98 6.29 1.44 2.0 6.00 6.50 7.00 8.0 ▁▂▁▅▇ Liking Buskey Chilled 0 1.00 5.61 1.82 1.0 4.00 6.00 7.00 8.0 ▁▂▃▃▇ Liking Buskey RT 1 0.98 4.56 1.84 1.0 3.00 5.00 6.00 7.0 ▃▂▃▂▇ Liking Eden Chilled 1 0.98 4.77 2.40 1.0 3.00 5.00 7.00 8.0 ▆▃▅▂▇ Liking Eden RT 0 1.00 4.53 2.05 1.0 3.00 4.00 6.00 8.0 ▃▅▇▂▅ We can even use skim() with it’s helper function yank() to make quick summary tables suitable for printing in a report: cider_dryness %&gt;% group_by(Sample_Name) %&gt;% skim() %&gt;% yank(skim_type = &quot;numeric&quot;) Variable type: numeric skim_variable Sample_Name n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Dryness 1911 Chilled 0 1.00 2.97 0.85 1.0 2.50 3.00 3.50 4.8 ▂▇▇▇▃ Dryness 1911 RT 1 0.98 3.32 0.99 1.5 2.45 3.25 4.12 5.0 ▆▅▃▇▅ Dryness Buskey Chilled 0 1.00 2.75 0.83 1.3 2.00 2.80 3.50 4.2 ▆▇▆▇▆ Dryness Buskey RT 1 0.98 2.86 0.88 1.2 2.22 2.75 3.50 4.9 ▅▇▆▆▂ Dryness Eden Chilled 1 0.98 2.76 1.00 1.0 2.00 2.65 3.50 4.9 ▅▇▆▅▂ Dryness Eden RT 0 1.00 2.90 0.93 1.0 2.30 3.00 3.50 4.8 ▅▆▆▇▃ Liking 1911 Chilled 0 1.00 6.06 1.75 1.0 5.00 6.00 7.00 8.0 ▂▁▃▅▇ Liking 1911 RT 1 0.98 6.29 1.44 2.0 6.00 6.50 7.00 8.0 ▁▂▁▅▇ Liking Buskey Chilled 0 1.00 5.61 1.82 1.0 4.00 6.00 7.00 8.0 ▁▂▃▃▇ Liking Buskey RT 1 0.98 4.56 1.84 1.0 3.00 5.00 6.00 7.0 ▃▂▃▂▇ Liking Eden Chilled 1 0.98 4.77 2.40 1.0 3.00 5.00 7.00 8.0 ▆▃▅▂▇ Liking Eden RT 0 1.00 4.53 2.05 1.0 3.00 4.00 6.00 8.0 ▃▅▇▂▅ skimr is so powerful that I recommend checking out the vignettes: browseVignettes(\"skimr\"). 6.5.3 group_by() and summarize()/count() Sometimes, however, you want to provide specific data summaries that are for your own exploratory use, and tools like summary() aren’t powerful enough, while tools like skim() provide too much detail. For example, you might want to check the number of times a sample is evaluated in a dataset, or the average value for different groups, or some custom calculation. There is a very flexible framework for that in the tidyverse, provided by the combination of group_by() and long (tidy) data. Our cider_dryness data set is in nearly tidy format: we have two observations per row, Liking and Dryness. We could fix this by using pivot_longer() to flip those into two columns called, for example, “Attribute” and “Rating”, but right now we’ll ignore it and just focus on the Liking variable. Recall that the group_by() function tells R that there is some column in the dataset that is a grouping variable–we can even have more than one grouping column, and R will find all combinations. So, for example, in this dataset we have Sample_Name, which tells us about the cider (and its serving temperature). There are 6 total unique sample names (3 ciders and 2 serving temperatures). What summarize() does is lets us define a summary statistic we want for each group. So, in this case, let’s imagine we want to know the mean, median, and standard deviation for the Liking of each cider. cider_dryness %&gt;% group_by(Sample_Name) %&gt;% summarize(mean_liking = mean(Liking, na.rm = TRUE), median_liking = median(Liking, na.rm = TRUE), sd_liking = sd(Liking, na.rm = TRUE)) ## # A tibble: 6 × 4 ## Sample_Name mean_liking median_liking sd_liking ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1911 Chilled 6.06 6 1.75 ## 2 1911 RT 6.29 6.5 1.44 ## 3 Buskey Chilled 5.61 6 1.82 ## 4 Buskey RT 4.56 5 1.84 ## 5 Eden Chilled 4.77 5 2.40 ## 6 Eden RT 4.53 4 2.05 This is a very powerful pattern for quickly getting summarized tables for either further analysis or for output to reports and publications. Frequently, one of the things we most want to know is “how many observations are there in a particular group?” summarize() provides a helper function for that, which only works in these calls: n(). So we’d write: cider_dryness %&gt;% group_by(Sample_Name) %&gt;% summarize(number_of_obs = n()) ## # A tibble: 6 × 2 ## Sample_Name number_of_obs ## &lt;chr&gt; &lt;int&gt; ## 1 1911 Chilled 49 ## 2 1911 RT 49 ## 3 Buskey Chilled 49 ## 4 Buskey RT 49 ## 5 Eden Chilled 49 ## 6 Eden RT 49 This is actually so important that there is a shortcut: count(), which allows us to skip the group_by() and just specify the counting variable within the function: cider_dryness %&gt;% count(Panelist_Code) ## # A tibble: 49 × 2 ## Panelist_Code n ## &lt;chr&gt; &lt;int&gt; ## 1 akyle99_vtu 6 ## 2 andy9221_vtu 6 ## 3 ann.sandbrook@vt.edu 6 ## 4 blittleson_vtu 6 ## 5 ccamryn3_vtu 6 ## 6 ccomber1_vtu 6 ## 7 cdavis98_vtu 6 ## 8 cierrajames22_vtu 6 ## 9 deckerj6_vtu 6 ## 10 dhsawhney_vtu 6 ## # … with 39 more rows ## # ℹ Use `print(n = ...)` to see more rows Of course, this can be used on grouped data separately, but it saves typing of lines of code. It also allows the user to specify multiple grouping variables within count() if desired. This is especially useful when you are dealing with datasets that may be incomplete, rather than the result of experimental designs that are forced to be complete. Let’s observe this by dropping missing data from our cider_dryness data using the useful drop_na() utility function. cider_dryness %&gt;% drop_na(Liking) %&gt;% count(Sample_Name) ## # A tibble: 6 × 2 ## Sample_Name n ## &lt;chr&gt; &lt;int&gt; ## 1 1911 Chilled 49 ## 2 1911 RT 48 ## 3 Buskey Chilled 49 ## 4 Buskey RT 48 ## 5 Eden Chilled 48 ## 6 Eden RT 49 Now we can see that our data are incomplete, and we have to decide how we might want to deal with that. Let’s find out where our missing data are: cider_dryness %&gt;% drop_na(Liking) %&gt;% count(Panelist_Code) %&gt;% filter(n &lt; 6) # why did we ask for panelists with less than 6 ratings? ## # A tibble: 1 × 2 ## Panelist_Code n ## &lt;chr&gt; &lt;int&gt; ## 1 ejcole_vtu 3 We can drop this panelist from our dataset if we are concerned about their data making our design unbalanced. 6.6 Statistics review We have now begun to move from describing our “raw” data–the type of data we have, its shape and structure–to developing ways in which we want to explore teh data to provide descriptive summaries and other information about our data. For the rest of this class we’re going to explore some common summary statistics that you have certainly encountered before. We’re going to see how to calculate these in R, how to fit them into the data analysis workflows we’ve started to explore, and how to think about these. I think it’s worth paraphrasing Hadley Wickham -Wickham and Grolemund (2017) here on what I mean by exploratory statistics. We are often taught in basic statistics that the purpose of our analyses are inference: testing hypotheses that are (hopefully) generalizable. But what we are doing today–and for most of the class, and in a lot of our work in general–is description. Wickham writes: Traditionally, the focus of modelling is on inference, or for confirming that an hypothesis is true. Doing this correctly is not complicated, but it is hard. There is a pair of ideas that you must understand in order to do inference correctly: Each observation can either be used for exploration or confirmation, not both. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration. This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading. What we are doing in this class is exploring data. We will often find illuminating patterns that can spur further research–this is exactly the pattern that led to the generation of the real data we’re using to learn with. But we should be cautious about using these data to actually build models that we think predict as-yet-unobserved phenomena. We don’t have the right supporting structure for that kind of inference right now. 6.6.1 Measures of central tendency Most often, we start by asking what single value best summarizes the whole range of our observations. Typically, this will be a measure of central tendency, meaning a value around which our observations are centered. We are all pretty familiar with the idea of an average, but it is worth reviewing to think through what these mean from the perspective of summarizing our data. 6.6.1.1 Mean The (arithmetic) mean has the familiar form of \\(\\bar{x} = \\frac{x_{1} + x_{2} + ... + x_{n}}{n}\\). This is the expected value of our variable–the value which, given no other information except our sample, is our best guess for the value of x. In R, the mean() function takes a vector and returns the mean value. It will throw an error if there is missing data (e.g., NA). mean(apples$His) ## [1] 0.4214286 mean(cider_dryness$Liking) ## [1] NA mean(cider_dryness$Liking, na.rm = TRUE) # use na.rm = TRUE to drop NAs before the mean is calculated ## [1] 5.305842 It is important to note that mean() works on vectors. What is wrong with the following function call? mean(1, 2, 3, 4, 5) # What should the mean be? ## [1] 1 The mean is biased by the shape of our distribution: it is a good guess at central tendency when our data are clumped up around a central value, but it becomes less and less realistic when our data are very spread out (e.g., high variance) have long tails in one direction or another have multiple “clumps” (e.g., bimodal data) What do you think the mean tells us about the following artificial data sets? # Let&#39;s make some pathological data tibble(normal = rnorm(100, mean = 0, sd = 1), spread_out = rnorm(100, mean = 0, sd = 10), long_tailed = c(rnorm(90, mean = 0, sd = 1), rnorm(10, mean = 50, sd = 5)), bimodal = c(rnorm(50, mean = -2, sd = 1), rnorm(50, mean = 2, sd = 1)), obs = 1:100) %&gt;% pivot_longer(names_to = &quot;distribution&quot;, values_to = &quot;x&quot;, -obs) %&gt;% # And now we&#39;ll plot it to look at how the means work ggplot(aes(x = x)) + geom_histogram(color = &quot;white&quot;, fill = &quot;grey&quot;) + # This messy layer just calculates means for each group and plots them as a dashed line geom_vline(aes(xintercept = mean_x), data = . %&gt;% group_by(distribution) %&gt;% summarize(mean_x = mean(x)), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) + theme_classic() 6.6.1.1.1 Other means: harmonic, geometric, etc There are other ways to calculate a mean that satisfy different definitions. For example, the geometric mean is the n-th root of the product of n numbers: \\(\\bar{x}_{geom} = \\sqrt[n]{x_{1} * x_{2} * ... * x_{n}}\\). The geometric mean makes sense when considering the central tendency of sets of numbers that will naturally be multiplied, or for other naturally exponential phenomena like growth rates. It can be calculated as psych::geometric.mean(). The third common mean is the harmonic mean, defined as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations: \\(\\bar{x}_{harm} = \\frac{n}{\\frac{1}{x_{1}} + \\frac{1}{x_{2}} + ... + \\frac{1}{x_{n}}}\\). The harmonic mean does not suffer from the influence of outliers in the same way as the arithmetic mean. It can be calculated as psych::harmonic.mean() The relationship between the three means is generally \\(\\bar{x}_{harm} &lt; \\bar{x}_{geom} &lt; \\bar{x}\\), except in the case where all observations are equal. For our dataset: psych::harmonic.mean(cider_dryness$Liking, na.rm = TRUE) ## [1] 4.010369 psych::geometric.mean(cider_dryness$Liking, na.rm = TRUE) ## [1] 4.768266 mean(cider_dryness$Liking, na.rm = TRUE) ## [1] 5.305842 6.6.1.2 Median In contrast to the mean, the median is the central value in our observations when they are ordered from least to greatest. The median, therefore, is not affected by extreme outliers and long tails in the ways that the mean(s) typically are. For this reason the median is often called “nonparametric”–it is an equally good estimator of the central tendency even when our data is not parameterized by the “clumped around the center” distribution typical of the normal (bell-shaped) distribution. tibble(normal = rnorm(100, mean = 0, sd = 1), long_tailed = c(rnorm(90, mean = 0, sd = 1), rnorm(10, mean = 50, sd = 5)), obs = 1:100) %&gt;% pivot_longer(names_to = &quot;distribution&quot;, values_to = &quot;x&quot;, -obs) %&gt;% # And now we&#39;ll plot it to look at how the means work ggplot(aes(x = x)) + geom_histogram(color = &quot;white&quot;, fill = &quot;grey&quot;) + # This messy layer just calculates means for each group and plots them as a dashed line geom_vline(aes(xintercept = median_x), data = . %&gt;% group_by(distribution) %&gt;% summarize(median_x = median(x)), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + facet_wrap(~distribution, scales = &quot;free&quot;) + theme_classic() In R, we calculate the median using median(). The same arguments and warnings from mean() apply here. median(apples$His) ## [1] 0.365 median(cider_dryness$Liking) ## [1] NA median(10, 1, 1, 1, 1) # why is this wrong? ## [1] 10 6.6.1.3 Mode In most statistics classes, we learn about the mode and quickly forget it. The mode is defined as the value of x that occurs most frequently, which means that for real-valued (numeric) variables, it is not very useful. But the mode is actually really valuable when we think about categorical variables, which may be extremely frequent in our datasets, especially if we collect observational datasets. For example, we might ask In 100 petri dishes, which microbial spp. is most abundant most often? In sensory studies of cider, which word(s) get used most frequently by consumers? Which soil type is found most frequently in 100 random tracts sampled from around Blacksburg? We can sometimes represent these statistics as means or medians, but we are really talking about modes and categorical variables. Therefore, it’s a shame that R doesn’t actually have a function to easily calculate the mode! It is pretty easy to use the group_by()/count()/slice_*() to find the mode of a dataset: # How can we use these functions to find the most frequently occurring category? However, it might be better to define a function that works more like mean() and median(). This solution is given at Stack Overflow. Modes &lt;- function(x) { ux &lt;- unique(x) tab &lt;- tabulate(match(x, ux)) ux[tab == max(tab)] } Modes(cider_dryness$Liking) ## [1] 7 This introduces a nice utility function: unique() takes a vector and tells you what values are in it: unique(c(1, 1, 1, 2, 3, 3, 4)) ## [1] 1 2 3 4 ggplot(aes(x = Liking), data = cider_dryness) + geom_density() + geom_vline(aes(xintercept = mean(Liking, na.rm = TRUE)), linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_vline(aes(xintercept = median(Liking, na.rm = TRUE)), linetype = &quot;dotted&quot;, color = &quot;red&quot;) + geom_vline(aes(xintercept = Modes(Liking)), linetype = &quot;dotdash&quot;, color = &quot;red&quot;) + annotate(&quot;label&quot;, x = c(6, mean(cider_dryness$Liking, na.rm = TRUE), Modes(cider_dryness$Liking)), y = c(0.05, 0.07, 0.08), label = c(&quot;median&quot;, &quot;mean&quot;, &quot;mode&quot;)) + theme_classic() ## Warning: Removed 3 rows containing non-finite values (stat_density). Or, to sum it all up, we could calculate some kind of average of means… XKCD proposes a better mean. 6.6.2 Variation No matter what single value we choose as a representative of our data, it is unlikely to truly communicate the shape of our data. Partly, that is why I am emphasizing the use of simple plots–in many cases these will be much better for exploring our data than even a couple numeric summary statistics. We can continue to enrich our numerical summaries, however, by describing the variation of our observations as well as their central tendency. There are a couple common measures for variation that will be useful. 6.6.2.1 Variance The most common way to report variation is variance (and its square root, standard deviation). Variance is intimately related to the arithmetic mean: it is defined (for a sample) as the average squared distance from the mean: \\(\\sigma^2 = \\frac{1}{n - 1}\\sum{(x_{i}-\\bar{x})^2}\\). We define it this way in order to account for the fact that we expect both positive and negative differences from the mean; otherwise we’d end up with an average distance of 0. The standard deviation is just the (positive) square root of variance: \\(sd = \\sqrt{\\sigma^2}\\). Standard deviation, however, has the advantage of being in the same units as the mean (and the original variable), meaning that we can interpret standard deviation as the average distance an observation will be away from the mean, which is a more intuitive measurement. In R, we calculate variance with var() and standard deviation with sd(). Both take vectors and need to account for NAs. var(apples$His) ## [1] 0.08257352 sd(apples$His) ## [1] 0.2873561 var(cider_dryness$Liking) # how do we fix this? ## [1] NA 6.6.2.2 Quantiles and the Interquartile Range A related concept which we’ve encountered already in drawing boxplots is the idea of the Interquartile Range (IQR)–for a dataset, the IQR is the difference between the 25th and 75th percentiles of the dataset. Understanding the IQR requires understanding the idea of quantiles: in non-technical terms, quantiles are the division of an ordered set of values into groups with equal numbers of values per groups. The idea is best demonstrated with some common, named quantiles: If we divide into 10 groups, we have deciles. If we divide into 100 groups, we have percentiles. If we talk about someone “being in the 95th percentile” with regard to some test score (say GRE), we mean that their score falls into the range of scores (for their cohort) that is the 95th group. If we divide into 4 groups, we will have quartiles and have 4 sets of groups each with the same number of observations. The values that are at the edge of each group define the quartile ranges. Obviously in the case of the IQR we are rewriting in terms of percentiles, but it should be clear that the 25th and 75th percentile are the 2nd and 3rd quartile, respectively. The median is in fact the number that represents a split into two quantiles (the upper and lower half of the observations). Therefore, the median will also fall at the 2nd quartile (the 50th percentile or 5th decile). The IQR represents an alternative measure of variation. A traditional observation in intro statistics is that, when the data are normal-ish, “~2/3 of the observations fall within 1 sd of the mean”. But as we see in our cider_dryness and apples data we can have quite non-normal distributions. The IQR is another “nonparametric” estimate, in that it doesn’t rely on any assumptions of how our data are made (i.e., distributed)–rather, we just order our data and draw lines at the 25th and 75th percentiles. If the IQR is broad, we have a lot of “spread” (variation) in our data; if the IQR is narrow observations tend to be close to our central measure (the median or 50th percentile in this case). This can help inform our expectations and observations. We can get the IQR in quite a few ways. If we want the cut points for the percentiles (the values for the 25th and 75th percentiles), the summary() function on any numeric vectors actually returns that as part of its summarization. If we want the actual distance between the boundaries, the IQR() function returns that. The fivenum() function returns Tukey’s proposed data summary, which is the minimum, the maximimum, the median, and the 25th and 75th percentile observations. summary(apples$His) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.2350 0.3650 0.4214 0.6025 1.3200 IQR(apples$His) ## [1] 0.3675 fivenum(apples$His) ## [1] 0.000 0.230 0.365 0.610 1.320 The main difference between summary() and fivenum() is that the latter will try to return percentile values that are actually observations in the dataset, whereas summary() will generally interpolate values that may not actually occur in the dataset. 6.6.3 Linear Association Linear association, in data analysis, is closely associated with the ideas of covariance and correlation. You’ve probably learned about these both in statistics courses, and I will be going into each briefly here. But my goal here is to motivate our thinking about these concepts with more concrete examples, as well as some thinking about the persistent power of the “linear model”, despite (or because of) its limitations. We’ll find that, as an explanatory and exploratory tool, linear models are quite powerful, even when they are obviously incorrect. Let’s start by looking at scatterplots of data from our apples dataset. Scatterplots have a strong, visual relationship to our idea of linear association. We look only at 8/20 measured amino acids for visual clarity. ggplot(apples) + geom_point(aes(x = .panel_x, y = .panel_y)) + geom_autodensity() + facet_matrix(vars(3:10), layer.diag = 2) + theme_bw() In many cases, it appears that there is a cloud of points with no clear relationship in each box. In other cases (for example, Asp ~ Glu and Asn ~ Ser) there seem to be “associations” between the values. What do we mean by an association? Very broadly, we mean that the magnitude of one of the variables tells us something about the magnitude of the other. So, for example, it appears that higher values of Aspartate are related to higher values of Glutamate (with the exception of one very strong outlier). Although we don’t see any obvious examples in this visualization, we might also see the opposite kind of trends–if a high value of one variable is predictably paired with a low value of another, that would still be a linear association: just a negative one. What other trends can we see in these plots? 6.6.3.1 Covariance The idea that two variables change together, predictably, is formalized in the idea of covariance. Covariance between two variables, \\(x\\) and \\(y\\), is formally defined as \\(cov(x, y) = \\sigma_{xy} = \\frac{1}{n - 1}\\sum{(x_{i} - \\bar{x}) * ({y_{i} - \\bar{y})}}\\). There is no reason at all for you to memorize this (I had to look it up to make sure I was gettig the definition right). But looking at the equation gives us a couple insights its worth remembering: There must be some intrinsic relationship between \\(x\\) and \\(y\\) for the concept of covariance (and linear association in general) to make sense. For example, if \\(x\\) is temperature in Blacksburg over the last week measured at 10-minute intervals, and \\(y\\) is the molar mass of every common carbohydrate, we have two numeric variables that have no intrinsic relationship whatsoever. We cannot pair values of \\(x\\) and \\(y\\) in this case–we do not have the \\(i\\) subscript connecting observations. We can’t even really graph these two variables against each other in a scatterplot (a sure warning sign). In the case of apples, we can pair \\(x\\) and \\(y\\) as different amino acids, because they are measured on the same sample (\\(i\\)). Ignoring the normalization factor (\\(\\frac{1}{n - 1}\\)), which scales covariance so that it doesn’t grow with more observations, the equation says “for each observation \\(i\\), take how far \\(x_{i}\\) and \\(y_{i}\\) are from their central tendencies, and multiply them, then add up all these pairs”. So if when \\(x_{i}\\) is bigger than average, \\(y_{i}\\) is also bigger than average, their product will be very large, and if the reverse is true (\\(x_{i}\\) small, \\(y_{i}\\) small), the negatives will cancel when multiplied, and so \\(\\sigma_{xy}\\) will get very large. If the situation is completely reversed (\\(x_{i}\\) small, \\(y_{i}\\) big or vice versa), then \\(\\sigma_{xy}\\) will grow very negative (why?) If there is no association, and we cannot guess anything about \\(y_{i}\\) from \\(x_{i}\\), then \\(\\sigma_{xy}\\) will be quite small (why?) Notice that we are not scaling \\(x\\) or \\(y\\). So if we \\(x\\) is on the order of magnitude of, say 1e4, and \\(y\\) is on the order of magnitude of 1e-2, variation in \\(x\\) will dominate the covariance. It means we cannot directly compare covariances among different pairs of variables. Oof, that was mathy. The main point is to get a conceptual handle on the idea of covariance, as it actually underlies a lot of the statistical inference we will do. We want to know if and when two variables tend to vary together, as it can help us form and test hypotheses about underlying causes. In R, we can get covariance between two variables by using the cov() function. We can get a single covariance by giving two vectors to cov(x, y), but we need to be sure they are ordered by the intrinsic connecting variable. So in the case of apples, that is we need to have the same Variety and Rep ID variables for each observation: (correct_covariance &lt;- cov(apples$Asp, apples$Glu)) ## [1] 0.8754642 (incorrect_covariance &lt;- cov(apples$Asp, apples$Glu[sample(1:42)])) ## [1] -0.01823577 correct_covariance == incorrect_covariance ## [1] FALSE It is more common, when working with the type of multivariate datasets we usually have (and have in this case), to examine a “covariance matrix”–this is the numerical equivalent of the kind of scatterplot matrix we saw above. It is a symmetrical matrix (what does this mean?) in which each cell contains the covariance of the variables for the row and column, and the diagonal gives the variance of each variable: round(cov(apples[,-c(1, 2)]), 3) ## His Asn Ser Gln Arg Gly Asp Glu Thr Ala ## His 0.083 0.282 0.013 0.037 0.006 0.000 0.087 0.047 -0.006 0.027 ## Asn 0.282 99.205 2.107 6.634 -0.286 0.304 12.785 4.419 0.398 3.966 ## Ser 0.013 2.107 0.075 0.183 0.027 0.010 0.307 0.204 0.008 0.087 ## Gln 0.037 6.634 0.183 0.694 0.075 0.027 0.960 0.596 0.009 0.332 ## Arg 0.006 -0.286 0.027 0.075 0.116 0.008 0.089 0.250 0.002 0.056 ## Gly 0.000 0.304 0.010 0.027 0.008 0.003 0.043 0.036 0.001 0.019 ## Asp 0.087 12.785 0.307 0.960 0.089 0.043 2.465 0.875 0.043 0.645 ## Glu 0.047 4.419 0.204 0.596 0.250 0.036 0.875 0.956 0.024 0.319 ## Thr -0.006 0.398 0.008 0.009 0.002 0.001 0.043 0.024 0.011 0.004 ## Ala 0.027 3.966 0.087 0.332 0.056 0.019 0.645 0.319 0.004 0.296 ## GABA 0.012 1.286 0.038 0.096 0.012 0.005 0.253 0.110 0.005 0.067 ## Pro 0.012 0.946 0.029 0.100 0.038 0.007 0.187 0.131 -0.001 0.108 ## Cys -0.003 -0.136 -0.004 -0.019 -0.002 0.000 -0.028 -0.014 0.000 -0.006 ## Lys -0.001 -0.241 0.001 -0.016 0.020 0.002 -0.055 0.031 0.003 -0.006 ## Tyr -0.008 -0.465 -0.013 -0.077 0.006 0.001 -0.146 -0.037 0.004 -0.025 ## Met -0.006 0.092 0.004 -0.038 -0.005 0.002 -0.001 -0.010 0.007 -0.004 ## Val -0.039 -3.168 -0.086 -0.273 0.004 -0.012 -0.511 -0.201 0.000 -0.160 ## Ile -0.001 -0.302 -0.008 -0.075 0.003 0.001 -0.108 -0.022 0.011 -0.029 ## Leu 0.004 -0.861 -0.011 -0.068 0.019 0.000 -0.162 -0.004 0.000 -0.037 ## Phe -0.131 -25.601 -0.878 -2.268 -0.274 -0.125 -4.326 -1.965 0.033 -1.398 ## GABA Pro Cys Lys Tyr Met Val Ile Leu Phe ## His 0.012 0.012 -0.003 -0.001 -0.008 -0.006 -0.039 -0.001 0.004 -0.131 ## Asn 1.286 0.946 -0.136 -0.241 -0.465 0.092 -3.168 -0.302 -0.861 -25.601 ## Ser 0.038 0.029 -0.004 0.001 -0.013 0.004 -0.086 -0.008 -0.011 -0.878 ## Gln 0.096 0.100 -0.019 -0.016 -0.077 -0.038 -0.273 -0.075 -0.068 -2.268 ## Arg 0.012 0.038 -0.002 0.020 0.006 -0.005 0.004 0.003 0.019 -0.274 ## Gly 0.005 0.007 0.000 0.002 0.001 0.002 -0.012 0.001 0.000 -0.125 ## Asp 0.253 0.187 -0.028 -0.055 -0.146 -0.001 -0.511 -0.108 -0.162 -4.326 ## Glu 0.110 0.131 -0.014 0.031 -0.037 -0.010 -0.201 -0.022 -0.004 -1.965 ## Thr 0.005 -0.001 0.000 0.003 0.004 0.007 0.000 0.011 0.000 0.033 ## Ala 0.067 0.108 -0.006 -0.006 -0.025 -0.004 -0.160 -0.029 -0.037 -1.398 ## GABA 0.033 0.024 -0.001 -0.004 -0.013 0.006 -0.061 -0.003 -0.010 -0.480 ## Pro 0.024 0.052 -0.001 0.002 -0.002 0.003 -0.054 -0.006 -0.006 -0.608 ## Cys -0.001 -0.001 0.006 0.002 0.008 0.005 0.012 0.013 0.010 0.081 ## Lys -0.004 0.002 0.002 0.013 0.017 0.013 0.023 0.023 0.018 0.087 ## Tyr -0.013 -0.002 0.008 0.017 0.054 0.026 0.057 0.048 0.031 0.118 ## Met 0.006 0.003 0.005 0.013 0.026 0.048 0.013 0.043 0.022 -0.046 ## Val -0.061 -0.054 0.012 0.023 0.057 0.013 0.225 0.065 0.061 1.383 ## Ile -0.003 -0.006 0.013 0.023 0.048 0.043 0.065 0.099 0.046 0.343 ## Leu -0.010 -0.006 0.010 0.018 0.031 0.022 0.061 0.046 0.054 0.401 ## Phe -0.480 -0.608 0.081 0.087 0.118 -0.046 1.383 0.343 0.401 18.893 Notice that, even though this is a table showing covariances among different amino acid concentrations, we have vastly different scales: some of our amino acids (Asparagine, Phenylalanine) have variances that are orders of magnitude larger than others. This makes it hard to understand which covariances are actually important. 6.6.3.2 Correlation The final point (#3) above leads us to the idea of linear or Pearson correlation, which can be thought of as “standardized covariance”. The standardization is done by dividing the covariance \\(x\\) and \\(y\\) by the standard deviations of each variable: \\(cor(x, y) = \\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_{x}*\\sigma_{y}}\\). This preserves the directionality (positive or negative) of the covariance, but accounts for differences in scale in \\(x\\) and \\(y\\)–in fact, it means that \\(\\rho_{xy}\\) is bounded within \\([-1, 1]\\). If the correlation is equal to +1, it means that there is a perfect, positive (linear) association between \\(x\\) and \\(y\\), and if it is equal to -1 it means that there is a perfect negative association. If it is 0 it means there is no linear association. We can calculate correlations (and correlation matrices) in R using the cor() function. It works exactly the same as cov(), including the requirement that we be careful with making sure that the intrinsic variable(s) connecting \\(x\\) and \\(y\\) aren’t disrupted. cor(apples$Asp, apples$Glu) ## [1] 0.5703138 cor(apples$Asp, apples$Glu[sample(1:42)]) ## [1] 0.009757854 And, typically, we will generate correlation matrices instead: round(cor(apples[, -c(1, 2)]), 3) ## His Asn Ser Gln Arg Gly Asp Glu Thr Ala ## His 1.000 0.099 0.170 0.155 0.057 -0.016 0.193 0.166 -0.203 0.171 ## Asn 0.099 1.000 0.774 0.800 -0.084 0.570 0.818 0.454 0.377 0.732 ## Ser 0.170 0.774 1.000 0.803 0.285 0.652 0.715 0.761 0.266 0.585 ## Gln 0.155 0.800 0.803 1.000 0.264 0.613 0.734 0.732 0.100 0.732 ## Arg 0.057 -0.084 0.285 0.264 1.000 0.450 0.167 0.750 0.048 0.303 ## Gly -0.016 0.570 0.652 0.613 0.450 1.000 0.509 0.695 0.253 0.669 ## Asp 0.193 0.818 0.715 0.734 0.167 0.509 1.000 0.570 0.257 0.755 ## Glu 0.166 0.454 0.761 0.732 0.750 0.695 0.570 1.000 0.232 0.600 ## Thr -0.203 0.377 0.266 0.100 0.048 0.253 0.257 0.232 1.000 0.071 ## Ala 0.171 0.732 0.585 0.732 0.303 0.669 0.755 0.600 0.071 1.000 ## GABA 0.234 0.716 0.761 0.638 0.197 0.560 0.894 0.622 0.236 0.687 ## Pro 0.179 0.415 0.461 0.527 0.485 0.597 0.521 0.586 -0.037 0.868 ## Cys -0.140 -0.179 -0.204 -0.305 -0.086 -0.041 -0.237 -0.184 0.013 -0.155 ## Lys -0.042 -0.216 0.023 -0.170 0.514 0.268 -0.311 0.282 0.265 -0.104 ## Tyr -0.122 -0.201 -0.203 -0.399 0.070 0.061 -0.400 -0.164 0.149 -0.200 ## Met -0.097 0.042 0.072 -0.207 -0.071 0.196 -0.002 -0.049 0.292 -0.031 ## Val -0.285 -0.671 -0.665 -0.692 0.026 -0.465 -0.687 -0.434 0.000 -0.619 ## Ile -0.012 -0.097 -0.091 -0.287 0.024 0.067 -0.219 -0.072 0.332 -0.171 ## Leu 0.054 -0.371 -0.177 -0.352 0.235 -0.034 -0.444 -0.017 0.002 -0.293 ## Phe -0.105 -0.591 -0.738 -0.626 -0.185 -0.537 -0.634 -0.462 0.072 -0.591 ## GABA Pro Cys Lys Tyr Met Val Ile Leu Phe ## His 0.234 0.179 -0.140 -0.042 -0.122 -0.097 -0.285 -0.012 0.054 -0.105 ## Asn 0.716 0.415 -0.179 -0.216 -0.201 0.042 -0.671 -0.097 -0.371 -0.591 ## Ser 0.761 0.461 -0.204 0.023 -0.203 0.072 -0.665 -0.091 -0.177 -0.738 ## Gln 0.638 0.527 -0.305 -0.170 -0.399 -0.207 -0.692 -0.287 -0.352 -0.626 ## Arg 0.197 0.485 -0.086 0.514 0.070 -0.071 0.026 0.024 0.235 -0.185 ## Gly 0.560 0.597 -0.041 0.268 0.061 0.196 -0.465 0.067 -0.034 -0.537 ## Asp 0.894 0.521 -0.237 -0.311 -0.400 -0.002 -0.687 -0.219 -0.444 -0.634 ## Glu 0.622 0.586 -0.184 0.282 -0.164 -0.049 -0.434 -0.072 -0.017 -0.462 ## Thr 0.236 -0.037 0.013 0.265 0.149 0.292 0.000 0.332 0.002 0.072 ## Ala 0.687 0.868 -0.155 -0.104 -0.200 -0.031 -0.619 -0.171 -0.293 -0.591 ## GABA 1.000 0.577 -0.048 -0.205 -0.301 0.156 -0.714 -0.061 -0.231 -0.612 ## Pro 0.577 1.000 -0.047 0.059 -0.045 0.055 -0.499 -0.083 -0.107 -0.612 ## Cys -0.048 -0.047 1.000 0.178 0.449 0.311 0.321 0.559 0.567 0.245 ## Lys -0.205 0.059 0.178 1.000 0.645 0.523 0.437 0.642 0.692 0.179 ## Tyr -0.301 -0.045 0.449 0.645 1.000 0.516 0.514 0.658 0.570 0.117 ## Met 0.156 0.055 0.311 0.523 0.516 1.000 0.127 0.632 0.423 -0.048 ## Val -0.714 -0.499 0.321 0.437 0.514 0.127 1.000 0.434 0.557 0.672 ## Ile -0.061 -0.083 0.559 0.642 0.658 0.632 0.434 1.000 0.626 0.252 ## Leu -0.231 -0.107 0.567 0.692 0.570 0.423 0.557 0.626 1.000 0.396 ## Phe -0.612 -0.612 0.245 0.179 0.117 -0.048 0.672 0.252 0.396 1.000 By scaling our variables, we no longer have the problem of differences in measurement scale overwhelming possible associations. A good example here is the relationship between Leucine and Isoleucine–their covariance is only 0.046, because they are both present in very low levels in these apples, but when scaled we see there is a pretty strong linear association as represented by correlation: 0.626. 6.6.3.3 Caveats with linear association Linear association is powerful and simple, and in fact when we start to get into ideas of inference in the next several weeks, we will see that its simplicity offers some advantages. But it also is very rigid and can ignore clear patterns in data. Kieran Healy -Healy (2019) argues that this is a good reason for relying equally on data visualization in exploratory data analysis, using the famous Anscombe dataset (Anscombe 1973). tidy_anscombe %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + facet_wrap(~series, scales = &quot;free&quot;) + theme_bw() This is a set of 4 artificial datasets that are constructed with the constraint that for each, \\(\\rho_{xy}\\) is the same. But it is obvious that the same pattern is not at play in each. For each dataset, explain what pattern you see, and whether the drawn line showing correlation is a good fit: Series 1: Series 2: Series 3: Series 4: Healy gives his own, expanded version of this type of example at his website, in arguing that, when we are exploring data, it is worth plotting rather than making assumptions (for example, that associations are linear). This is a practice I recommend, and it will also help you develop your skills with data wrangling and visualization. References References "],["advancing-our-wrangling.html", "7 Advancing our wrangling 7.1 Today’s data 7.2 Review of control flow 7.3 Using vectorization in R 7.4 Relational data 7.5 Writing readable and reproducible scripts References", " 7 Advancing our wrangling The week after Spring Break is a good time for wrapping up loose ends and taking a breath. The goal for this week is to go over some final loose ends for data wrangling and basic exploration, then to pivot to your presentation of your first data analysis projects. Following this week we’re going to be turning to (statistical) inference and modeling, while of course continuing to struggle with and develop skills in data wrangling and visualization. So today we have a bit of a hodgepodge. We’re going to start by quickly reviewing concepts of control flow, and then looking at some common, powerful alternatives you’ll encounter for “vectorizing” operations in R. Then, we’re going to talk about some basic concepts of “relational data” (if you’ve ever used SQL or =VLOOKUP()/=HLOOKUP() functions in Excel you’ll know what we’re talking about). Finally, we’ll talk a little bit about “readable” programs and scripts, with some notes on how it’s a good idea to keep notes for yourselves and other for reproducible research. 7.1 Today’s data We’re going to use data from a paper my group published on developing a “Check All That Applies” (CATA) scale for hard ciders (Phetxumphou, Cox, and Lahne 2020). The dataset we are going to be using consists of two tables: a table of the CATA attributes (and metadata) on a per-panelist, per-sample basis (CiderCataAttributes.csv; this table could be thought of as “semi-tidy”) and a table of rated liking (on a 9-pt scale) on a per-panelist basis (cider_liking.csv; this is a wide dataset). We are going to import and tidy the data here. # For the cider CATA table cider_cata &lt;- read_csv(&quot;Files/Week 9/CiderCataAttributes.csv&quot;) %&gt;% select(Sample_Name, Panelist_Name, FermentedApples:Smooth) # For the liking table cider_liking &lt;- read_csv(&quot;Files/Week 9/cider_liking.csv&quot;) Our data looks like this right off the bat: # There are too many columns in cider_cata to display nicely head(cider_cata) %&gt;% select(1:5, 28) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = FALSE) Sample_Name Panelist_Name FermentedApples OverripeApples FreshApples Smooth Bold rock Paulette Cairns 0 0 1 1 Buskeys Paulette Cairns 1 0 0 0 Blue Bee Paulette Cairns 0 0 0 0 Potters Paulette Cairns 1 0 0 0 Cobbler Mountain Paulette Cairns 0 1 1 0 Big Fish Paulette Cairns 0 0 0 0 head(cider_liking) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = FALSE) Panelist_Name Bold rock Buskeys Blue Bee Potters Cobbler Mountain Big Fish Paulette Cairns 8 4 1 5 5 3 Renata Vieira Carneiro 8 8 4 4 6 4 Jennifer Acuff 4 6 5 6 7 6 Lily Yang 8 4 3 3 9 6 Brandon Lester 7 8 3 8 4 8 Harry Schonberger 4 4 2 7 4 4 We can also tidy up the data (get it into long form) as good practice and because it’ll make our lives easier for certain things, later. # Tidy the CATA data cider_cata %&gt;% pivot_longer(names_to = &quot;cata_attribute&quot;, values_to = &quot;checked&quot;, -c(&quot;Sample_Name&quot;, &quot;Panelist_Name&quot;)) -&gt; tidy_cider_cata # Tidy the liking data cider_liking %&gt;% pivot_longer(names_to = &quot;cider_name&quot;, values_to = &quot;liking&quot;, -&quot;Panelist_Name&quot;) -&gt; tidy_cider_liking And now our data are in the format we generally like to see: head(tidy_cider_cata) %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_styling(full_width = FALSE) Sample_Name Panelist_Name cata_attribute checked Bold rock Paulette Cairns FermentedApples 0 Bold rock Paulette Cairns OverripeApples 0 Bold rock Paulette Cairns FreshApples 1 Bold rock Paulette Cairns Alcohol 0 Bold rock Paulette Cairns WineLike 0 Bold rock Paulette Cairns Floral 0 head(tidy_cider_liking) %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_styling(full_width = FALSE) Panelist_Name cider_name liking Paulette Cairns Bold rock 8 Paulette Cairns Buskeys 4 Paulette Cairns Blue Bee 1 Paulette Cairns Potters 5 Paulette Cairns Cobbler Mountain 5 Paulette Cairns Big Fish 3 7.2 Review of control flow Control flow, as we’ve learned since we first covered it, is at the heart of coding for research. Being able to automate choices (conditional logic, like if statements) and iterations (loops, like for loops) is where much of the power in coding lies. However, these ideas aren’t necessarily intuitive or clear at first, and so I think we will all benefit from a brief review. This is especially true today, because we’re going to talk about some approaches that build on the basic intuitions of control flow to accomplish more, more quickly. 7.2.1 Loops Loops, or more formally, iteration, are one of the key “computers are good at this” tasks. A loop is a line or lines of code that we want to execute repeatedly, often changing some part of the input of the code. For example, we might want to use a vector of \\(x\\)-values as input to some function \\(f(x)\\) and save all of the results. Rather than repeatedly cutting and pasting the \\(f(x)\\) for each \\(x\\), we can write a for loop that feeds each \\(x\\) into the function. For a trivial example, we might want to get the results of \\(f(x) = \\frac{1}{sin^2(x)}\\) for \\(x = (1, 2, 5, 10)\\). We could write: 1 / sin(1)^2 ## [1] 1.412283 1 / sin(2)^2 ## [1] 1.20945 1 / sin(5)^2 ## [1] 1.087505 1 / sin(10)^2 ## [1] 3.378847 But this is tedious and prone to errors. It is also not feasible when we have more than a handful of \\(x\\)-values. How can we rewrite this as a loop? # How can we rewrite the same calculations above as a loop? for(inverse in c(1, 2, 5, 10)){ result &lt;- 1 / sin(inverse)^2 print(result) } ## [1] 1.412283 ## [1] 1.20945 ## [1] 1.087505 ## [1] 3.378847 Obviously, this is not a real example, but you can imagine how this could be useful for, for example, calculating some kind of conversion or other function based on your experimental results–you might want to convert from measured weight change in vial to brix in a fermentation, for example, at a set of time points. We have two basic loops available to us in R: for and while loops. In general, for loops require us to tell R how many times we want our code to execute, whereas while loops are more general, and can work even when you don’t know how many iterations you want. A key thing to remember about loops is that any variable created within the loop will be overwritten every time the loop re-executes. Therefore, you must either create a space to store the output of the loop or be executing the code for its “side effects”. 7.2.1.1 for loop review Recall that the syntax for a for loop requires you to set some sort of counter or index, which defines the number of iterations. In a real experiment, a common index might be the experimental units or treatments. In our cider data for today, we have 6 ciders as experimental treatments, and 56 subjects as experimental units. A legitimate question we could ask about the CATA data is: Averaged across experimental units (ciders), how many attributes did each subject check? We can use a for loop to iterate across subjects to answer this question: for(subject in unique(tidy_cider_cata$Panelist_Name)){ attributes_checked &lt;- tidy_cider_cata %&gt;% filter(Panelist_Name == subject) %&gt;% pull(checked) paste(subject, &quot;checked, on average,&quot;, sum(attributes_checked) / 6, &quot;attributes.&quot;) %&gt;% print() } ## [1] &quot;Paulette Cairns checked, on average, 6 attributes.&quot; ## [1] &quot;Renata Vieira Carneiro checked, on average, 4.66666666666667 attributes.&quot; ## [1] &quot;Jennifer Acuff checked, on average, 3.16666666666667 attributes.&quot; ## [1] &quot;Lily Yang checked, on average, 6.66666666666667 attributes.&quot; ## [1] &quot;Brandon Lester checked, on average, 4.33333333333333 attributes.&quot; ## [1] &quot;Harry Schonberger checked, on average, 5.66666666666667 attributes.&quot; ## [1] &quot;Grace Earnhart checked, on average, 4.33333333333333 attributes.&quot; ## [1] &quot;Andrea Green checked, on average, 5.33333333333333 attributes.&quot; ## [1] &quot;Kathryn Racine checked, on average, 5.5 attributes.&quot; ## [1] &quot;Elizabeth Clark checked, on average, 4.16666666666667 attributes.&quot; ## [1] &quot;Julien Cadot checked, on average, 4.16666666666667 attributes.&quot; ## [1] &quot;Yanhong He checked, on average, 5.16666666666667 attributes.&quot; ## [1] &quot;Elizabeth Chang checked, on average, 5.83333333333333 attributes.&quot; ## [1] &quot;Kaysha Perrin checked, on average, 6.5 attributes.&quot; ## [1] &quot;Tessa Batterton checked, on average, 5.16666666666667 attributes.&quot; ## [1] &quot;Sean O&#39;Keefe checked, on average, 5.83333333333333 attributes.&quot; ## [1] &quot;Connor Owens checked, on average, 6 attributes.&quot; ## [1] &quot;Casey Feher checked, on average, 7.5 attributes.&quot; ## [1] &quot;Austin Skeens checked, on average, 6.5 attributes.&quot; ## [1] &quot;Kim Waterman checked, on average, 4.5 attributes.&quot; ## [1] &quot;Michael Hughes checked, on average, 5.5 attributes.&quot; ## [1] &quot;Aili Wang checked, on average, 6.33333333333333 attributes.&quot; ## [1] &quot;J&#39;Nai Phillips checked, on average, 7.16666666666667 attributes.&quot; ## [1] &quot;Vincent Kessinger checked, on average, 6.33333333333333 attributes.&quot; ## [1] &quot;Claire Marik checked, on average, 4.5 attributes.&quot; ## [1] &quot;Renee Boyer checked, on average, 4 attributes.&quot; ## [1] &quot;Deepak Poudel checked, on average, 5 attributes.&quot; ## [1] &quot;Hannah Patton checked, on average, 4.33333333333333 attributes.&quot; ## [1] &quot;Joell Eifert checked, on average, 6 attributes.&quot; ## [1] &quot;Brittany Carter checked, on average, 7.5 attributes.&quot; ## [1] &quot;Jasmil Perez checked, on average, 5.83333333333333 attributes.&quot; ## [1] &quot;Amy Moore checked, on average, 5.66666666666667 attributes.&quot; ## [1] &quot;Ellen Krupar checked, on average, 6.5 attributes.&quot; ## [1] &quot;Kiri DeBose checked, on average, 5.16666666666667 attributes.&quot; ## [1] &quot;Amy Rasor checked, on average, 6.83333333333333 attributes.&quot; ## [1] &quot;Elizabeth Merin checked, on average, 9.5 attributes.&quot; ## [1] &quot;James Baker checked, on average, 3.83333333333333 attributes.&quot; ## [1] &quot;Lauren Krauss checked, on average, 2.83333333333333 attributes.&quot; ## [1] &quot;Bryce Swanson checked, on average, 6.66666666666667 attributes.&quot; ## [1] &quot;Pamela Pack checked, on average, 4.83333333333333 attributes.&quot; ## [1] &quot;Nathan Briggs checked, on average, 9.83333333333333 attributes.&quot; ## [1] &quot;Beck Hall checked, on average, 5.5 attributes.&quot; ## [1] &quot;Beth Kirby checked, on average, 6.33333333333333 attributes.&quot; ## [1] &quot;Andrea Dietrich checked, on average, 5.33333333333333 attributes.&quot; ## [1] &quot;Velva Groover checked, on average, 4.16666666666667 attributes.&quot; ## [1] &quot;Laura Rasnick checked, on average, 4.16666666666667 attributes.&quot; ## [1] &quot;Maureen Sroufe checked, on average, 3.66666666666667 attributes.&quot; ## [1] &quot;Betti Kreye checked, on average, 4.5 attributes.&quot; ## [1] &quot;josh obenhaus checked, on average, 2.5 attributes.&quot; ## [1] &quot;Brennan Shepard checked, on average, 4.5 attributes.&quot; ## [1] &quot;Jay Sim checked, on average, 2.66666666666667 attributes.&quot; ## [1] &quot;Ted Sindabizera Ntwari checked, on average, 3 attributes.&quot; ## [1] &quot;Jamie Chen checked, on average, 6.5 attributes.&quot; ## [1] &quot;Sungwoo Kim checked, on average, 4.66666666666667 attributes.&quot; ## [1] &quot;Sydney Goy checked, on average, 3.66666666666667 attributes.&quot; ## [1] &quot;matthew stephen ferby checked, on average, 5 attributes.&quot; Note that we used the paste() and print() functions to write the average of each subject’s checked attributes to the console/the R Markdown. But if we looked up subject and attributes_checked, which are both variables created in the loop, we see only the values for the last iteration: subject ## [1] &quot;matthew stephen ferby&quot; attributes_checked ## [1] 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 ## [38] 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 ## [75] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 0 ## [112] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 ## [149] 0 1 1 0 0 0 1 0 If we really wanted these results (and remember that there are better ways to get them, such as the group_by() and summarize() workflow), we would want to create a vector or dataframe to store each iteration’s results into. 7.2.1.2 while loop review On the other hand, while loops execute repeatedly until the condition in the loop is FALSE. So, for example, the following loop would run infinitely, requiring you to force-stop the loop in the console: while(TRUE){ print(&quot;This loop will run forever.&quot;) } This seems really useless, but, especially in simulations, we might want to be able to run a block of code for an unknown number of iterations. Or, if you are writing code that will be deployed (this is really moving into coding for production, rather than coding for research), you might not know the “shape” of the input you’re going to get, which will make while loops more flexible. Briefly, while loops are in fact more general versions of for loops–any for loop can be rewritten as a while loop, but the reverse is untrue (for example, the trivial infinite loop above cannote be rewritten as a for loop). We could, however, use the break command with some conditional to break out of an infinite loop to replicate the for loop above as a while loop. 7.2.2 Conditionals Often, we want to have our code make simple decisions for us. We want to be able to execute one kind of code in some conditions, and another in other conditions. This is where if (and the helper else) statements come in handy. By wrapping a piece of code in an if statement, the code will only be executed if the logical test associated with the if is TRUE. For a trivial example in our data set, we might want to print out the name of every cider that is rated at &gt; 7 on the liking scale. To do so, we can use an if statement, along with a for loop. # start with a loop to step through every line of the tidy_cider_liking data frame for(i in 1:nrow(tidy_cider_liking)){ # we look at the i-th liking rating and compare it to 7, then print something if(tidy_cider_liking$liking[i] &gt; 7) print(paste(tidy_cider_liking$Panelist_Name[i], &quot;liked&quot;, tidy_cider_liking$cider_name[i], &quot;a lot.&quot;)) } ## [1] &quot;Paulette Cairns liked Bold rock a lot.&quot; ## [1] &quot;Renata Vieira Carneiro liked Bold rock a lot.&quot; ## [1] &quot;Renata Vieira Carneiro liked Buskeys a lot.&quot; ## [1] &quot;Lily Yang liked Bold rock a lot.&quot; ## [1] &quot;Lily Yang liked Cobbler Mountain a lot.&quot; ## [1] &quot;Brandon Lester liked Buskeys a lot.&quot; ## [1] &quot;Brandon Lester liked Potters a lot.&quot; ## [1] &quot;Brandon Lester liked Big Fish a lot.&quot; ## [1] &quot;Grace Earnhart liked Big Fish a lot.&quot; ## [1] &quot;Andrea Green liked Bold rock a lot.&quot; ## [1] &quot;Andrea Green liked Buskeys a lot.&quot; ## [1] &quot;Andrea Green liked Blue Bee a lot.&quot; ## [1] &quot;Julien Cadot liked Blue Bee a lot.&quot; ## [1] &quot;Julien Cadot liked Potters a lot.&quot; ## [1] &quot;Elizabeth Chang liked Bold rock a lot.&quot; ## [1] &quot;Kaysha Perrin liked Bold rock a lot.&quot; ## [1] &quot;Kaysha Perrin liked Big Fish a lot.&quot; ## [1] &quot;Connor Owens liked Bold rock a lot.&quot; ## [1] &quot;Connor Owens liked Buskeys a lot.&quot; ## [1] &quot;Connor Owens liked Cobbler Mountain a lot.&quot; ## [1] &quot;Casey Feher liked Bold rock a lot.&quot; ## [1] &quot;Casey Feher liked Potters a lot.&quot; ## [1] &quot;Austin Skeens liked Cobbler Mountain a lot.&quot; ## [1] &quot;Michael Hughes liked Bold rock a lot.&quot; ## [1] &quot;Vincent Kessinger liked Bold rock a lot.&quot; ## [1] &quot;Deepak Poudel liked Bold rock a lot.&quot; ## [1] &quot;Deepak Poudel liked Big Fish a lot.&quot; ## [1] &quot;Hannah Patton liked Bold rock a lot.&quot; ## [1] &quot;Hannah Patton liked Buskeys a lot.&quot; ## [1] &quot;Hannah Patton liked Cobbler Mountain a lot.&quot; ## [1] &quot;Jasmil Perez liked Bold rock a lot.&quot; ## [1] &quot;Jasmil Perez liked Buskeys a lot.&quot; ## [1] &quot;Amy Moore liked Potters a lot.&quot; ## [1] &quot;Amy Moore liked Big Fish a lot.&quot; ## [1] &quot;Ellen Krupar liked Bold rock a lot.&quot; ## [1] &quot;Kiri DeBose liked Bold rock a lot.&quot; ## [1] &quot;Elizabeth Merin liked Bold rock a lot.&quot; ## [1] &quot;Elizabeth Merin liked Big Fish a lot.&quot; ## [1] &quot;Bryce Swanson liked Bold rock a lot.&quot; ## [1] &quot;Bryce Swanson liked Big Fish a lot.&quot; ## [1] &quot;Nathan Briggs liked Blue Bee a lot.&quot; ## [1] &quot;Nathan Briggs liked Potters a lot.&quot; ## [1] &quot;Nathan Briggs liked Big Fish a lot.&quot; ## [1] &quot;Beck Hall liked Blue Bee a lot.&quot; ## [1] &quot;Beck Hall liked Potters a lot.&quot; ## [1] &quot;Beth Kirby liked Bold rock a lot.&quot; ## [1] &quot;Beth Kirby liked Potters a lot.&quot; ## [1] &quot;Beth Kirby liked Big Fish a lot.&quot; ## [1] &quot;Velva Groover liked Bold rock a lot.&quot; ## [1] &quot;Laura Rasnick liked Bold rock a lot.&quot; ## [1] &quot;Maureen Sroufe liked Bold rock a lot.&quot; ## [1] &quot;Maureen Sroufe liked Cobbler Mountain a lot.&quot; ## [1] &quot;Maureen Sroufe liked Big Fish a lot.&quot; ## [1] &quot;Betti Kreye liked Bold rock a lot.&quot; ## [1] &quot;Brennan Shepard liked Buskeys a lot.&quot; ## [1] &quot;Brennan Shepard liked Blue Bee a lot.&quot; ## [1] &quot;Jay Sim liked Bold rock a lot.&quot; ## [1] &quot;Jamie Chen liked Bold rock a lot.&quot; ## [1] &quot;Sydney Goy liked Potters a lot.&quot; ## [1] &quot;matthew stephen ferby liked Cobbler Mountain a lot.&quot; Again, this is a kind of trivial example; a better approach would be to use indexing (either directly, using [] or using the tidyverse filter()), but it demonstrates the basic workflow. Often, we’ll use if statements programmatically in order to write functions that can make basic decisions 7.3 Using vectorization in R If you dig into the help posts on StackExchange/Overflow, you’ll frequently run across solutions using “vectorized” approaches, and explanations of speed improvements (for complex tasks) that recommend vectorization over loops. While the specifics of vectorization are beyond the scope of this class (and, indeed, beyond my ability to explain clearly or accurately), the basic concept isn’t hard: R is built around the idea of efficiently applying transformations to vectorized data, whereas loops like for and while break down vectors into each of their elements and then apply transformations to each element. I think that an analogy would be something like this: a_sequence &lt;- 1:1e4 # vectorized approach: tic() sum(a_sequence) ## [1] 50005000 toc() ## 0.002 sec elapsed # looped approach tic() sum_amount &lt;- 0 for(i in 1:length(a_sequence)) sum_amount &lt;- sum_amount + a_sequence[i] toc() ## 0.003 sec elapsed These both work fine, but you can see that there is more overhead for the loop - storing and erasing variables as it proceeds along the chain. You can imagine that, in a language that is built for the former, doing the latter is going to be less quick (and less memory-efficient, which is not going to be a problem for you in most initial applications, but can be a serious barrier). Why doesn’t everyone just learn the vectorized approaches off the bat, then? Well, there are two main reasons: Not all programming languages support vectorization in the same way as R, or even at all, but loops and conditionals are in general features of every programming language. Therefore, it is easier to learn about and use the looped structure. Loops are more intuitive to test and write–and it is always better to get code that works first, then make it efficient second. So sometimes it is better to sketch out a for loop (especially because the pseudocode for such a structure is easy to conceptualize), and then, if necessary, figure out the vectorized version. In coding for research, we often just need our code to run correctly and reproducibly–we don’t care as much about efficiency. So briefly learning about vectorization is important, because it may become necessary, but it may also be something you just need to recognize, rather than be able to produce effectively on your own. 7.3.1 tidyverse approaches: map() family Usually we start our discussion with base R functions and then move on the simpler-but-require-loading-a-package tidyverse functions. In this case, however, we’re going to do the reverse, because I think that the purrr::map() family of functions is so much simpler and easier to understand than the base R functions that I think the concept will be much clearer. In effect, we use map() whenever we want to apply the same function to every element of a vector. This is a lot like a for loop, in which we step through each element (usually using a line of code that is something like for(i in 1:nrow(vector))). But map() and its relations do this faster and with less typing–the latter is not only because we’re lazy, but because if we type less we’re less likely to make mistakes. So, for a toy example, imagine we want to get the mean liking for each cider in the cider_liking data frame. # What happens if we just try to take the mean of the data frame? mean(cider_liking[,-1]) ## Warning in mean.default(cider_liking[, -1]): argument is not numeric or logical: ## returning NA ## [1] NA # We can take one mean at a time: mean(cider_liking$`Bold rock`) ## [1] 6.696429 mean(cider_liking$Buskeys) ## [1] 6.053571 # We could step through with a loop through the columns for(variable in 2:ncol(cider_liking)) print(mean(cider_liking[[variable]])) ## [1] 6.696429 ## [1] 6.053571 ## [1] 3.946429 ## [1] 4.982143 ## [1] 4.821429 ## [1] 5.607143 # Or we can use map() to do this cleanly # (We may want to drop the Panelist_Name column because it is a character) cider_liking %&gt;% select(-Panelist_Name) %&gt;% map(mean) ## $`Bold rock` ## [1] 6.696429 ## ## $Buskeys ## [1] 6.053571 ## ## $`Blue Bee` ## [1] 3.946429 ## ## $Potters ## [1] 4.982143 ## ## $`Cobbler Mountain` ## [1] 4.821429 ## ## $`Big Fish` ## [1] 5.607143 The key to understanding what is happening is that a data frame (and so a tibble) is a vector of vectors (that are all the same length, so that it is “rectangular”): str(cider_liking) ## spec_tbl_df [56 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Panelist_Name : chr [1:56] &quot;Paulette Cairns&quot; &quot;Renata Vieira Carneiro&quot; &quot;Jennifer Acuff&quot; &quot;Lily Yang&quot; ... ## $ Bold rock : num [1:56] 8 8 4 8 7 4 3 8 4 5 ... ## $ Buskeys : num [1:56] 4 8 6 4 8 4 7 9 6 6 ... ## $ Blue Bee : num [1:56] 1 4 5 3 3 2 2 8 2 2 ... ## $ Potters : num [1:56] 5 4 6 3 8 7 4 7 7 6 ... ## $ Cobbler Mountain: num [1:56] 5 6 7 9 4 4 3 6 4 5 ... ## $ Big Fish : num [1:56] 3 4 6 6 8 4 8 7 4 7 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Panelist_Name = col_character(), ## .. `Bold rock` = col_double(), ## .. Buskeys = col_double(), ## .. `Blue Bee` = col_double(), ## .. Potters = col_double(), ## .. `Cobbler Mountain` = col_double(), ## .. `Big Fish` = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; As we started by saying, map() applies a function to all of the elements of a vector. So, here, map() is going to each column (we dropped the Panelist_Name column to avoid getting an NA b/c it is a character) and applying the function we asked for: mean(). By default, map() returns a list of the same length as the original vector, which has names matching the elements it is applied along. If we want to not have a list at the end, we could use the unlist() function at the end of our pipe to collapse down to a vector: cider_liking %&gt;% select(-Panelist_Name) %&gt;% map(mean) %&gt;% unlist() ## Bold rock Buskeys Blue Bee Potters ## 6.696429 6.053571 3.946429 4.982143 ## Cobbler Mountain Big Fish ## 4.821429 5.607143 But I keep talking about the map() family, and what I mean by this is functions like map_dbl(), which includes this step and will either return a vector of doubles (see the _dbl?) or will return an error. This lets you be sure it is doing what you think it is doing. cider_liking %&gt;% select(-Panelist_Name) %&gt;% map_dbl(mean) ## Bold rock Buskeys Blue Bee Potters ## 6.696429 6.053571 3.946429 4.982143 ## Cobbler Mountain Big Fish ## 4.821429 5.607143 There are equivalent versions for all the standard data types in R. There are also more exotic versions that return data frames, or that use conditional logic (map_if()), but I am not going to cover these here. 7.3.1.1 Anonymous functions In the example above, we used mean() as the function we passed to map(). But what if you wanted to do something that isn’t already a named function? For example, what if you want to get all numbers greater than the mean liking of each column? This isn’t a pre-defined function. The first option we might think of is that we can write such a function, give it a name, and then use map() to apply it: above_the_mean &lt;- function(numbers){ numbers[numbers &gt; mean(numbers)] } cider_liking %&gt;% select(-Panelist_Name) %&gt;% map(above_the_mean) ## $`Bold rock` ## [1] 8 8 8 7 8 8 8 7 9 8 7 8 7 7 8 7 8 8 7 7 9 7 8 8 9 7 8 7 8 8 8 9 8 8 8 7 ## ## $Buskeys ## [1] 8 8 7 9 7 7 8 7 7 7 7 8 8 7 7 7 7 7 7 7 7 7 8 7 ## ## $`Blue Bee` ## [1] 4 5 8 8 6 4 4 4 7 7 4 4 6 5 4 5 7 4 9 8 4 7 4 6 5 8 6 4 6 ## ## $Potters ## [1] 5 6 8 7 7 7 6 8 6 7 6 6 7 8 5 7 7 6 7 8 6 5 9 8 8 6 5 6 6 8 6 ## ## $`Cobbler Mountain` ## [1] 5 6 7 9 6 5 7 6 8 8 6 6 7 8 6 7 7 7 5 7 5 8 6 7 7 5 8 ## ## $`Big Fish` ## [1] 6 6 8 8 7 7 7 6 8 7 7 6 7 8 6 6 7 8 6 8 6 8 6 9 7 8 7 8 7 7 6 6 Note that here we are using map() because we are returning a “ragged” list, where each element has a different length. But this means that, if this function is something we just want that one time, we now have it floating around in our memory. This is annoying. R actually allows for the definition of “anonymous functions on the fly within other function calls, by just giving the function without assigning it to a name: cider_liking %&gt;% select(-Panelist_Name) %&gt;% map(function(x) x[x &gt; mean(x)]) # conventionally we use `x` as the variable name in anonymous functions ## $`Bold rock` ## [1] 8 8 8 7 8 8 8 7 9 8 7 8 7 7 8 7 8 8 7 7 9 7 8 8 9 7 8 7 8 8 8 9 8 8 8 7 ## ## $Buskeys ## [1] 8 8 7 9 7 7 8 7 7 7 7 8 8 7 7 7 7 7 7 7 7 7 8 7 ## ## $`Blue Bee` ## [1] 4 5 8 8 6 4 4 4 7 7 4 4 6 5 4 5 7 4 9 8 4 7 4 6 5 8 6 4 6 ## ## $Potters ## [1] 5 6 8 7 7 7 6 8 6 7 6 6 7 8 5 7 7 6 7 8 6 5 9 8 8 6 5 6 6 8 6 ## ## $`Cobbler Mountain` ## [1] 5 6 7 9 6 5 7 6 8 8 6 6 7 8 6 7 7 7 5 7 5 8 6 7 7 5 8 ## ## $`Big Fish` ## [1] 6 6 8 8 7 7 7 6 8 7 7 6 7 8 6 6 7 8 6 8 6 8 6 9 7 8 7 8 7 7 6 6 But purrr and the tidyverse actually use an even more compact “lambda function” or “one-sided formula” notation for this: cider_liking %&gt;% select(-Panelist_Name) %&gt;% map(~ .x[.x &gt; mean(.x)]) ## $`Bold rock` ## [1] 8 8 8 7 8 8 8 7 9 8 7 8 7 7 8 7 8 8 7 7 9 7 8 8 9 7 8 7 8 8 8 9 8 8 8 7 ## ## $Buskeys ## [1] 8 8 7 9 7 7 8 7 7 7 7 8 8 7 7 7 7 7 7 7 7 7 8 7 ## ## $`Blue Bee` ## [1] 4 5 8 8 6 4 4 4 7 7 4 4 6 5 4 5 7 4 9 8 4 7 4 6 5 8 6 4 6 ## ## $Potters ## [1] 5 6 8 7 7 7 6 8 6 7 6 6 7 8 5 7 7 6 7 8 6 5 9 8 8 6 5 6 6 8 6 ## ## $`Cobbler Mountain` ## [1] 5 6 7 9 6 5 7 6 8 8 6 6 7 8 6 7 7 7 5 7 5 8 6 7 7 5 8 ## ## $`Big Fish` ## [1] 6 6 8 8 7 7 7 6 8 7 7 6 7 8 6 6 7 8 6 8 6 8 6 9 7 8 7 8 7 7 6 6 In this notation, ~ takes the place of function(x), and .x takes the place of the variable being passed. In fact, if you are using a single variable for your function, you can even just use . to mean “all the data”–purrr calls this a “pronoun”, but it does make the above code punishingly hard to read (too compact can be as bad as too long)! I suggest reading the documentation for map() as well as the relevant sections in R4DS (21.5-21.8) if you’re interested in this subject. map() and its family are very powerful, although they can be difficult to get the hang of at first. 7.3.2 Base R approaches: apply() family Unfortunately, if you think map() is a little hard to learn, the base R approach is not necessarily going to be easier. The base R approach to vectorization involves the apply() function and its related cousins. I’ll just mention two: lapply()–which is meant to signifify “list apply()”, and base apply(). Essentially, lapply() acts the same way as map()–it applies a function (its second argument) to every element of a list (vector), and returns a list of the same length. However, because it’s not part of the tidyverse it can’t use lambda functions: cider_liking %&gt;% select(-Panelist_Name) %&gt;% lapply(mean) ## $`Bold rock` ## [1] 6.696429 ## ## $Buskeys ## [1] 6.053571 ## ## $`Blue Bee` ## [1] 3.946429 ## ## $Potters ## [1] 4.982143 ## ## $`Cobbler Mountain` ## [1] 4.821429 ## ## $`Big Fish` ## [1] 5.607143 Meanwhile, apply() by itself expects “rectangular” data as input: it will also take higher-dimensional arrays. It is fast and powerful, but the shape of data it returns is kind of unpredictable. It works by specifying the “margin” or “axis” you want to apply the function across. So, if we have a 2-d data structure, MARGIN = 1 will apply() a function to each row; MARGIN = 2 will apply() the function to each column. Once we get to higher dimensions, care (and good visualization skills) are required. # By applying mean to the columns, we can recreate our map()/lapply() example cider_liking %&gt;% column_to_rownames(&quot;Panelist_Name&quot;) %&gt;% apply(2, mean) ## Bold rock Buskeys Blue Bee Potters ## 6.696429 6.053571 3.946429 4.982143 ## Cobbler Mountain Big Fish ## 4.821429 5.607143 # By using MARGIN = 1, we can get the mean for each subject cider_liking %&gt;% column_to_rownames(&quot;Panelist_Name&quot;) %&gt;% apply(1, mean) %&gt;% head() ## Paulette Cairns Renata Vieira Carneiro Jennifer Acuff ## 4.333333 5.666667 5.666667 ## Lily Yang Brandon Lester Harry Schonberger ## 5.500000 6.333333 4.166667 You will also see other versions of these: sapply(), vapply(), tapply(), etc. These functions are powerful but not necesssarily intuitive. You will encounter them in help files and when searching for answers, and you can learn to use them, but I wouldn’t necessarily recommend them as your first solution. 7.4 Relational data Often, our datasets will include multiple data tables that give different information about related (or the same) samples, observations, etc. A common example might be in survey responses: you might have one table of the survey responses for each question, identified by the respondent. Then you might have a second table of demographic attribtues for each respondent. In this case, the respondent is the key variable–it links the table of responses to the table of demographics. You could somehow use this relationship to understand, for example, the average responses to each question on a demographic category basis. It’s easy to imagine other situations in which this kind of relationship happens: A table of time-series outcomes (for microbial growth, fermentation, quality parameters) on a set of samples, and a table of sample treatment characteristics. Microbiological, chemical, and colorimetric readings on the same set of samples, produced by different instruments. A derived (summary) table linked back to the table it was generated from. A master production table for product lots on a per-plant/per-field basis, with a master ID linking each lot to any tests you might want to run! I picked the dataset for today after a quick browse for a simple but not uninteresting two-table dataset. Looking at tidy_cider_cata and tidy_cata_liking, answer the following questions: What is the key variable(s) that uniquely identifiies and links an observation between the two tables? What is the information that is unique to each table? (Hint: this one should be clear from the names.) Often, when we are presented with data in multiple tables, we will want to be able to somehow merge the data tables. Because each table is uniquely identified, there isn’t one single right way to do so–the merges we want to make will depend on our reason for wanting to make them. For example, above I suggested we might want to merge demographic and survey responses in order to average responses for, say, different racial or ethnic groups. I like the typology of merging data that is given in Stat 545: Bind–this involves just taking 2 (or more) data frames of the same dimension and smashing them together. This is a naive-but-easy approach to merging data tables, and it is something to be used with caution if you use it at all. This relies on you to get it right by manually ordering the rows or columns, and if you don’t there won’t be any warning when things are incorrect. An easy way to screw up your analysis. Join/Lookup–these terms come from database management, and rely on setting a variable(s) that uniquely identifies cases (rows) to produce a new table. These are better because they will fail in predictable ways if your data don’t actually match. 7.4.1 *bind() functions R provides two base functions for merging rectangular data: rbind(): add together two or more tables on a row-wise basis. This in essence “stacks” the data. cbind(): add together two or more tables on a column-wise basis. This in essence “lines up” the tables side by side. 7.4.1.1 row binding If our data is in wide format, we usually have one row per case/subject/sample, with multiple variables in the column. If our data is in long/tidy format, we usually have one observation per row. In either case, if we wanted to add together two tables row-wise, we would usually be adding new observations. In our particular example, if we look at our tidied data sets, we have one table in which we have collected rating liking (tidy_cider_liking) and one in which we have collected binary presence/absence of a bunch of sensory attributes. Maybe we want to treat liking as another sensory attribute, and add it to our data frame: rbind(tidy_cider_cata, tidy_cider_liking) ## Error in rbind(deparse.level, ...): numbers of columns of arguments do not match Oops! If we look at the structure of our two dataframes we’ll see that they’re not equivalent: str(tidy_cider_cata) ## tibble [8,736 × 4] (S3: tbl_df/tbl/data.frame) ## $ Sample_Name : chr [1:8736] &quot;Bold rock&quot; &quot;Bold rock&quot; &quot;Bold rock&quot; &quot;Bold rock&quot; ... ## $ Panelist_Name : chr [1:8736] &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; ... ## $ cata_attribute: chr [1:8736] &quot;FermentedApples&quot; &quot;OverripeApples&quot; &quot;FreshApples&quot; &quot;Alcohol&quot; ... ## $ checked : num [1:8736] 0 0 1 0 0 0 0 0 1 0 ... str(tidy_cider_liking) ## tibble [336 × 3] (S3: tbl_df/tbl/data.frame) ## $ Panelist_Name: chr [1:336] &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; &quot;Paulette Cairns&quot; ... ## $ cider_name : chr [1:336] &quot;Bold rock&quot; &quot;Buskeys&quot; &quot;Blue Bee&quot; &quot;Potters&quot; ... ## $ liking : num [1:336] 8 4 1 5 5 3 8 8 4 4 ... In fact, there are a couple of requirements for row binding that I’ll list here just to save time: You must have the same number of columns in all tables The columns must all have the same data type (character, numeric, etc) The columns must have the same names tidy_cider_liking %&gt;% transmute(Sample_Name = cider_name, Panelist_Name, cata_attribute = &quot;liking&quot;, checked = liking) %&gt;% rbind(tidy_cider_cata, .) ## # A tibble: 9,072 × 4 ## Sample_Name Panelist_Name cata_attribute checked ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bold rock Paulette Cairns FermentedApples 0 ## 2 Bold rock Paulette Cairns OverripeApples 0 ## 3 Bold rock Paulette Cairns FreshApples 1 ## 4 Bold rock Paulette Cairns Alcohol 0 ## 5 Bold rock Paulette Cairns WineLike 0 ## 6 Bold rock Paulette Cairns Floral 0 ## 7 Bold rock Paulette Cairns Tart 0 ## 8 Bold rock Paulette Cairns Candied 0 ## 9 Bold rock Paulette Cairns Fruity 1 ## 10 Bold rock Paulette Cairns Berries 0 ## # … with 9,062 more rows ## # ℹ Use `print(n = ...)` to see more rows How can we make sure this worked? Of these two, rbind() is the less “dangerous” binding approach, because we usually store either full cases or observations in rows. Screwing up a row bind will generally not have as dire consequences for your analysis. But how could this be dangerous? There are so many requirements. # What is wrong with this scene? How would you know that something went wrong? tidy_cider_liking %&gt;% transmute(Sample_Name = Panelist_Name, Panelist_Name, cata_attribute = liking, checked = liking) %&gt;% rbind(tidy_cider_cata, .) ## # A tibble: 9,072 × 4 ## Sample_Name Panelist_Name cata_attribute checked ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bold rock Paulette Cairns FermentedApples 0 ## 2 Bold rock Paulette Cairns OverripeApples 0 ## 3 Bold rock Paulette Cairns FreshApples 1 ## 4 Bold rock Paulette Cairns Alcohol 0 ## 5 Bold rock Paulette Cairns WineLike 0 ## 6 Bold rock Paulette Cairns Floral 0 ## 7 Bold rock Paulette Cairns Tart 0 ## 8 Bold rock Paulette Cairns Candied 0 ## 9 Bold rock Paulette Cairns Fruity 1 ## 10 Bold rock Paulette Cairns Berries 0 ## # … with 9,062 more rows ## # ℹ Use `print(n = ...)` to see more rows The tidyverse equivalent of rbind() is dplyr::bind_rows(), which adds some functionality and is efficient for large dataframes. 7.4.1.2 column binding Column binding, using the cbind() function, is generally used when we want to add new variables to our data set (rather than new observations). This makes it a riskier task, because we have to assume that the order of our new columns is the same as our old columns. In our example, it might actually make more sense to add liking as a new column–each cider from each subject has a single liking rating attached to it, after all, and it is a qualitatively different type of variable than a check-box–it is an integer-valued liking rating. So, let’s try using cbind() to add tidy_cider_liking$liking to our tidy_cider_cata dataset. bad_cbind &lt;- tidy_cider_cata %&gt;% cbind(liking = tidy_cider_liking$liking) head(bad_cbind) ## Sample_Name Panelist_Name cata_attribute checked liking ## 1 Bold rock Paulette Cairns FermentedApples 0 8 ## 2 Bold rock Paulette Cairns OverripeApples 0 4 ## 3 Bold rock Paulette Cairns FreshApples 1 1 ## 4 Bold rock Paulette Cairns Alcohol 0 5 ## 5 Bold rock Paulette Cairns WineLike 0 5 ## 6 Bold rock Paulette Cairns Floral 0 3 Looks good, right? What is wrong with this picture? Well, it looks like the same panelist (Paulette Cairns) rated the same cider (Bold rock) with 6 different liking ratings… uh oh. This demonstrates the key risk of column binding: R, in attempting to be “helpful”, will invisibly try to make it work. In this case, what is happening behind the scenes is that R is realizing that tidy_cider_liking$liking is 336 rows long, while tidy_cider_cata is 8736 rows, and so R recycles the vector to fill in the rest. The liking ratings don’t actually line up with the other data frame, but R will not warn you at all. To make this work properly, you need to take several steps: Make sure that you know the row order of the target data frame (here, tidy_cider_cata). Put the origin variable(s) in the same order as the target. Make sure that the two are of the same length. In our example, the order of the data frames can be uniquely specified by the Panelist_Name and the Sample_Name. Once we make sure each one is in the same order, we will need to make 26 copies of each liking rating, since that’s how many CATA attributes each panelist is providing for each cider, and so this is how many times the liking rating will need to be pasted over. good_cbind &lt;- tidy_cider_liking %&gt;% # arrange into a known order arrange(Panelist_Name, cider_name) %&gt;% # get just the liking ratings pull(liking) %&gt;% # repeat each liking rating 26 times rep(each = 26) %&gt;% cbind( # sort tidy_cider_cata into the same order arrange(tidy_cider_cata, Panelist_Name, Sample_Name), # Add the liking column liking = . ) head(good_cbind) ## Sample_Name Panelist_Name cata_attribute checked liking ## 1 Big Fish Aili Wang FermentedApples 0 5 ## 2 Big Fish Aili Wang OverripeApples 0 5 ## 3 Big Fish Aili Wang FreshApples 0 5 ## 4 Big Fish Aili Wang Alcohol 1 5 ## 5 Big Fish Aili Wang WineLike 1 5 ## 6 Big Fish Aili Wang Floral 0 5 Phew, that was a lot of steps. How can we check we did it right? Above we give just one of the ways in which this can go wrong. Say we wanted to use our new datasets in an analysis. Let’s see what has happened to our mean liking ratings: # Means from the bad cbind bad_cbind %&gt;% group_by(Sample_Name) %&gt;% summarize(mean_liking = mean(liking)) ## # A tibble: 6 × 2 ## Sample_Name mean_liking ## &lt;chr&gt; &lt;dbl&gt; ## 1 Big Fish 5.32 ## 2 Blue Bee 5.36 ## 3 Bold rock 5.43 ## 4 Buskeys 5.32 ## 5 Cobbler Mountain 5.25 ## 6 Potters 5.43 # Means from the good cbind good_cbind %&gt;% group_by(Sample_Name) %&gt;% summarize(mean_liking = mean(liking)) ## # A tibble: 6 × 2 ## Sample_Name mean_liking ## &lt;chr&gt; &lt;dbl&gt; ## 1 Big Fish 5.61 ## 2 Blue Bee 3.95 ## 3 Bold rock 6.70 ## 4 Buskeys 6.05 ## 5 Cobbler Mountain 4.82 ## 6 Potters 4.98 # The true mean likings tidy_cider_liking %&gt;% group_by(cider_name) %&gt;% summarize(mean_liking = mean(liking)) ## # A tibble: 6 × 2 ## cider_name mean_liking ## &lt;chr&gt; &lt;dbl&gt; ## 1 Big Fish 5.61 ## 2 Blue Bee 3.95 ## 3 Bold rock 6.70 ## 4 Buskeys 6.05 ## 5 Cobbler Mountain 4.82 ## 6 Potters 4.98 Yikes. You can see how a careless mistake (which, again, will not cause R to give you an error) can have ramifications for your future analysis. FWIW, the tidyverse equivalent and improvement to cbind() is dplyr::bind_cols(). 7.4.2 *_join() functions Wouldn’t it be nice if there were a way to avoid this kind of problem? What a loaded question! In fact, the concept of a “join”, which comes from relational data can do exactly this. I strongly recommend working through the examples from R4DS, and I find the Stat 545 “cheatsheet” on joins to be a really helpful quick reference. I will be going over these tools in relatively light detail (since we’re already late in the class here, and I will be asking you to use these in your own work). The key (ha, wait for it) concept for understanding joins is a “key” identifier for each table. A key is a column or a set of columns that uniquely identifies each row of the table. That is, the values of the column(s) only occur once in the table. In our example, if we look at tidy_cider_liking, we see that the key is the combination c(Panelist_Name, cider_name). We can use count() to confirm that this is true: tidy_cider_liking %&gt;% group_by(Panelist_Name, cider_name) %&gt;% # Using sort = TRUE guarantees that the largest n will be at the top--if this is &gt;1 we haven&#39;t identified the key count(sort = TRUE) ## # A tibble: 336 × 3 ## # Groups: Panelist_Name, cider_name [336] ## Panelist_Name cider_name n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aili Wang Big Fish 1 ## 2 Aili Wang Blue Bee 1 ## 3 Aili Wang Bold rock 1 ## 4 Aili Wang Buskeys 1 ## 5 Aili Wang Cobbler Mountain 1 ## 6 Aili Wang Potters 1 ## 7 Amy Moore Big Fish 1 ## 8 Amy Moore Blue Bee 1 ## 9 Amy Moore Bold rock 1 ## 10 Amy Moore Buskeys 1 ## # … with 326 more rows ## # ℹ Use `print(n = ...)` to see more rows So, what is the key for tidy_cider_cata? It is possible that your data do not have a key within the data–Hadley Wickham and Grolemund (2017) gives the example of the nycflights13::flights table. In that case, you might create an external identifier key yourself–the easiest way to do so might be just using the row number, which can be accessed using the row_number() function. Keys are important because they allow you to specify the relationships between observations in each table. Typically, relationships between tables are one-to-many; otherwise, it would be possible to simply combine the tables into one large table. What is the relationship between the two cider tables? For each unique cider and panelist combination in tidy_cider_liking, there are ________ unique observations in tidy_cider_cata. This relationship will dictate the type of combinations we can make. For example, if there are many observations in one table for each one in a second, how we can combine table one into table two will be different than if the reverse were true. The action of combining tables through key identifiers is called “joining”. For convenience’s sake here we’ll talk about two tables X and Y, Wickham and Grolemund (2017) defines two categories of joins: In “mutating joins”, the resulting table combines the variables from X and Y, creating a new table with variables from both X and Y, with rows that are the result of finding (different kinds of) matches between the keys of X and Y. This is the equivalent of dplyr::mutate(). In “filtering joins”, the resulting table looks in Y for entries from X, and filters the resulting table without creating or combining variables. This is the equivalent of using dplyr::filter(). I can’t recommend enough the diagrams that Hadley Wickham and Grolemund (2017) provides in R4DS–take a look at these to understand all of the variations of joins. I will not have time to over all of them here. We will look at the most common example of each in my workflow. 7.4.2.1 mutating joins with left_join() A left join looks up each unique key in X, finds the first match for that key in Y, and then adds all the columns from Y to X in the order that matches the columns in X. If there is no match for a particular key in X, the join will create an empty (NA) field. This is the most common case for joining, and it is what we were trying to accomplish with our column binding example above. Schematic diagrams of mutating joins from Wickham and Grolemund (2017) In tidyverse, the function for making a left join is easy to remember: left_join(). It takes as input two tables, with the first one provided being used as X (the “left”) table, and the second being “Y”. If the tables have the same named key columns, left_join() will try to join using this, but it’s usually best to explicitly provide the by = argument to left_join(), because if you have two similarly named columns that are not unique keys, this could cause errors. # by using the pipe, tidy_cider_cata becomes X, and tidy_cider_liking becomes Y joined_cider_cata &lt;- tidy_cider_cata %&gt;% # Notice here we specify the keys, and if they are named differently we write it as &lt;&quot;x_key&quot;&gt; = &lt;&quot;y_key&quot;&gt; left_join(tidy_cider_liking, by = c(&quot;Panelist_Name&quot;, &quot;Sample_Name&quot; = &quot;cider_name&quot;)) Notice that I used a named vector to specify the keys to join on. This might look weird, but the reason it is written as above is because both tables have subjects in the Panelist_Name column, but the names of the cider are in tidy_cider_cata$Sample_Name and tidy_cider_liking$cider_name. We are telling left_join() a vector of characters that name the columns that are keys, and in the case when they are not the same we are explicitly specifying this. If I hadn’t done this, we would see some weird results: # What is this using as the joining key? What will the result be? tidy_cider_cata %&gt;% left_join(tidy_cider_liking) ## # A tibble: 52,416 × 6 ## Sample_Name Panelist_Name cata_attribute checked cider_name liking ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bold rock Paulette Cairns FermentedApples 0 Bold rock 8 ## 2 Bold rock Paulette Cairns FermentedApples 0 Buskeys 4 ## 3 Bold rock Paulette Cairns FermentedApples 0 Blue Bee 1 ## 4 Bold rock Paulette Cairns FermentedApples 0 Potters 5 ## 5 Bold rock Paulette Cairns FermentedApples 0 Cobbler Mountain 5 ## 6 Bold rock Paulette Cairns FermentedApples 0 Big Fish 3 ## 7 Bold rock Paulette Cairns OverripeApples 0 Bold rock 8 ## 8 Bold rock Paulette Cairns OverripeApples 0 Buskeys 4 ## 9 Bold rock Paulette Cairns OverripeApples 0 Blue Bee 1 ## 10 Bold rock Paulette Cairns OverripeApples 0 Potters 5 ## # … with 52,406 more rows ## # ℹ Use `print(n = ...)` to see more rows We can check that our join worked properly by looking at that same means table we checked before. Note that this check is for this case, specifically–this does not guarantee that things have gone right in every possible situation. # This looks right - check back to see if it matches joined_cider_cata %&gt;% group_by(Sample_Name) %&gt;% summarize(mean = mean(liking)) ## # A tibble: 6 × 2 ## Sample_Name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 Big Fish 5.61 ## 2 Blue Bee 3.95 ## 3 Bold rock 6.70 ## 4 Buskeys 6.05 ## 5 Cobbler Mountain 4.82 ## 6 Potters 4.98 What if we wanted to reverse the join, and use tidy_cider_liking as the left-side (X)? Because it is the “one” in the one-to-many relationship, we’d either have to decide on a specific CATA attribute we want to join, or rethink our resulting structure. tidy_cider_liking %&gt;% left_join(cider_cata, by = c(&quot;Panelist_Name&quot;, &quot;cider_name&quot; = &quot;Sample_Name&quot;)) %&gt;% select(1:8) ## # A tibble: 336 × 8 ## Panelist_Name cider…¹ liking Ferme…² Overr…³ Fresh…⁴ Alcohol WineL…⁵ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Paulette Cairns Bold r… 8 0 0 1 0 0 ## 2 Paulette Cairns Buskeys 4 1 0 0 1 0 ## 3 Paulette Cairns Blue B… 1 0 0 0 0 0 ## 4 Paulette Cairns Potters 5 1 0 0 0 0 ## 5 Paulette Cairns Cobble… 5 0 1 1 0 0 ## 6 Paulette Cairns Big Fi… 3 0 0 0 1 0 ## 7 Renata Vieira Carneiro Bold r… 8 0 0 0 0 0 ## 8 Renata Vieira Carneiro Buskeys 8 1 0 0 0 0 ## 9 Renata Vieira Carneiro Blue B… 4 1 0 0 1 0 ## 10 Renata Vieira Carneiro Potters 4 0 0 0 0 0 ## # … with 326 more rows, and abbreviated variable names ¹​cider_name, ## # ²​FermentedApples, ³​OverripeApples, ⁴​FreshApples, ⁵​WineLike ## # ℹ Use `print(n = ...)` to see more rows Note that we now have equivalent data structures–we could have gotten either of these by joining and then using either pivot_longer() or pivot_wider(). This is because the unique key identifiers here that we are joining on are the combination of subject and cider. Another way to look at joins from Wickham and Grolemund (2017) 7.4.2.2 filtering joins with anti_join() Filtering joins let you use a Y table as a reference for selecting rows–rather than columns–from X. In the case of anti_join(), as the name implies, this removes all rows from X that match the keys in Y. Schematic of filtering with anti-joins from Wickham and Grolemund (2017) Let’s imagine that we had a list of panelists who had been removed from our lab for rowdy behavior. We know their results are bad because they were busy partying. Let’s create a fake list of these panelists, and then use anti_join() to take them out of our datasets. banned_panelists &lt;- cider_liking %&gt;% slice_sample(n = 7) tidy_cider_liking %&gt;% anti_join(banned_panelists, by = &quot;Panelist_Name&quot;) ## # A tibble: 294 × 3 ## Panelist_Name cider_name liking ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Renata Vieira Carneiro Bold rock 8 ## 2 Renata Vieira Carneiro Buskeys 8 ## 3 Renata Vieira Carneiro Blue Bee 4 ## 4 Renata Vieira Carneiro Potters 4 ## 5 Renata Vieira Carneiro Cobbler Mountain 6 ## 6 Renata Vieira Carneiro Big Fish 4 ## 7 Jennifer Acuff Bold rock 4 ## 8 Jennifer Acuff Buskeys 6 ## 9 Jennifer Acuff Blue Bee 5 ## 10 Jennifer Acuff Potters 6 ## # … with 284 more rows ## # ℹ Use `print(n = ...)` to see more rows tidy_cider_cata %&gt;% anti_join(banned_panelists, by = &quot;Panelist_Name&quot;) ## # A tibble: 7,644 × 4 ## Sample_Name Panelist_Name cata_attribute checked ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bold rock Renata Vieira Carneiro FermentedApples 0 ## 2 Bold rock Renata Vieira Carneiro OverripeApples 0 ## 3 Bold rock Renata Vieira Carneiro FreshApples 0 ## 4 Bold rock Renata Vieira Carneiro Alcohol 0 ## 5 Bold rock Renata Vieira Carneiro WineLike 0 ## 6 Bold rock Renata Vieira Carneiro Floral 0 ## 7 Bold rock Renata Vieira Carneiro Tart 0 ## 8 Bold rock Renata Vieira Carneiro Candied 0 ## 9 Bold rock Renata Vieira Carneiro Fruity 1 ## 10 Bold rock Renata Vieira Carneiro Berries 0 ## # … with 7,634 more rows ## # ℹ Use `print(n = ...)` to see more rows Note that in both cases we see the number of rows go down as these rows are removed through anti_join(). We also see that we can keep a table that is separate and refer to it–this is useful if we programmatically update it (say we are always keeping a “do not serve” list) and for clarity in coding. 7.4.2.3 other joins For a more complete documentation of other joins, I once again recommend the R4DS section or checking out ?left_join. 7.5 Writing readable and reproducible scripts Finally, it’s a good time to revisit some principles for writing scripts and R Markdowns that will work well in the future. We’ll keep this brief and use the headers as discussion points, rather than exhaustive explorations. 7.5.1 Comments In general, when we are coding we are very focused on the problem at hand. We’ll try a bunch of stuff, and then go with what works. But our solutions are often not clear from the code itself. Use # in order to provide comments in the code itself, explaining what each chunk or line does. Especially if you’ve written something compact and clever, like the map() functions above, it may require a lot of effort for someone else (or your future self) to interpret. Comments will help your code be useful for longer. 7.5.2 Whitespace R is very generous about whitespace; it doesn’t enforce conventions around it. But, as we learned in week 1, writing crazily spaced code is hard to read and makes your code difficult to audit. This is even a problem with very accomplished programmers; I recommend anyone take a look at the R code in Richard McElreath (2020)’s wonderful book, which is very difficult to parse because of his arbitrary spacing and his use of uninformative variable names. 7.5.3 Use informative variable names …speaking of which, why not help yourself and others out and use variable names that are meaningful? In older coding languages, there were limits on lengths and characters for names, so people would use conventions like “CamelCaps” and “abbrvtns”. This isn’t necessary with R, and it can make complicated commands hard to follow. If you’re coding for research, don’t do this! 7.5.4 Running from scratch Most importantly, before you turn in assignments to me or save a project that you are done with for the moment, make sure the script/Markdown runs from scratch. Sometimes, if you’re working between the console and a script, or between multiple scripts, it’s easy to create variables or run analyses without actually writing them into your script. Or maybe you are working out of order. Restarting R, clearing all memory, and making sure it can run from the first to the last line will make sure your work survives and is reproducible. References References "],["inference-from-data.html", "8 Inference from data 8.1 Motivation: inference from experiments 8.2 “A resampling approach” to statistics 8.3 What do we know about statistics? References", " 8 Inference from data Welcome back! I know everyone has just been counting the days to get back in the classroom. Me too! This week we’re going to be starting on the second half of the class, in which we move from the basics of how to code for research–learning how R works, learning basic concepts of control flow and data wrangling–to the why of our coding work: analyzing data. This will be the focus of the rest of the class! In the last few weeks you actually started to learn a lot about how to do exploratory and descriptive data analysis: how to make basic, useful exploratory plots, how to generate summaries of data that tell you something about what you’ve got. You also have spent some time putting together your first data-analysis report, which made use of those skills. So this week we will start with a summary of what inferential data analysis is, and why we might want to employ it in our work. We’ll start off with an application to motivate the discussion: looking at some data from Michael Wesolowski’s thesis on Salmonella inhibition with ethanol vapor. Then, we’ll step back and talk about how small experiments like this one can be used for inferring knowledge about more general situations. We’ll talk about heuristic (traditional) statistical approaches and resampling approaches to inference. Finally, we’ll talk about the counterexample of how we usually learn and think about statistical inference in science, and how it contrasts with what we’re doing here. 8.1 Motivation: inference from experiments Today we’re going to work with a subset of data from Michael Wesolowski’s 2017 thesis work on the effect of ethanol vapor on Salmonell spp. on produce. Michael ran a number of experiments, but we are going to consider just the experiment on the effect of exposure time on Salmonella CFUs on tomatoes. This is the first column of data in the Thesis Data Mastersheet.xls data file which you can download from a few places on Canvas, in particular the Files/Week 10 directory in the course setup package. head(tomato_salmonella) ## # A tibble: 6 × 9 ## `Time (sec)` `Trial #` Replicate Plate…¹ Count Addit…² Total…³ Total…⁴ Log C…⁵ ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 A 1 352 0.01 1.10e-4 3.20e6 6.51 ## 2 0 1 A 2 51 0.001 1.10e-5 4.64e6 6.67 ## 3 0 1 B 1 52 0.001 1.10e-5 4.73e6 6.68 ## 4 0 1 B 2 75 0.001 1.10e-5 6.82e6 6.83 ## 5 0 1 C 1 53 0.001 1.10e-5 4.82e6 6.68 ## 6 0 1 C 2 55 0.001 1.10e-5 5.00e6 6.70 ## # … with abbreviated variable names ¹​`Plate #`, ²​`Additional Dilution`, ## # ³​`Total Dilution`, ⁴​`Total CFU/mL`, ⁵​`Log CFU/mL` skimr::skim(tomato_salmonella) Table 8.1: Data summary Name tomato_salmonella Number of rows 72 Number of columns 9 _______________________ Column type frequency: character 3 numeric 6 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Time (sec) 0 1 1 2 0 4 0 Replicate 0 1 1 1 0 3 0 Count 0 1 1 5 0 30 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Trial # 0 1 2.00 0.82 1.00 1.00 2.00 3.00 3.00 ▇▁▇▁▇ Plate # 0 1 1.50 0.50 1.00 1.00 1.50 2.00 2.00 ▇▁▁▁▇ Additional Dilution 0 1 0.08 0.04 0.00 0.08 0.10 0.10 0.10 ▂▁▁▁▇ Total Dilution 0 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ▂▁▁▁▇ Total CFU/mL 0 1 1132885.96 2181060.41 909.92 909.92 909.92 450864.42 7279344.86 ▇▁▁▁▁ Log CFU/mL 0 1 4.17 1.55 2.96 2.96 2.96 5.19 6.86 ▇▁▂▁▃ The key independent variable in this experiment (one part of the thesis) is the time for which the tomatoes were exposed to ethanol vapor–Time (sec)–and the dependent (outcome variable) is the total number of colony forming units/mL, which we will express on a log scale–Log CFU/ml. The question Michael had was whether the treatment decreased the number of colony-forming units (CFUs) on the surface of the tomatoes. We can get an intuition for the answer to that question by looking at the mean and SD for each time exposure: tomato_salmonella %&gt;% group_by(`Time (sec)`) %&gt;% summarize(mean = mean(`Log CFU/mL`), sd = sd(`Log CFU/mL`)) ## # A tibble: 4 × 3 ## `Time (sec)` mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6.60 0.220 ## 2 10 3.25 0.671 ## 3 15 3.32 0.686 ## 4 5 3.53 0.831 It certainly looks like any time exposure to ethanol vapor reduces the number of CFUs on the tomatoes. It isn’t so clear that longer exposures lead to further decreases, though. We have two questions we might want to answer from this simple example: What are the confidence intervals around our point-estimates of group means? Is the difference we observed important? Are we seeing a real effect? Let’s use what we’ve learned about simulation and random sampling to answer each of these questions using a coding approach, and then circle back to a statistical heuristic approach. 8.2 “A resampling approach” to statistics Today we are going to learn some approaches to thinking about inference with data that don’t rely on assumptions about formula or rules. These are called “resampling” approaches to statistics, and they instead rely on us being a bit clever about using computational power to do simulations onour datasets. This sounds fancy and intimidating, and I am pleased to say that while it may be a little bit alien, it is not! As a caveat, however, I will note that we are going to just learn about the basics of these approaches today (and in this class in general). There are whole books on this topic, and more sophisticated simulationist approaches like Bayesian statistics are going to be the most precise and powerful models (for example see McElreath 2020). My hope is that giving you a taste of a simulationist approach shows you how powerful you are once you know the basics of coding for research! 8.2.1 Confidence intervals around group means (the “bootstrap”) The first questions we have is about our sample means, which are “point estimtes” for the actual, population mean. What am I saying here? Well, let’s take a minute to unpack the idea of a point estimate, and then apply it to our example. First, let’s imagine we have the normal distribution: p &lt;- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + geom_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + labs(y = NULL) + theme_classic() p This is our population distribution. We know that its true population mean is 0, and its true population standard deviation is 1, by definition. Now, let’s pull 3 samples of 100 observations from the normal distribution with mean = 0 and sd = 1, and take a look at where they go on the graph: normal_samples &lt;- tibble(x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100)) skim(normal_samples) Table 8.2: Data summary Name normal_samples Number of rows 100 Number of columns 3 _______________________ Column type frequency: numeric 3 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist x1 0 1 -0.14 0.94 -2.19 -0.83 -0.19 0.59 2.22 ▃▆▇▆▁ x2 0 1 -0.09 0.97 -2.32 -0.88 0.06 0.66 2.07 ▂▆▆▇▂ x3 0 1 0.03 0.97 -2.44 -0.53 0.07 0.74 2.43 ▁▃▇▅▁ We can see what is happening here by plotting these onto our normal distribution: p + geom_vline(xintercept = mean(normal_samples$x1), color = &quot;red&quot;, size = 0.75) + geom_vline(xintercept = mean(normal_samples$x2), color = &quot;red&quot;, size = 0.75, linetype = &quot;dashed&quot;) + geom_vline(xintercept = mean(normal_samples$x3), color = &quot;red&quot;, size = 0.75, linetype = &quot;dotted&quot;) Even though our samples are coming from the exact same population, they have different means (and standard deviations) because of sampling error. Although, as a side note, I will say that “error” makes it sound like something we can fix–in fact, this variation is fundamental and cannot be avoided. To conclude: even when we are sampling from a perfectly defined and known population, our samples will only approximate that population. But in the long term, if we draw enough samples they will end up in the limit being unbiased approximations. Armed with that knowledge, let’s return to our example. 8.2.1.1 A bootstrapping example In this example, for each exposure-time treatment, we have 18 observations (with some more internal replication structure that we will ignore here). Thus, when we calculate our point-estimate summary statistics, we are basing that estimated group mean (or SD or whatever) on those 18 observations. But how would it change if we had 18 other observations? What we know is our sampling-error biased mean, and what we want to get a better picture of is the population these means came from. In our explanation above, what this is equivalent to is trying to infer the shape of the normal distribution with only 1 of the 100-sample draws (that we showed were full of errors). How can we do this? The answer here is that we can use an approach called “resampling” to build a simulated population from our observed sample, and then use that population to understand how representative our observed point-estimate statistics are. This sounds complicated, but in practice it is very intuitive. The description from Bruce and Bruce (2017) for the steps to do this is quite clear (paraphrased here in almost pseudocode): From your sample, draw a sample of the same size with replacement. Record the mean, SD, or other relevant statistic from your “resampled” sample and record the information. Repeate steps 1-2 a large number of times (a typical number is \\(N=10,000\\), but this can vary depending on computational intensivity) Use the new dataset of statistics to calculate a confidence interval or to draw boxplots or histograms to compare to your point estimate. The reason this approach works well is the “with replacement” bit. This means that, rather than just shuffling our data, we will be sometimes drawing the same sample multiple times. Over the long run, this creates a population that would plausibly generate our sample, as shown in Bruce and Bruce (2017): Schematic example of bootstrapping (Bruce and Bruce 2017, 58) Again, this sounds like we’re doing something mystically complicated, but the steps above are each things we already know how to do. Let’s take a look in practice, with our dataset. We have 4 different means we want to resample, one for each of the different treatments. We’ll write a little for loop to do this for us (although you could check out the boot package for more functionality). # Here we set a number of iterations for the bootstrap. If your computer is slower, consider setting this to 100 just for the example. boot_reps &lt;- 1000 # for each step of the loop, we will generate a new sample for each of our # treatments and take its mean booted_means &lt;- tibble(`0` = numeric(boot_reps), `5` = numeric(boot_reps), `10` = numeric(boot_reps), `15` = numeric(boot_reps)) for(i in 1:boot_reps){ this_loop_means &lt;- list() # for each step, we will make our lives easier (in terms of typing) by stepping through a loop of the treatments for(time in c(0, 5, 10, 15)){ tomato_salmonella %&gt;% filter(`Time (sec)` == time) %&gt;% pull(`Log CFU/mL`) %&gt;% sample(x = ., size = 18, replace = TRUE) %&gt;% # this is the &quot;bootstrap&quot; step, note replace = TRUE mean %&gt;% c(this_loop_means, .) -&gt; # don&#39;t forget to store the results of the loop somewhere! this_loop_means } booted_means[i, ] &lt;- this_loop_means } Now we have a big (10,000 row) data frame, in which each row is a plausible simulation of means we could have gotten if our experimental sample had been different, but drawn from the same population! Let’s go ahead and do a little basic data exploration on this dataset. I think the easiest way to look at this is using box plots: booted_means %&gt;% mutate(boot_id = row_number()) %&gt;% pivot_longer(names_to = &quot;exposure_time&quot;, values_to = &quot;logCFU&quot;, -boot_id) %&gt;% mutate(exposure_time = as.numeric(exposure_time)) %&gt;% ggplot(aes(x = exposure_time, y = logCFU, group = exposure_time)) + geom_boxplot() + geom_point(data = tibble(exposure_time = c(0, 5, 10, 15), logCFU = tomato_salmonella %&gt;% group_by(`Time (sec)`) %&gt;% summarize(mean = mean(`Log CFU/mL`)) %&gt;% pull(mean)), color = &quot;red&quot;) + theme_classic() It looks like while with no ethanol vapor we have quite a narrow range of Salmonella CFUs, with exposure variation grows quite a bit! In addition, it seems like our good results for a 5-second exposure, while certainly not a fluke, are somewhat unexpectedly on the low end of CFUs we’d expect to observe if we ran the same study again. Similarly, our 15-second exposure appears to be somewhat pessimistic! Thus, sampling error might explain the odd trend we observe in our actual results of increased CFUs with increased ethanol vapor exposure. We can look at this numerically as well: # Actual observed means tomato_salmonella %&gt;% group_by(`Time (sec)`) %&gt;% summarize(mean = mean(`Log CFU/mL`)) %&gt;% pull(mean) ## [1] 6.604979 3.247841 3.315539 3.526595 # And our bootstrapped ranges summary(booted_means) ## 0 5 10 15 ## Min. :6.444 Min. :3.042 Min. :2.959 Min. :2.959 ## 1st Qu.:6.571 1st Qu.:3.408 1st Qu.:3.143 1st Qu.:3.225 ## Median :6.608 Median :3.527 Median :3.248 Median :3.315 ## Mean :6.605 Mean :3.529 Mean :3.247 Mean :3.315 ## 3rd Qu.:6.639 3rd Qu.:3.644 3rd Qu.:3.353 3rd Qu.:3.405 ## Max. :6.740 Max. :4.126 Max. :3.852 Max. :3.857 This tells us that, while our results are significant, we should have some caution in making strong inferences about a time-order effect. As responsible scientists, reporting these kinds of ranges will be very valuable for our audiences, letting them understand the context in which our reported results originate. 8.2.1.2 Advantages of the bootstrap You may ask yourself why do this kind of work to generate a confidence interval, when we already have a formula for that: \\(CI = \\bar{x} ± 1.96*se\\), where \\(se = \\sigma/\\sqrt{n}\\) is the standard error of the mean. We can alter this for small samples by replacing \\(1.96\\) with the 95th-percentile value for the appropriate \\(t\\)-distribution. But this approach assumes that you understand the underlying random (“stochastic”) process that generates your data–this is called a parametric approach because it assumes that you know the function and parameters (the normal distribution) that generates your observations. Hold on, what did I just say? The above means that we are assuming that a bell-shaped curve (normal or \\(t\\)) generated our obesrvations. If we drew enough, we’d end up seeing that familiar normal bell-curve. This is a very strong assumption that, prior to wide availability of computational power, was necessary. But it’s not a good assumption. Let’s take a quick look at our data and see if it looks normal: # set your boot reps to something appropriate boot_reps &lt;- 1000 # Again, if your computer is slow consider setting this to 100 instead just for the example boot_samples &lt;- tibble(boot_id = 1:boot_reps) # this is just a place to put the observations for(time in c(0, 5, 10, 15)){ tomato_salmonella %&gt;% filter(`Time (sec)` == time) %&gt;% pull(`Log CFU/mL`) %&gt;% sample(x = ., size = boot_reps, replace = TRUE) %&gt;% cbind(boot_samples, .) -&gt; # don&#39;t forget to save it boot_samples } boot_samples %&gt;% rename(`0` = 2, `5` = 3, `10` = 4, `15` = 5) %&gt;% pivot_longer(names_to = &quot;exposure_time&quot;, values_to = &quot;logCFU&quot;, -boot_id) %&gt;% mutate(exposure_time = as.numeric(exposure_time)) -&gt; boot_samples boot_samples %&gt;% ggplot(aes(x = logCFU)) + geom_histogram(bins = 20) + facet_wrap(~exposure_time, scales = &quot;free&quot;) + theme_classic() This is plainly not data drawn from a normal distribution. The advantage of the bootstrap is that it is nonparametric: it makes no assumptions about how the data are generated and works equally well for any kind of statistic or any kind of data. It just requires writing some pseudocode to properly generate the loop, and then you have an empirical distribution for your data. Neat! 8.2.2 Is that difference important (permutation tests)? So far we have used resampling to mostly answer questions about our point-estimates: how reliable and stable are our sample statistics? But when we think about inference, we usually are trying to connect a cause to an effect. In our example, we might want to ask questions like: Does any ethanol vapor exposure reduce the number of CFUs on the surface of tomatoes? Are the differences among our treatments meaningful? These are the kinds of questions that lead to testable hypotheses. In future classes, we are going to touch on models of statistical analysis like the linear model which give us formulas to deal with these questions. In particular, we might use ANOVA (ANalysis Of VAriance) to answer #1, and linear regression to answer the second question. But today we’re going to just use some raw computational power to do so. 8.2.2.1 Does ethanol vapor reduce the number of CFUs on the surface of tomatoes? We already know that we have a total of 72 observations in our dataset, with 18 for each treatment. That means in one treatment we have no ethanol exposure, and we have increasing duration of ethanol-vapor exposure in the other treatments. Let’s calculate our point-estimates of the mean log CFUs for each treatment again: tomato_salmonella %&gt;% group_by(`Time (sec)`) %&gt;% summarize(mean_CFUs = mean(`Log CFU/mL`)) ## # A tibble: 4 × 2 ## `Time (sec)` mean_CFUs ## &lt;chr&gt; &lt;dbl&gt; ## 1 0 6.60 ## 2 10 3.25 ## 3 15 3.32 ## 4 5 3.53 It does certainly look like having no ethanol exposure leads to almost 3 orders of magnitude more CFUs (because we’re in log units) than any timed exposure to ethanol vapor. We can use basic logic to frame a counterfactual–“if ethanol exposure has no effect, would we see the same difference in log CFUs?” And our ability to manipulate data with code allows us to do exactly this! Specifically, we are going to manipulate our data set so as to “redo” our experiment thousands of times. In each case, we are going to take our 72 observations–the measured log CFUs–and randomly reassign them to the 72 experimental conditions–our ethanol-vapor time exposures–that could have generated them. This will give us 18 new observations for each time treatment, for which we will take a new point-estimate mean. We will then repeat this many times (usually thousands) to get a distribution of permuted means. Finally, we will compare our actual observations of mean log CFUs to the permuted means to determine if our observations show a particularly extreme value. Let’s see how this works in practice, and then we will return to outline the general steps of a Permutation Test from Bruce and Bruce (2017). # first, just to show how this works, let&#39;s do it once: permutation_example &lt;- tomato_salmonella %&gt;% select(`Time (sec)`, `Log CFU/mL`) %&gt;% bind_cols(sample(tomato_salmonella$`Time (sec)`, replace = FALSE)) %&gt;% # unlike bootstrapping we use replace = FALSE to just permute our sample rename(actual_time = 1, logCFU = 2, permuted_time = 3) permutation_example ## # A tibble: 72 × 3 ## actual_time logCFU permuted_time ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0 6.51 0 ## 2 0 6.67 10 ## 3 0 6.68 10 ## 4 0 6.83 10 ## 5 0 6.68 0 ## 6 0 6.70 0 ## 7 0 6.75 0 ## 8 0 6.78 15 ## 9 0 6.36 15 ## 10 0 6.39 0 ## # … with 62 more rows ## # ℹ Use `print(n = ...)` to see more rows We can then compare our observed means to our permuted means: # Our actual observations permutation_example %&gt;% group_by(actual_time) %&gt;% summarize(mean = mean(logCFU)) ## # A tibble: 4 × 2 ## actual_time mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 0 6.60 ## 2 10 3.25 ## 3 15 3.32 ## 4 5 3.53 # Our permuted observations permutation_example %&gt;% group_by(permuted_time) %&gt;% summarize(mean = mean(logCFU)) ## # A tibble: 4 × 2 ## permuted_time mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 0 4.94 ## 2 10 4.47 ## 3 15 4.14 ## 4 5 3.15 It is immediately apparent that the difference between the groups is less extreme in or permuted example. This makes sense: some observations that actually came from the high-log CFU 0-second treatment have now been assigned to other times, while some from the low-log CFU treatments have been assigned to 0 seconds. But just a single comparison isn’t enough to prove anything. What we will do is repeat this experiment thousands of times: permutation_reps &lt;- 1000 # set this to 100 or so if your computer is slow for the demo permuted_samples &lt;- tibble(time = character(), mean_logCFUs = numeric(), permutation_id = numeric() ) for(i in 1:permutation_reps){ permuted_times &lt;- sample(tomato_salmonella$`Time (sec)`, replace = FALSE) # make a scrambled set of treatments permuted_means &lt;- tibble(time = permuted_times, logCFUs = tomato_salmonella$`Log CFU/mL`) %&gt;% group_by(time) %&gt;% summarize(mean_logCFUs = mean(logCFUs)) %&gt;% mutate(permutation_id = i) # keep track of the permutation rep that generated these means # Don&#39;t forget to actually store the reps of the for() loop! permuted_samples &lt;- bind_rows(permuted_samples, permuted_means) } Now we can ask the question we care about of these simulated data: how many times are our actual observed means different from the permuted means? actual_means &lt;- tomato_salmonella %&gt;% group_by(`Time (sec)`) %&gt;% summarize(mean_CFUs = mean(`Log CFU/mL`)) %&gt;% rename(time = 1, mean_logCFUs = mean_CFUs) %&gt;% mutate(time = as.numeric(time)) # For example, let&#39;s take a look at the 0-second treatment permuted_samples %&gt;% mutate(time = as.numeric(time)) %&gt;% ggplot(aes(x = mean_logCFUs)) + geom_histogram(bins = 20, color = &quot;black&quot;, fill = &quot;grey&quot;) + geom_point(data = actual_means, aes(y = 0), color = &quot;red&quot;) + facet_wrap(~time) + theme_classic() It certainly looks like our observed values are more extreme than the permuted values! We can get a numerical result for this by asking what proportion of our permuted results are larger or smaller than our observed values. For our purposes, we mostly care about just the untreated (0 second) tomatoes, and in this case we want to know if our observed mean logCFU values are larger than we’d expect to see if ethanol treatment weren’t having an effect. actual_value &lt;- actual_means %&gt;% filter(time == 0) %&gt;% pull(mean_logCFUs) permuted_samples %&gt;% filter(time == 0) %&gt;% mutate(bigger = mean_logCFUs &gt; actual_value) %&gt;% summarize(total_bigger = sum(bigger)) ## # A tibble: 1 × 1 ## total_bigger ## &lt;int&gt; ## 1 0 So we can see that in 1000 permuted samples we never saw average log CFUs larger than our observed value. This is strong evidence that ethanol treatment has some effect on Salmonella CFUs. This approach is a specific application of that given by Bruce and Bruce (2017, 89, with some paraphrasing): Create a place to store your permuted results! Get your tidy dataset into a form that is amenable to using a for() loop and filter() and group_by() commands Select the treatment variable you are investigating and then shuffle it (break the link between it and the observed variable) Calculate the statistic of interest for your analysis (usually a mean, but not necessarily) for each permuted group Remember to store your results and the relevant associated data (often storing a permutation_id is useful, as we see below) Repeat steps 2-5 some large number of times (often 1000, but it can be done more if you have the time and computational power) Compare the distribution of permuted results to your observed results 8.2.2.2 Are the differences among our treatments meaningful? We don’t just care about a single treatment–we are interested in whether these is meaningful variation among all of our treatments. We are going to end today with a preview of the approach we can take towards this question, with a resampling/permutation approach to ANOVA. ANOVA is a procedure you’ve learned about before, and it relies on the simple observation that, if some grouping variable (“treatment”) is important, there will be less variation within groups than there is between different groups. This intution is shown here (with the assumption of normality that is important for linear-model ANOVA statistics): ANOVA diagram, from Gabriel Liguori We are going to take a simple approach to answering the basic question of one-way ANOVA (where a single experimental variable is manipulated) using our resampled data. We already generated the data we need: skim(permuted_samples) Table 8.3: Data summary Name permuted_samples Number of rows 4000 Number of columns 3 _______________________ Column type frequency: character 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace time 0 1 1 2 0 4 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist mean_logCFUs 0 1 4.17 0.31 3.14 3.95 4.16 4.39 5.61 ▁▇▇▂▁ permutation_id 0 1 500.50 288.71 1.00 250.75 500.50 750.25 1000.00 ▇▇▇▇▇ We are going to compare the variation between groups in our actual sample to the variation between groups in our permutations. Our question is: is the observed variation between groups in actuality larger than it would be if the treatments were in fact meaningless? Our measure of variation is going to be simple variance: var(). Let’s see how this works: # Our observed between-group variation actual_means %&gt;% summarize(actual_variance = var(mean_logCFUs)) ## # A tibble: 1 × 1 ## actual_variance ## &lt;dbl&gt; ## 1 2.64 # Our permuted samples variation permuted_samples %&gt;% group_by(permutation_id) %&gt;% # we want to calculate variance within each permutation rep %&gt;% summarize(permuted_variance = var(mean_logCFUs)) %&gt;% ggplot(aes(x = permuted_variance)) + geom_histogram(bins = 20, color = &quot;black&quot;, fill = &quot;grey&quot;) + geom_point(aes(y = 0, x = actual_variance), data = actual_means %&gt;% summarize(actual_variance = var(mean_logCFUs)), color = &quot;red&quot;) + theme_classic() It is very apparent that the between-group variation in our real sample is much larger than any of the permuted between-group variations. I leave it to you to work out how to get the actual proportion of permuted samples that it is larger than. As a final note, our permutation approach here does not require any of the assumptions that ANOVA typically makes. It does not matter that (as we showed above) our data is very non-normal. Instead, we have just reasoned out a way to examine sources of variation and compare them without making any assumptions of how or why our data is shaped how it is. However, please do note that working out these permutations for complex designs is quite difficult, and often computationally expensive. When your designs become more complex, better-designed models for simulation like Bayesian approaches (McElreath 2020) will work much better! 8.3 What do we know about statistics? Finally, as we move into inference, I want to quickly review some ideas about statistical inference for scientific research that we typically take for granted. We usually learn about statistics in conjunction with the scientific method in a research-methods or stats class: The scientific method, image by J.R. Bee (OK, I just can’t stop myself here, but this is one of the problems with our framework, as we’ll see below. The “Conclusion” in this cartoon actually is never reached in our idea of the Null Hypothesis Test, because all we can do is reject the null. This is counterintuitive and also not a necessary part of the scientific method.) Typically, the basic stats classes we all take in our undergrad and grad careers follow a trajectory that is something like this: Talk about probability, usually using coin flips and dice rolls. Talk about “the scientific method”, as above. Introduce the idea of a hypothesis as a falsifiable statement. Concentrate on the counterintuitive but key idea of the null hypothesis. Talk about how to calculate key statistics about samples: central tendencies (means, medians, etc), variations (variance, deviation). Introduce probability distributions based on situations, typically: The binomial distribution for coin flips. The normal distribution for something like heights of students in the class. Don’t forget about the t-distribution, too… Maybe some more esoteric distributions like the Poisson distribution or the \\(\\xi^2\\)-distribution. Introduce a bunch of heuristic calculations for how to take a sample you think comes from some distribution to the theoretical null distribution. These are statistical tests. This is where you usually learn about a menu of tests that you select based on how you believe your sample is distributed, your specific null hypothesis, etc, sch as the following: Flowchart for selecting a statistical test, taken from the Nishimura Lab. I think we have all seen a chart like this one, and we’ve all experienced the problem of asking a question like: OK, I collected the data from my experiment. Now which is the right statistical test to conduct? … Uh… In order to use this kind of chart, we need to make a lot of assumptions. We have to have a well-defined, falsifiable hypothesis (which we should really have before we start collecting data, but…), we need to understand the various tests enough to understand how to use them, and we need to understand the various assumptions underlying each test (this is the point at which we tend to fail the most often). I don’t like these charts and this melange of statistical methods for all of these reasons! In particular, I dislike them because the assumptions that underlie the tests are important, but because they are rarely achieved in practice our tests are often failing in unpredictable ways. But even more so, I dislike them because, for many simple problems, we simply do not need to be going to all this trouble. These types of charts, and the statistical tests they lead to, are the result of the need for heuristic approaches to statistics: as we talked about in Week 3, statistics originated in the 17th centuries with studies of probability in the absence of simulation. It was simply not practical to simulate “null” distributions, so early theorists developed very clever, computationally tractable approaches that avoided the need for simulation. But now we have computers. And we have learned to write simple code to make simulations for ourselves, which can often (much more intelligibly) help us explore our data and test our hypotheses. So, on that note, and with what we’ve learned today, I look forward to trying out applications of this code with you on Thursday! References References "],["the-linear-model---regression-and-anova.html", "9 The linear model - regression and ANOVA 9.1 Dataset example for today 9.2 Review of measurement levels 9.3 Linear regression 9.4 A note about causality 9.5 A coding digression: factor 9.6 Analysis of variance (ANOVA) 9.7 Incorporating multiple predictors References", " 9 The linear model - regression and ANOVA # Setup chunk is made visible for clarity knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(skimr) library(naniar) library(ggforce) set.seed(11) # Figure out a dataset we want to use here berry_data &lt;- read_csv(&quot;Files/Week 11/berry_data.csv&quot;) Last week we got a taste of statistical inference, along with a brief preview of Analysis of Variance. This week we’re going to review (because you learned these in statistics, right?) the concept of the linear model–very broadly–which encompasses both simple and multiple regression as well as one-way and multi-way ANOVA. Today we’re going to start by talking about the general problem of predicting one continuous (interval or ratio) variable from another. This is the classical recipe for linear regression–we’ll talk about the lm() function in R, how to interpret results, and look into a bootstrapping approach for estimating stability of our estimates and a permutation approach that will help us see whether our results are significant. We will spend some time looking at how to plot results from linear regression using a combination of geom_point()/geom_jitter() and geom_smooth(), as well as faceting. We’ll spend relatively little time on the assumptions we should really examine, but I’ll try to mention them. Then we will spend a little while talking about a particularly vexing kind of variable type you may already have encountered in R: factor-type data. These data are key in both the analysis and the plotting of ANOVA. Then we’ll look at a very closely related problem: how to predict a continuous (interval or ratio) variable from a discrete (interval or ordinal) predictor. This is the classic recipe for ANOVA, and we will look at the aov() function (a thin wrapper for the lm()) as well as pointing towards some other, more powerful packages (including the afex package). We’ll also revisit our permutation approach for ANOVA. We will also spend some time on plotting results from ANOVA, with a special focus on interaction (mean-effect) plots. These plots give powerful visualizations of both the main effects (treatment effects) and interaction effects (differential effects of one treatment based on levels of another treatment). We will not be looking in any detail at multiple regression or multiway ANOVA this week; that material will be introduced next week, when we will think more broadly about different methods for exploring how complex, interacting independent variables can be investigated in our models. 9.1 Dataset example for today The data we’ll be working with today is from research I conducted with Driscoll’s, Inc. in 2019 (just months before the pandemic). It is a subset of data from a Central Location Test on berry liking, in which we asked subjects to rate their liking on different sets of berries (blackberries, blueberries, strawberries, raspberries) using different kinds of rating scales. Some of the results of the study were published in a paper by Yeung et al. (2021). If you’re really interested in the details you can find them there, as well as some examples of applied bootstrapping and permutation tests! Today, however, we’re just interested in using the data to explore some applications of the linear model. Let’s take a look at our data: berry_data %&gt;% skim() Table 8.1: Data summary Name Piped data Number of rows 3350 Number of columns 8 _______________________ Column type frequency: character 3 numeric 5 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace age 270 0.92 5 5 0 3 0 gender 270 0.92 4 6 0 3 0 berry 0 1.00 9 10 0 4 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist subject 0 1.00 1075965.50 136631.18 1031946 1032642 1033289 1034014 1522576 ▇▁▁▁▁ sample 0 1.00 3.40 1.66 1 2 3 5 6 ▇▃▃▃▃ 9pt_overall 905 0.73 5.68 2.10 1 4 6 7 9 ▂▅▂▇▅ us_overall 790 0.76 8.49 3.81 0 6 9 11 15 ▃▅▇▇▅ lms_overall 848 0.75 20.24 43.74 -100 -9 26 56 100 ▁▃▆▇▃ This data is semi-tidy: each row has 3 observations on it: the rating of the berry by the particular subject in terms of the 9-pt hedonic scale (9pt_overall), the Labeled Magnitude Scale (lms_overall) and the Unstructured Line Scale (us_overall). The first of these 3 scales produces ordinal data that is often treated as interval-level, and the latter two are reputed to produce interval-level data (again, see Yeung et al. (2021) for details on some of these assumptions and their validity in this dataset). You’ll notice we have some missing data in our dataset. We can take a closer look at this using the naniar package (which is great for dealing with the common situation of missing data): berry_data %&gt;% vis_miss(cluster = TRUE) It looks like we have some big chunks of missing data (because of the way the data were collected: subjects did not have to attend every session). This is not a situation made-up for your benefit, just a characteristic of these data! We are going to use these data to investigate the relationships among the scales, as well as predicting liking outcomes from categorical variables like age-category and gender. 9.2 Review of measurement levels Typically, we talk about four “levels of measurement” in regards to data that dictate what kinds of analysis we should conduct (although we can do whatever we want, with sometimes disastrous results): Classical levels of measurement are nominal, ordinal, interval, and ratio The borders between these categories is not as hard and fast as we’d wish. For example, the famous 9-pt hedonic scale (well, famous in my discipline!) is really an ordinal response scale: Some scales used in sensory evaluation including the 9-pt hedonic scale We can have different kinds of variables in all different roles in our datasets. For example, in this dataset we have both nominal and ordinal predictors (independent or experimental variables): Berry type (nominal) Gender (nominal) Age (ordinal) Berry sample ID (nominal, even though it is listed as an integer) Subject ID (nominal, even though given as numeric) We also have ordinal and interval-level outcomes (dependent or measured variables): 9-pt scale (ordinal, treated as interval) Labeled Affective Magnitude Scale (interval, although often claimed to be ratio) Visual Analog Scale (interval) Technically, the choice of what statistical model we can fit to our data depends on the types of data we have! Frequently, we will violate the rules, but knowing that they exist will let us understand what risks we are taking. With that, let’s talk about one of the most frequent situations we encounter in research: predicting the value of one interval/ratio-level variable from another. This is linear regression. 9.3 Linear regression Let’s begin by taking a quick look at the correlations among the various forms of measured liking for our berry_data: berry_data %&gt;% select(contains(&quot;overall&quot;)) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) ## 9pt_overall us_overall lms_overall ## 9pt_overall 1.0000000 0.2550950 0.2573497 ## us_overall 0.2550950 1.0000000 0.2687089 ## lms_overall 0.2573497 0.2687089 1.0000000 berry_data %&gt;% select(contains(&quot;overall&quot;)) %&gt;% ggplot() + geom_smooth(aes(x = .panel_x, y = .panel_y), method = &quot;lm&quot;, color = &quot;red&quot;) + geom_jitter(aes(x = .panel_x, y = .panel_y), alpha = 0.1) + facet_matrix(vars(1:3)) + theme_classic() We can see that while there are positive relationships among our variables, they are not as strongly correlated as we might assume for instruments that putatively measure the same underlying construct (overall liking). This is going to be our justification for exploring linear regression with what otherwise seems like it would be a kind of silly example. We already learned that correlation (usually represented with the symbols \\(r\\) or \\(\\rho\\)) represents the strength of linear association between two numeric variables. Correlations fall into the range \\([-1, 1]\\): \\(-1\\) indicates perfect anticorrelation: when \\(x\\) goes up, \\(y\\) goes down \\(0\\) indicates no relationship between the variables \\(1\\) indicates perfect correlation: when \\(x\\) goes up, \\(y\\) goes up However, while correlation can give us an idea of the strength of a relationship, it doesn’t quantify the relationship. Informally (as I do most things in this class, because I’m not a statistician!), this is exactly what linear regression does: in its simple form, linear regression quantifies the relationship between \\(x\\) and \\(y\\) whose strength is represented by the correlation \\(r\\). 9.3.1 Bivariate (2-variable) relationships imply a third variable Before we begin to explore these models, a quick aside: these kinds of bivariate relationships always imply a third, implicit variable: the pairing variable, which is often give as \\(i\\), as in the index for pairs of variables: \\((x_1, y_1), (x_2, y_2), ..., (x_i, y_i)\\). This seems like something trivial to point out, but it is actually important and can be kind of puzzle. For example, this is the problem with apparent (false) historical correlations: Tight association between shark attacks and ice-cream consumption, from Statology. The real, underlying relationship is between shark attacks and time, and between ice-cream consumption and time, but timepoint is being used as the pairing variable and so it creates a false correlation between the two other variables. We often assume that our underlying pairing variable, \\(i\\), is a meaningless index, but it can often have consequences for our data. It is always worth at least explicitly stating how our data is structured to help us understand these (and other) kinds of errors we might encounter. 9.3.2 Simple linear regression With that all in mind, let’s state that the overall goal of linear regression is to obtain an estimate of the effect of one (usually continuous or interval/ratio) variable, \\(x\\), on the value of another (continuous) variable, \\(y\\). In order to estimate this relationship, we need some set of paired \\(\\{(x_i, y_i)\\}\\) so that we can estimate the relationship. We typically write this relationship as: \\(\\hat{y}_i = \\beta*x_i + \\alpha\\) In plain English, we are saying that from each observed \\(x_i\\), we generate an estimate for \\(y_i\\), which we write as \\(\\hat{y}_i\\), meaning it is an estimate that contains error, by multiplying \\(x_i\\) by some quantity \\(\\beta\\) and adding a constant quantity \\(\\alpha\\). Typically, we calculate both \\(\\alpha\\) and \\(\\beta\\) through Ordinary Least Squares, which gives us an exact numerical solution in order to minimize the quantity \\(\\sum{(y_i-\\hat{y}_i)^2}\\), which is the difference between our estimated and observed values. I will not be deriving this method at all in this class. I had to memorize the derivation (based in linear algebra) once, and I have mostly forgotten it. It is not too tricky, and it is interesting for the right sort of person, but it isn’t super pertinent to our everyday job of analyzing data as scientists. Instead, we’re going to take a look at how to run linear regression in R, how to interpret and plot the output, and some basic considerations. The command for the linear model in R (which includes both regression and ANOVA) is lm(). With our data, we can write the following: berry_lm &lt;- lm(`9pt_overall` ~ lms_overall, data = berry_data) # note that this is not a pipe-friendly function, need to use &quot;data = .&quot; berry_lm ## ## Call: ## lm(formula = `9pt_overall` ~ lms_overall, data = berry_data) ## ## Coefficients: ## (Intercept) lms_overall ## 5.34898 0.01254 class(berry_lm) ## [1] &quot;lm&quot; typeof(berry_lm) ## [1] &quot;list&quot; Notice that when we print berry_lm we just get a simple couple lines to the console: a list of the “function call” which tells us the model we fit, and estimates for the coefficients. In this case, we have only an (Intercept) coefficient, which is our \\(\\alpha\\), and an lms_overall coefficient, which is our \\(\\beta\\). This doesn’t provide us with any information on whether this model is a good model. In order to get that information, we are going to do two things: examine a summary table of the output and make some visual plots to explore the model. # We can get a pretty informative summary of our lm objects by using summary() summary(berry_lm) ## ## Call: ## lm(formula = `9pt_overall` ~ lms_overall, data = berry_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.6032 -1.5622 0.3626 1.6510 4.8927 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.348976 0.051718 103.43 &lt;2e-16 *** ## lms_overall 0.012542 0.001082 11.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.041 on 1893 degrees of freedom ## (1455 observations deleted due to missingness) ## Multiple R-squared: 0.06623, Adjusted R-squared: 0.06574 ## F-statistic: 134.3 on 1 and 1893 DF, p-value: &lt; 2.2e-16 This gives us the same information and more! We can see, for example, that a t-test run on the lms_overall \\(\\beta\\) coefficient is apparently “significant” (more on this later), and we also can learn that despite this our model is pretty awful: the \\(R^2\\), which estimates for us how much variability in \\(y\\) is explained by our model and in simple regression is literally the squared correlation coefficient (\\(r^2\\)), is very low! Even though our model is a significant fit, we really aren’t explaining much at all about the variability of the results. Why is this? Let’s take a look at our actual data: berry_data %&gt;% ggplot(aes(x = lms_overall, y = `9pt_overall`)) + geom_jitter() + geom_smooth(method = &#39;lm&#39;) + # geom_smooth adds a fitted model estimate to the plot theme_classic() We can see that, while there is indeed an underlying positive trend, it is quite weak: we have a lot of low values of 9pt_overall for high values of lms_overall, and vice versa. We can use extractor functions to get some more useful information from our model to help us understand what’s going on. We use coefficients() to get the coefficients of the model (\\(\\alpha\\) and \\(\\beta\\) in this case) if we want to use them somewhere else (for example in bootstrapping, see below). coefficients(berry_lm) ## (Intercept) lms_overall ## 5.34897610 0.01254223 We can extract predicted values for the model using the predict() function. We can also use this function to predict values for new \\(x\\) values which we want to estimate our expected \\(y\\) values for by providing them in the newdata = argument (as a vector). We can also get fitted (predicted) \\(\\hat{y}\\) values using the fitted() function. predicted_y &lt;- predict(berry_lm) %&gt;% as_tibble(rownames = &quot;observation&quot;) We can use this to take a look at our model’s effectiveness visually. We’re going to Select the observed 9pt_overall values from our data Get the predicted observations from our model Join them into a single data table Plot them against each other If the resulting plot is close to a diagonal line, we’ll know that our model is performing well, because ideally we want the error (\\(y_i-\\hat{y}_i\\)) to be very small! predicted_y %&gt;% # we create a new observation variable to filter our left_join left_join(berry_data %&gt;% mutate(observation = as.character(row_number()))) %&gt;% ggplot(aes(x = value, y = `9pt_overall`)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + theme_classic() + labs(x = &quot;Predicted value&quot;, y = &quot;Observed value&quot;) Unfortunately, it is clear that our model really doesn’t do much to predict the variation in our outcome variable. Equivalently, there is only a weak relationship between our predicted \\(\\hat{y}_i\\) values and our actual \\(y_i\\) values. This is just one of the interesting things we found out in this study! 9.3.2.1 Bonus: what’s the underlying (implied) variable(s) in this model? After spending a little bit of time on thinking about implicit links caused by the indexing variable, we immediately skimmed over what \\(i\\) means in this study Looking back at our skim(), what variable(s) links the observations of 9pt_overall and lms_overall? # Give your answers here! 9.3.2.2 A resampling approach to estimating quality of regression Given how bad our model explanation is, and how little effect lms_overall seems to have (we could read our model as saying that our estimated 9pt_overall score increases by 0.013 points for every 1 point increase in lms_overall), it is curious that we are so certain that our observed effect is significant. What does this mean? First, we might want to ask about the stability of our estimate. We can use this via bootstrapping, just as we did for means and variances in the last class. bootstrapped_betas &lt;- numeric(1000) for(i in 1:1000){ # Resample our data with replacement bootstrap_data &lt;- berry_data[sample(1:nrow(berry_data), replace = TRUE), ] # run the same model on the new sample bootstrap_model &lt;- lm(`9pt_overall` ~ lms_overall, data = bootstrap_data) # Get the lms_overall (beta) coefficient bootstrapped_betas[i] &lt;- coefficients(bootstrap_model)[2] } # Now we can plot the distribution of these beta coefficients and compare them to our observed coefficient bootstrapped_betas %&gt;% as_tibble() %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 20, fill = &quot;grey&quot;, color = &quot;black&quot;) + geom_point(aes(x = coefficients(berry_lm)[2], y = 0), color = &quot;red&quot;) + theme_classic() quantile(bootstrapped_betas, c(0.05, 0.95)) ## 5% 95% ## 0.01060681 0.01449443 There is an almost 40% difference between our bootstrapped confidence-interval’s upper and lower bounds, which is not super tight. We could also use a permutation approach to consider the significance of our estimate. When we break the implied pairing between our two variables (disrupt the \\(i\\) variable), what do we learn? Unlike our last example, in which we considered complete permutations, here we are also going to do an estimated or “bootstrapped permutation” test. We will only run 1000 of the almost infinitely large possible reorderings of the \\(i\\) variable: permuted_betas &lt;- numeric(1000) for(i in 1:1000){ # Scramble our data (not just resample) permuted_data &lt;- tibble(`9pt_overall` = sample(berry_data$`9pt_overall`, replace = FALSE), lms_overall = berry_data$lms_overall) # run the same model on the new sample bootstrap_model &lt;- lm(`9pt_overall` ~ lms_overall, data = permuted_data) # Get the lms_overall (beta) coefficient permuted_betas[i] &lt;- coefficients(bootstrap_model)[2] } permuted_betas %&gt;% as_tibble() %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 20, fill = &quot;grey&quot;, color = &quot;black&quot;) + geom_point(aes(x = coefficients(berry_lm)[2], y = 0), color = &quot;red&quot;) + theme_classic() + labs(x = &quot;Permuted beta coefficient&quot;, y = &quot;count&quot;) Happily, it does appear that the relationship, while weak, is indeed stronger than we’d expect to observe if there were no relationship between our subjects’ reported liking on 2 scales! That’s a relief. 9.3.3 Checking your model We’ve spent a little bit of time talking about how to interpret a model and how to investigate whether it is telling us anything useful. But an important facet of using any statistical model (that I am barely going to touch on) is investigating the fit of the model. The lm() function tries to help you out with this: by using plot() on an lm object, you can see a set of plots that give you information about model fit. plot(berry_lm) Each of these plots gives you information about how well the model fits. In this set, I tend to look most at the first two plots: the fitted values against the residuals and the QQ-plot of theoretical quantiles vs standardized residuals. The first plot tells us whether there is a systematic pattern in (mis)-fitting our data. The question is whether the model fits worse (bigger absolute residuals) at some set of fitted values than at others. It does appear that there is a systematic relationship between fitted values and residuals (\\(y_i - \\hat{y_i}\\)), which tells us that this model is not unbiased: it will tend to predict more extreme values at either end of the range of our predicted values. The second plot tells us something about the normality of our data. It orders the residuals from lowest to highest, and then assigns them to quantiles of the normal distribution. Then, it plots those assigned quantiles against the actual normalized residuals. In a well-fitting model, the residuals should cling pretty close to a straight line–in our model, we see the same systematic deviation at both low and high levels (in this case represented as quantiles). This is an indicator of non-normality in our data, which in turn means our estimates are probably bad. The last 2 plots give more technical information about the model fit (and look wild for this model), and I will not go into them here. Beyond these simple plots, there are many important different ways to assess a model. But we will not go into them here, because we start to get into statistical assumptions, and I think you should learn about these methods from someone who is more expert in them. My goal is to equip you with the tools to manipulate data and execute these analyses, and interpretation is best left to the domain experts! 9.4 A note about causality We are all familiar with the phrase “correlation does not imply causation”. But why do we say this so frequently? XKCD can’t say for certain whether statistics classes work. Without getting too into the weeds (save that for Week 14!), we know a few things that should make us cautious! Correlation is symmetrical: cor(x, y) == cor(y, x). So we can’t say anything about what is causing what! Correlation is a binary relationship: we only look at cor(x, y), so if \\(z\\) is causing both \\(x\\) and \\(y\\), we will see a strong correlation but won’t know why. Human brains seem to be simultaneous pattern-recognition and hypothesis-generation machines. So when we see associations (like correlations) we immediately make up plausible theories for them. We need to be cautious about our intuitions in the presence of these kinds of data explorations! 9.5 A coding digression: factor So far we have talked about using one continuous variable to explain/predict variation in another continuous variable. Often, however, we will have categorical variables whose effect on some outcome variable \\(y\\) we want to explore. In this case, we are going to end up working with a king of R data type we haven’t really explore yet: factor. I always recommend reading in the weekly assignment, but I am going to call it out here as well to remind you to check these out: the R4DS chapter on factors (Wickham and Grolemund 2017) is probably the best resource for this class. The links at the “Learning More” section of that chapter are great resources for those of you interested in the programming history of R The short story behind the existence of factors is that they are memory- and computationally-efficient ways to deal with limited sets of (possible non-numeric) labels, which to us are strings like \"male\", \"female\", \"red“, etc. In early R history it was not really necessary to deal with strings much, and so it was easier to largely treat them as labels for underlying integers. This is something we all do frequently in our own data-management without thinking about: think about how in an Excel lab notebook we fill the cells in a column like”Income level” by writing a value like “2”, which represents something like “$20-$30,000/year”. This is the idea behind how R treats factor data. A factor data type is a set of (small) integers with an attribute tag of levels that correspond to those labels. We’re going to examine this in the context of the berry_data. berry_types &lt;- berry_data$berry unique(berry_types) # we don&#39;t want to print all 3000+ duplicates ## [1] &quot;raspberry&quot; &quot;blackberry&quot; &quot;blueberry&quot; &quot;strawberry&quot; class(berry_types) ## [1] &quot;character&quot; typeof(berry_types) ## [1] &quot;character&quot; It is clear that we could do some kind of transformation where we tell R that 1 = \"raspberry\", etc to make integer labels for our berry types, and then R would only have to deal with numbers and a map that tells it which number corresponds to each unique character string in berry_types. This is what a factor does. In the language of factor, the label values are called levels. berry_levels &lt;- unique(berry_types) berry_factor &lt;- factor(berry_types, levels = berry_levels) str(berry_factor) ## Factor w/ 4 levels &quot;raspberry&quot;,&quot;blackberry&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... typeof(berry_factor) ## [1] &quot;integer&quot; class(berry_factor) ## [1] &quot;factor&quot; attributes(berry_factor) ## $levels ## [1] &quot;raspberry&quot; &quot;blackberry&quot; &quot;blueberry&quot; &quot;strawberry&quot; ## ## $class ## [1] &quot;factor&quot; We can get the levels back out of a factor by using levels() levels(berry_factor) ## [1] &quot;raspberry&quot; &quot;blackberry&quot; &quot;blueberry&quot; &quot;strawberry&quot; When we try to add a non-included level to a factor, R automatically coerces it to an NA. factor(c(&quot;raspberry&quot;, &quot;blueberry&quot;, &quot;cherry&quot;), levels = berry_levels) ## [1] raspberry blueberry &lt;NA&gt; ## Levels: raspberry blackberry blueberry strawberry Thus, a factor is R’s way of creating a map between (computationally expensive) strings and inexpensive, easy to manipulate integers. Many modern (tidyverse-type) functions no longer use factors as extensively, but a lot of base R functions do use them (in particular the aov() and lm() functions we are using a lot today). They either will request factors or convert strings to factors in the background silently. Another place that factors can be quite useful is in ggplot2. If we have an integer label (like sample in berry_data, which represents a sample #), by default it will be treated as numeric: berry_data %&gt;% ggplot(aes(x = lms_overall, y = `9pt_overall`, color = sample)) + geom_jitter() + theme_classic() Notice the gradient assigned to the color mapping for sample. If we tell R that sample is a factor (by using the as.factor() coercion function), ggplot() will recognize this and give us a much more useful (discrete) color mapping: berry_data %&gt;% mutate(sample = as.factor(sample)) %&gt;% ggplot(aes(x = lms_overall, y = `9pt_overall`, color = sample)) + geom_jitter() + theme_classic() Now that you’ve been introduced to factor data, you may have questions about, for example, how to add or subtract levels to a factor, or how to reorder those levels. Please consult the R4DS chapter on factors (Wickham and Grolemund 2017) to learn more–it is easy but slightly fiddly. And now, if you encounter weird effects in your treatment of string data, you’ll know why! 9.6 Analysis of variance (ANOVA) Now that we’ve introduced a new type of data in R, let’s justify it’s use! Often, we want to explain the variation in a (continuous) \\(y\\) variable by the effect of a categorical \\(x\\) variable–in base R like lm() these are usually treated (or coerced to) factor data. Happily, unless we want to make changes to levels, we can let R handle this behind the scenes. When we have some \\(x\\) variable that can only take on a set of discrete values, this is a categorical predictor, and our linear regression becomes instead Analysis of Variance: ANOVA. In ANOVA, we are interested in the effect of category membership \\(\\{x_1, x_2, ..., x_n\\}\\) on our predicted outcome variable \\(y\\). Instead of our classic regression linear equation, we tend to write ANOVA something like the following: \\(\\hat{y}_i = \\mu + \\tau_k\\) We are familiar with the LHS of this equation–our estimate for a particular, observed \\(y_i\\) is indicated as \\(\\hat{y}_i\\). The RHS looks a bit different: it is read as “the grand mean” (\\(\\mu\\)) “plus the group mean of the level of \\(x_i\\)” (which is written here as \\(\\tau_k\\), because we often talk about these as treatment means). This bit of mathiness can be rephrased intuitively as “our estimate of a particular \\(y\\)-value is the sum of the average of \\(y\\) plus the average effect of the treatment variable \\(x\\)”. Even though this sounds like a different problem, it’s actually our same equation from linear regression reframed for a situation in which \\(x\\) can only take on a delimited set of values. We have an example(s) of this in our berry_data set that will give us some context in this question. 9.6.1 One-way ANOVA In its simplest form, ANOVA is used to relate a single outcome \\(y\\) and a single predictor \\(x\\). We still have the implicit \\(i\\) variable that links our pairs of observations \\((x_i, y_i)\\), but now \\(x_i \\in \\{x_1, x_2, ..., x_n\\}\\)–it does not have an unlimted number of values it can take on, as it does in standard linear regression. Thus, our \\(\\beta\\) from linear regression is replaced by a set of \\(\\tau_k\\) values that represent the average effect for each possible group \\(x_i\\) can be in. But we could as easily write \\(\\beta_k\\)–the meaning is the same. It is just convention. In berry_data, berry is a categorical variable that represents berry type: it can be one of four values. unique(berry_data$berry) ## [1] &quot;raspberry&quot; &quot;blackberry&quot; &quot;blueberry&quot; &quot;strawberry&quot; Let’s model us_overall (scaled overall liking) on berry. This model asks: does the type of berry predict or explain a significant amount of liking? Can we say something about probable liking level if we know what type of berry the subject is tasting? berry_aov &lt;- berry_data %&gt;% lm(us_overall ~ berry, data = .) berry_aov ## ## Call: ## lm(formula = us_overall ~ berry, data = .) ## ## Coefficients: ## (Intercept) berryblueberry berryraspberry berrystrawberry ## 7.93878 1.30880 0.94541 -0.08662 summary(berry_aov) ## ## Call: ## lm(formula = us_overall ~ berry, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.2476 -2.8522 0.1478 3.0612 7.1478 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.93878 0.17031 46.614 &lt; 2e-16 *** ## berryblueberry 1.30880 0.22804 5.739 1.06e-08 *** ## berryraspberry 0.94541 0.22154 4.267 2.05e-05 *** ## berrystrawberry -0.08662 0.21934 -0.395 0.693 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.77 on 2556 degrees of freedom ## (790 observations deleted due to missingness) ## Multiple R-squared: 0.0246, Adjusted R-squared: 0.02346 ## F-statistic: 21.49 on 3 and 2556 DF, p-value: 9.602e-14 Notice we can just use our standard lm() function, even though our \\(x\\) variable, berry, is categorical. The model object and summary() for berry_aov has coefficients for 3/4 berry types–this is because R is converting berry to a factor in the background, which makes blackberry the first level (it is alphabetically first), and so the coefficients in the model are the values for a change from blackberry (the average liking for which is in the (Intercept) term). So blueberry is liked on average almost 1.31 pts on the line scale more than blackberry, whereas strawberry is liked slightly less on average by about 0.09 pts on the line scale. We also see our typical t-test estimates for significance for each level, but these are a bit harder to interpret for this model because these are actually each the same \\(x\\) variable. Therefore, we might want to use the anova() function, which is a version of summary() specific for ANOVA. anova(berry_aov) ## Analysis of Variance Table ## ## Response: us_overall ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## berry 3 916 305.423 21.489 9.602e-14 *** ## Residuals 2556 36328 14.213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This tells us that the \\(x\\) variable berry is overall significant, but doesn’t tell us about specific factor levels. We can get to this from the very start by using the aov() function, which is a version of lm() for ANOVA: berry_aov_2 &lt;- berry_data %&gt;% aov(us_overall ~ berry, data = .) # here berry_aov_2 ## Call: ## aov(formula = us_overall ~ berry, data = .) ## ## Terms: ## berry Residuals ## Sum of Squares 916.27 36327.52 ## Deg. of Freedom 3 2556 ## ## Residual standard error: 3.769966 ## Estimated effects may be unbalanced ## 790 observations deleted due to missingness summary(berry_aov_2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## berry 3 916 305.42 21.49 9.6e-14 *** ## Residuals 2556 36328 14.21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 790 observations deleted due to missingness The proper way to report this kind of model is “there was a significant effect of berry type on overall liking as measured by an unstructured line scale”. 9.6.2 Posthoc testing In ANOVA, we typically are interested in whether there are significant differences among our treatment levels: sure, we have estimates for \\(\\tau_{strawberry}\\), \\(\\tau_{raspberry}\\), etc, but are they actually different from each other? To answer these questions, we often use what are called posthoc tests, which have names like “Fisher’s Least Significant Difference” (Fisher’s LSD) or “Tukey’s Honestly Significant Difference” (Tukey’s HSD). I think the easiest way to calculate these are from the agricolae package. library(agricolae) LSD.test(y = berry_aov_2, trt = &quot;berry&quot;, console = TRUE) ## ## Study: berry_aov_2 ~ &quot;berry&quot; ## ## LSD t Test for us_overall ## ## Mean Square Error: 14.21265 ## ## berry, means and individual ( 95 %) CI ## ## us_overall std r LCL UCL Min Max ## blackberry 7.938776 4.058913 490 7.604816 8.272735 0 15 ## blueberry 9.247573 3.679441 618 8.950203 9.544943 0 15 ## raspberry 8.884181 3.540931 708 8.606354 9.162008 0 15 ## strawberry 7.852151 3.857093 744 7.581128 8.123173 0 15 ## ## Alpha: 0.05 ; DF Error: 2556 ## Critical Value of t: 1.960893 ## ## Groups according to probability of means differences and alpha level( 0.05 ) ## ## Treatments with the same letter are not significantly different. ## ## us_overall groups ## blueberry 9.247573 a ## raspberry 8.884181 a ## blackberry 7.938776 b ## strawberry 7.852151 b HSD.test(y = berry_aov_2, trt = &quot;berry&quot;, console = TRUE) ## ## Study: berry_aov_2 ~ &quot;berry&quot; ## ## HSD Test for us_overall ## ## Mean Square Error: 14.21265 ## ## berry, means ## ## us_overall std r Min Max ## blackberry 7.938776 4.058913 490 0 15 ## blueberry 9.247573 3.679441 618 0 15 ## raspberry 8.884181 3.540931 708 0 15 ## strawberry 7.852151 3.857093 744 0 15 ## ## Alpha: 0.05 ; DF Error: 2556 ## Critical Value of Studentized Range: 3.63555 ## ## Groups according to probability of means differences and alpha level( 0.05 ) ## ## Treatments with the same letter are not significantly different. ## ## us_overall groups ## blueberry 9.247573 a ## raspberry 8.884181 a ## blackberry 7.938776 b ## strawberry 7.852151 b The key output of these kinds of tests are the “letters”–the groups labeled with the same letter are close enough to each other based on the test’s criteria to be considered not significantly different. All of these methods have various responses to the problem of familywise error: when we run multiple tests on the same set of data, we are increasing our chance of finding a relationship that is just coincidental. This is often referred to as “p-hacking” or “significance mining”. We will return to this issue in Week 14, but this XKCD actually very aptly sums up the problem: Watch out for those green jellybeans! I will say this: do not use Fisher’s LSD. It is frequently used in published papers, and it is very very prone to this kind of error. While there is no perfect approach, Tukey’s HSD (and other methods) are less likely to give this kind of false positive. 9.6.3 Plotting ANOVA results Because ANOVA is based on categorical data, it doesn’t look very good to plot that data in scatterplots. Generally, boxplots or barplots will be the most effective visual representation of data from ANOVA. The following visualizations were adapted to some degree from Philipp Masur’s blog: berry_data %&gt;% ggplot(aes(x = berry, y = us_overall)) + geom_boxplot(aes(fill = berry), notch = TRUE) + geom_jitter(alpha = 0.1) + theme_classic() berry_data %&gt;% ggplot(aes(x = berry, y = us_overall)) + stat_summary(aes(fill = berry), fun.y = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_boot, geom = &quot;errorbar&quot;, width = 0.25) + theme_classic() Another classic plot for ANOVA that will become even more powerful when we begin to examine multiway analyses is the interaction plot, which is really a dot + line plot representing the same data as the bar plot: berry_data %&gt;% # We will make our own summary table rather than do transformations in ggplot group_by(berry) %&gt;% summarize(mean_liking = mean(us_overall, na.rm = TRUE), se_liking = sd(us_overall, na.rm = TRUE)/sqrt(n())) %&gt;% ggplot(aes(x = berry, y = mean_liking, color = berry, group = berry)) + geom_line(aes(group = 1), color = &quot;black&quot;) + geom_point(aes(y = mean_liking)) + geom_errorbar(aes(ymin = mean_liking - 2 * se_liking, ymax = mean_liking + 2 * se_liking), width = 0.25) + theme_classic() 9.6.4 Resampling approaches to ANOVA We actually learned about resampling approaches to ANOVA in last class, but let’s take a quick look at how we can modify our regression approach to get a better alternative to our posthoc tests. bootstrapped_means &lt;- tibble(berry = character(), mean_liking = numeric(), boot_id = numeric()) for(i in 1:1000){ berry_data %&gt;% group_by(berry) %&gt;% # The following line resamples to get the same size of data frame with replacement slice_sample(prop = 1, replace = TRUE) %&gt;% summarize(mean_liking = mean(us_overall, na.rm = TRUE)) %&gt;% mutate(boot_id = i) %&gt;% bind_rows(bootstrapped_means, .) -&gt; bootstrapped_means } bootstrapped_means %&gt;% ggplot(aes(x = berry, y = mean_liking)) + geom_boxplot() + geom_point(data = berry_data %&gt;% group_by(berry) %&gt;% summarize(mean_liking = mean(us_overall, na.rm = TRUE)), color = &quot;red&quot;) + theme_classic() bootstrapped_means %&gt;% group_by(berry) %&gt;% summarize(`5%` = quantile(mean_liking, 0.05), mean = mean(mean_liking), `95%` = quantile(mean_liking, 0.95)) ## # A tibble: 4 × 4 ## berry `5%` mean `95%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blackberry 7.66 7.94 8.24 ## 2 blueberry 9.00 9.25 9.49 ## 3 raspberry 8.66 8.88 9.10 ## 4 strawberry 7.62 7.85 8.07 9.7 Incorporating multiple predictors As a tease of our material for next week, we can always include multiple predictors in linear regression and ANOVA. This lets us simultaneously estimate the effect of multiple predictors on some outcome. So, just for fun, let’s estimate the effect of berry type and subject age on over all berry liking. berry_data %&gt;% aov(us_overall ~ berry * age, data = .) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## berry 3 849 283.10 19.970 8.78e-13 *** ## age 2 188 93.77 6.615 0.00137 ** ## berry:age 6 51 8.52 0.601 0.72949 ## Residuals 2388 33852 14.18 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 950 observations deleted due to missingness We can see that there is an overall effect of berry that remains even when the significant effect of age is also in the model. There doesn’t appear to be an interactive effect of berry and age, as shown by the third line for berry:age. We’ll talk about this in more detail tomorrow, but this plot should give an idea of what that means: berry_data %&gt;% ggplot(aes(x = age, y = us_overall, color = berry)) + geom_boxplot(notch = TRUE) + theme_classic() Often, when model multiple regression and ANOVA, we are not only interested in predicting \\(y\\), but in determining which different \\(x\\)s matter. This is the topic we will explore next week! References References "],["dealing-with-multiple-factors.html", "10 Dealing with multiple factors 10.1 Introduction 10.2 Multi-way ANOVA 10.3 Multiple regression 10.4 Some notes of caution on model selection 10.5 Categorization and Regression Trees (CART): A data-based approach 10.6 Wrap-up References", " 10 Dealing with multiple factors 10.1 Introduction Last week, we learned the basics of the linear model: how we might try to predict a continuous outcome based on a single continuous (regression) or categorical (ANOVA) predictor. This week, we’re going to tackle the much more common situation of having multiple (even many) possible predictors for a single continuous or categorical outcome. We’re going to switch up the order slightly and start by talking about multi-way ANOVA, which extends the basic model of ANOVA you have become somewhat familiar with to the case of multiple, categorical predictor variables. As you might guess, we are then going to talk about multiple regression, which extends regression to predicting a single, continuous outcome from multiple continuous or categorical predictors. Finally–and I am very excited and nervous to present this material–we’re going to dip our toes into one of the more modern, increasingly popular methods for predicting a single continuous or categorical outcome from multiple (many) predictors: categorization and regression tree (CART) models. These are powerful approaches that are the beginning of what we often refer to as machine learning, but we will be exploring them in their most accessible form. 10.1.1 Data example This week, we are going to build on the berry data from last week. If you glanced at the paper, you might have seen that we didn’t just collect liking data: we collected a whole bunch of other results about each berry. In order to keep our data manageable, we are going to work only with strawberry data and Labeled Affective Magnitude (LAM) outcomes, which gives us a much smaller dataset without all of the NAs we were dealing with last week. skim(berry_data) Table 8.1: Data summary Name berry_data Number of rows 510 Number of columns 24 _______________________ Column type frequency: character 1 factor 5 numeric 18 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace berry 0 1 10 10 0 1 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts subject 0 1 FALSE 85 103: 6, 103: 6, 103: 6, 103: 6 age 0 1 TRUE 3 36-: 222, 21-: 144, 51-: 144 gender 0 1 FALSE 2 fem: 426, mal: 84 sample 0 1 FALSE 6 1: 85, 2: 85, 3: 85, 4: 85 percent_shopping 0 1 TRUE 2 75-: 366, 50-: 144 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist lms_appearance 0 1 13.23 39.04 -92 -10.00 13 40.00 99 ▂▃▇▇▂ lms_overall 0 1 15.29 41.98 -100 -10.00 13 51.00 100 ▁▃▇▇▃ lms_taste 0 1 10.65 43.99 -100 -21.75 12 42.75 100 ▂▅▇▇▃ lms_texture 0 1 21.13 39.81 -99 -7.75 27 54.00 100 ▁▂▆▇▃ lms_aroma 0 1 19.47 33.39 -78 -1.00 12 45.75 99 ▁▁▇▅▂ cata_taste_floral 0 1 0.15 0.36 0 0.00 0 0.00 1 ▇▁▁▁▂ cata_taste_berry 0 1 0.46 0.50 0 0.00 0 1.00 1 ▇▁▁▁▇ cata_taste_grassy 0 1 0.31 0.46 0 0.00 0 1.00 1 ▇▁▁▁▃ cata_taste_fermented 0 1 0.14 0.35 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_fruity 0 1 0.42 0.49 0 0.00 0 1.00 1 ▇▁▁▁▆ cata_taste_citrus 0 1 0.18 0.38 0 0.00 0 0.00 1 ▇▁▁▁▂ cata_taste_earthy 0 1 0.26 0.44 0 0.00 0 1.00 1 ▇▁▁▁▃ cata_taste_none 0 1 0.06 0.24 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_peachy 0 1 0.08 0.27 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_caramel 0 1 0.03 0.17 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_grapey 0 1 0.11 0.32 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_melon 0 1 0.10 0.29 0 0.00 0 0.00 1 ▇▁▁▁▁ cata_taste_cherry 0 1 0.11 0.32 0 0.00 0 0.00 1 ▇▁▁▁▁ In this dataset, we now have not only lms_overall, but several other lms_* variables, which represent LAM liking for different modalities: taste, texture, etc. We also have a large number of cata_* variables, which represent “Check All That Apply” (CATA) questions asking subjects to indicate the presence or absence of a particular flavor. As before, our goal will be to understand overall liking, represented by the familiar lms_overall, but we will be using a new set of tools to do so. 10.2 Multi-way ANOVA When we first examined this dataset via ANOVA, we investigated the effect of the categorical predictor berry type on us_overall (recall this was our unstructured line-scale outcome). This week we will be mostly focusing on lms_overall as the outcome variable, and we’ve limited ourselves to berry == \"strawberry\" because we wanted a consistent set of “CATA” attributes (and less missing data). But we have added a lot of other predictor variables–both categorical (factor) and continuous (numeric) predictors. To motivate the idea of multi-way ANOVA, let’s think about how we might see differences in overall scale use by subject. This is a common issue in any kind of human-subjects research: berry_data %&gt;% ggplot(aes(x = subject, y = lms_overall, color = sample)) + geom_point(alpha = 0.5) + geom_hline(yintercept = 0, color = &quot;black&quot;, linetype = &quot;dashed&quot;) + theme_bw() + scale_color_viridis_d() + theme(axis.text.x = element_text(angle = 90, size = 6)) We can see that some subjects tend to use the upper part of the scale, and that while we see subjects are less likely to use the negative side of the scale (“endpoint avoidance”), some subjects are more likely to use the full scale than others. This indicates that the fact we have repeated measurements from each subject may introduce some nuisance patterns to our data. Now, let’s imagine we want to predict lms_overall based on how much shopping each subject does for their household. This is slightly arbitrary, but we could imagine our thought process is: “Well, those who buy a lot of strawberries might have stronger opinions about strawberry flavor than those who do less.” We learned how to set up a 1-way ANOVA last week: # How do we set up a 1-way ANOVA for lms_overall dependent on percent_shopping We can see that overall the effect is not significant (unsurprisingly), but perhaps the effect is obscured by the variation we noted in individual scale use by subject. We can use a multiway ANOVA to investigate this possibility: berry_data %&gt;% aov(lms_overall ~ percent_shopping * subject, data = .) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## percent_shopping 1 142 142.2 0.090 0.763706 ## subject 83 229247 2762.0 1.758 0.000181 *** ## Residuals 425 667758 1571.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hmm, well unsurprisingly that’s nothing either, although you can see that the effect-size estimate (F-statistic) for percent_shopping increased very slightly. We use the same function, aov(), to build multi-way ANOVA models, and the only difference is that we will add a new set of tools to our formula interface: + adds a main effect for a term to our formula &lt;variable 1&gt; * &lt;variable 2&gt; interacts the two (or more) variables as well as adding all lower-level interactions and main effects &lt;variable 1&gt; : &lt;variable 2&gt; adds only the interaction term between the two (or more) variables The type of model we just fit is often called a “blocked” design: our subjects are formally declared to be experimental “blocks” (units), and we accounted for the effect of differences in these blocks by including them in our model. This is why our estimate for the effect of percent_shopping increased slightly. This is the simplest example of a mixed-effects model, which are the subject of extensive research and writing. For more information on different kinds of ANOVA models, I highly recommend Statistical methods for psychology by David Howell (2010). While he does not write the textbook using R, his personal webpage (charmingly antiquated looking) has a huge wealth of information and R scripts for a number of statistical topics, including resampling approaches to ANOVA. You might note that we asked for the interaction (percent_shopping * subject) but we got no interaction effect in our model output. What gives? Well, subject is entirely nested within percent_shopping–if we know the subject variable we already know what percent_shopping they are responsible for. Let’s take a look at a model in which we predict lms_overall from some variables that do not have this relationship: berry_data %&gt;% aov(lms_overall ~ percent_shopping * age * sample, data = .) %&gt;% summary() ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## percent_shopping 1 142 142 0.090 0.7646 ## age 2 8369 4184 2.643 0.0722 . ## sample 5 101078 20216 12.766 1.17e-11 *** ## percent_shopping:age 2 2959 1479 0.934 0.3936 ## percent_shopping:sample 5 7195 1439 0.909 0.4750 ## age:sample 10 12143 1214 0.767 0.6609 ## percent_shopping:age:sample 10 14675 1467 0.927 0.5081 ## Residuals 474 750586 1584 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we see a number of interactions, none of them “signficant”. Again, this makes sense–honestly, it’s unclear why age, gender, or percent_shopping would really predict overall liking for strawberry varieties. But we do see the estimated interaction effects, like percent_shopping:age. Let’s try running one more multi-way ANOVA for variables we might actually expect to predict lms_overall: some of the CATA variables. These are binary variables that indicate the presence or absence of a particular sensory attribute. There are a lot of them. We are going to just pick a couple and see if they predict overall liking: berry_data %&gt;% aov(lms_overall ~ cata_taste_berry * cata_taste_fruity * cata_taste_fermented * sample, data = .) %&gt;% anova() %&gt;% # the anova() function returns a data.frame, unlike summary() as_tibble(rownames = &quot;effect&quot;) # purely for printing kind of nicely ## # A tibble: 16 × 6 ## effect Df Sum S…¹ Mean …² F val…³ `Pr(&gt;F)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cata_taste_berry 1 2.98e5 2.98e5 300. 2.86e-52 ## 2 cata_taste_fruity 1 4.74e4 4.74e4 47.7 1.61e-11 ## 3 cata_taste_fermented 1 3.05e4 3.05e4 30.7 5.15e- 8 ## 4 sample 5 3.07e4 6.14e3 6.18 1.47e- 5 ## 5 cata_taste_berry:cata_taste_fruity 1 9.93e1 9.93e1 0.100 7.52e- 1 ## 6 cata_taste_berry:cata_taste_fermented 1 2.57e3 2.57e3 2.58 1.09e- 1 ## 7 cata_taste_fruity:cata_taste_ferment… 1 2.13e3 2.13e3 2.15 1.43e- 1 ## 8 cata_taste_berry:sample 5 4.26e3 8.52e2 0.857 5.10e- 1 ## 9 cata_taste_fruity:sample 5 4.43e3 8.86e2 0.892 4.86e- 1 ## 10 cata_taste_fermented:sample 5 3.60e3 7.20e2 0.725 6.05e- 1 ## 11 cata_taste_berry:cata_taste_fruity:c… 1 1.63e2 1.63e2 0.164 6.86e- 1 ## 12 cata_taste_berry:cata_taste_fruity:s… 5 1.05e3 2.11e2 0.212 9.57e- 1 ## 13 cata_taste_berry:cata_taste_fermente… 5 1.64e3 3.29e2 0.331 8.94e- 1 ## 14 cata_taste_fruity:cata_taste_ferment… 4 1.12e3 2.81e2 0.283 8.89e- 1 ## 15 cata_taste_berry:cata_taste_fruity:c… 3 6.89e3 2.30e3 2.31 7.54e- 2 ## 16 Residuals 465 4.62e5 9.94e2 NA NA ## # … with abbreviated variable names ¹​`Sum Sq`, ²​`Mean Sq`, ³​`F value` Here we can see a lot of things (probably too many things, as we’ll get to in a moment). But I want to point out several aspects of this model that are of interest to us: The main effects of the CATA variables are all significant, as is the sample variable: the data tell us that the different strawberry samples are liked differently, and that the presence or absence of these 3 CATA variables matters as well. None of the interactions are very important. This means that different CATA effects, for example, don’t matter differently for overall liking for different samples. Because we requested interactions among 4 variables, we get estimates of interactions up to 4-way interactions. This is a lot of information, and probably an “overfit” model. However, we picked 3 CATA variables at random. There are 13 of these actually collected. If even 4-way interactions are probably overfit, how can we possibly pick which of these to include in our model? We will learn about a data-driven approach to this called a “Partition Tree” later today, but for now let’s look at a classic approach: stepwise regression/ANOVA. 10.2.1 Stepwise model fitting Imagine we have a large number of predictor variables, and part of our analysis needs are deciding which to include in a model. This is a common situation, as it corresponds to the general research question: “Which treatments actually matter?” We can approach this question by letting our data drive the analysis. We know about \\(R^2\\), which is an estimate of how well our model fits the data. There are many other fit criteria. What if we just add variables to our model if they make it fit the data better? This is a reasonable (although very risky, it turns out!) approach that is quite common. This is often called “stepwise model selection” or “fitting”. # We first subset our data for ease of use to just the variables we will consider berry_cata &lt;- berry_data %&gt;% select(lms_overall, sample, contains(&quot;cata&quot;)) # Then we fit a model with no variables intercept_only &lt;- aov(lms_overall ~ 1, data = berry_cata) # And we fit a model with ALL possible variables all &lt;- aov(lms_overall ~ .^2, data = berry_cata) # Finally, we tell R to &quot;step&quot; between the no-variable model and the model with # all the variables. At each step, R will consider all possible variables to # add to or subtract from the model, based on which one will most decrease a # fit criterion called the AIC (don&#39;t worry about it right now). This will # be repeated until the model can no longer be improved, at which point we have # a final, data-driven model stepwise &lt;- step(intercept_only, direction = &quot;both&quot;, scope = formula(all), trace = 0) anova(stepwise) %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 18 × 6 ## effect Df Sum S…¹ Mean …² F val…³ `Pr(&gt;F)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cata_taste_berry 1 298464. 298464. 339. 9.44e-58 ## 2 cata_taste_fruity 1 47443. 47443. 53.9 9.15e-13 ## 3 cata_taste_fermented 1 30470. 30470. 34.6 7.59e- 9 ## 4 sample 5 30699. 6140. 6.97 2.66e- 6 ## 5 cata_taste_floral 1 11900. 11900. 13.5 2.64e- 4 ## 6 cata_taste_caramel 1 8830. 8830. 10.0 1.64e- 3 ## 7 cata_taste_earthy 1 2848. 2848. 3.23 7.28e- 2 ## 8 cata_taste_melon 1 2803. 2803. 3.18 7.50e- 2 ## 9 cata_taste_none 1 694. 694. 0.788 3.75e- 1 ## 10 cata_taste_berry:cata_taste_floral 1 4671. 4671. 5.30 2.17e- 2 ## 11 cata_taste_berry:cata_taste_caramel 1 3576. 3576. 4.06 4.45e- 2 ## 12 sample:cata_taste_earthy 5 11938. 2388. 2.71 1.98e- 2 ## 13 cata_taste_fermented:cata_taste_melon 1 5446. 5446. 6.18 1.32e- 2 ## 14 cata_taste_berry:cata_taste_earthy 1 3069. 3069. 3.48 6.25e- 2 ## 15 cata_taste_berry:cata_taste_fermented 1 3902. 3902. 4.43 3.58e- 2 ## 16 cata_taste_fruity:cata_taste_none 1 2309. 2309. 2.62 1.06e- 1 ## 17 cata_taste_floral:cata_taste_melon 1 1779. 1779. 2.02 1.56e- 1 ## 18 Residuals 484 426306. 881. NA NA ## # … with abbreviated variable names ¹​`Sum Sq`, ²​`Mean Sq`, ³​`F value` # We can also see the actual effect on lms_overall of the selected variables stepwise$coefficients %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 26 × 2 ## effect value ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -3.02 ## 2 cata_taste_berry 38.5 ## 3 cata_taste_fruity 17.3 ## 4 cata_taste_fermented -28.0 ## 5 sample2 -10.7 ## 6 sample3 -1.70 ## 7 sample4 -7.15 ## 8 sample5 0.634 ## 9 sample6 -24.3 ## 10 cata_taste_floral 23.6 ## # … with 16 more rows ## # ℹ Use `print(n = ...)` to see more rows This kind of step-wise approach is very common, but it is also potentially problematic, as it finds a solution that fits the data, not the question. Whether this model will predict future outcomes is unclear. It is always a good idea when using these approaches to test the new model on unseen data, or to treat these approaches as exploratory rather than explanatory approaches: they are good for generating future, testable hypotheses, rather than giving a final answer or prediction. Therefore, in our example, we might generate a hypothesis for further testing: “It is most important for strawberries to have ‘berry’, ‘fruity’, and not ‘fermented’ tastes for overall consumer acceptability.” We can then test this by obtaining berries known to have those qualities and testing whether, for example, more “berry”-like strawberries are indeed better liked. 10.2.2 Plotting interaction effects We really don’t have much in the way of interaction effects in our ANOVA models, but let’s look at how we might visualize those effects–this is one of the most effective ways to both understand and communicate about interacting variables in multi-way models. # First, we&#39;ll set up a summary table for our data: we want the mean lms_overall # for each berry sample for when cata_taste_fermented == 0 and == 1 example_interaction_data &lt;- berry_data %&gt;% group_by(sample, cata_taste_fermented) %&gt;% summarize(mean_liking = mean(lms_overall), se_liking = sd(lms_overall)/sqrt(n())) example_interaction_data ## # A tibble: 12 × 4 ## # Groups: sample [6] ## sample cata_taste_fermented mean_liking se_liking ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 36.4 4.88 ## 2 1 1 -17.5 14.1 ## 3 2 0 17.6 3.77 ## 4 2 1 -26 13.8 ## 5 3 0 20.7 4.30 ## 6 3 1 20.8 15.5 ## 7 4 0 14.9 4.23 ## 8 4 1 -12.1 14.7 ## 9 5 0 37.2 3.74 ## 10 5 1 -7.46 13.6 ## 11 6 0 -5.69 4.37 ## 12 6 1 -35.7 10.4 # Now we can use this to draw a plot illustrating the INTERACTION of sample and # cata_taste_fermented on lms_overall p &lt;- example_interaction_data %&gt;% ggplot(aes(x = cata_taste_fermented, y = mean_liking, color = sample)) + geom_point() + geom_errorbar(aes(ymin = mean_liking - 2 * se_liking, ymax = mean_liking + 2 * se_liking), linetype = &quot;dashed&quot;, width = 0.1) + geom_line() + theme_classic() + scale_color_viridis_d() p We can see that there may in fact be some kind of interactive effect, in that while overall it seems like presence of a “fermented” flavor decreases liking for most samples, it doesn’t have much of an effect on Sample 3. But we don’t see this as an overall effect in our model, and our uncertainty (error) bars indicate why this might be: it seems like there is such high variability in that sample’s lms_overall ratings that we can’t say for sure whether that mean estimate is very reliable–I would go so far as to say we’d want do some further exploration of that sample itself in order to understand why it is so much more variable than the other samples. # This is an interactive plot from the plotly package, which you may enjoy # fooling around with as you move forward. It makes seeing plot elements a # little easier in complex plots. # It is possible to render ggplots interactive in one step # (mostly) by saving them and using the ggplotly() function. plotly::ggplotly(p) 10.3 Multiple regression So far we have focused our thinking on multiple, categorical predictors: variables that can take one of a set of discrete values. This is advantageous for learning about interactions, because it is much easier to visualize the interaction of categorical variables through interactions like those above. But what if we have several continuous predictors? Just as simple regression and ANOVA are analogues of each other, multiple regression is the equivalent of multi-way ANOVA when one or more of the predictors is continuous instead of categorical (when we have a mix of continuous and categorical variables we tend to deal with them through either multiple regression or with something called ANCOVA: Analysis of Co-Variance). Multiple regression involves predicting the value of an outcome from many continuous or categorical predictors. Multiple regression is also a topic that deserves (and is taught as) a course-length topic in its own right (see for example STAT 6634). I will be barely scratching the surface of multiple regression here–as always, I am going to attempt to show you how to implement a method in R. We can frame our objective in multiple regression as predicting the value of a single outcome \\(y\\) from a set of predictors \\(x_1, x_2, ..., x_n\\). \\(\\hat{y}_i = \\beta_1 * x_{1i} + \\beta_2 * x_{2i} + ... + \\beta_n * x_{ni} + \\alpha\\) The tasks in multiple regression are: To develop a model that predicts \\(y\\) To identify variables \\(x_1, x_2, ...\\) etc that are actually important in predicting \\(y\\) The first task is usually evaluated in the same way as we did in simple regression–by investigating goodness-of-fit statistics like \\(R^2\\). This is often interpreted as overall quality of the model. The second task is often considered more important in research applications: we want to understand which \\(x\\)-variables significantly predict \\(y\\). We usually assess this by investigating statistical significance of the \\(\\beta\\)-coefficients for each predictor. Finally, a third and important task in multiple regression is identifying interacting predictor variables. In my usual, imprecise interpretation, an interactive effect can be stated as “the effect of \\(x_1\\) is different at different values of \\(x_2\\)”–this means that we have a multiplicative effect in our model. The model given above omits interactive terms, which would look like: \\(\\hat{y}_i = \\beta_1 * x_{1i} + \\beta_2 * x_{2i} + ... + \\beta_n * x_{ni} + \\beta_{1*2} * x_{1i} * x_{2i} + ... + \\alpha\\) You can see why I left them out! 10.3.1 Fitting a multiple regression to our model It’s been long enough–let’s review what our dataset looks like: berry_data %&gt;% glimpse ## Rows: 510 ## Columns: 24 ## $ subject &lt;fct&gt; 1033784, 1033784, 1033784, 1033784, 1033784, 1033… ## $ age &lt;ord&gt; 51-65, 51-65, 51-65, 51-65, 51-65, 51-65, 51-65, … ## $ gender &lt;fct&gt; female, female, female, female, female, female, f… ## $ berry &lt;chr&gt; &quot;strawberry&quot;, &quot;strawberry&quot;, &quot;strawberry&quot;, &quot;strawb… ## $ sample &lt;fct&gt; 4, 1, 2, 6, 3, 5, 4, 1, 2, 6, 3, 5, 4, 1, 2, 6, 3… ## $ percent_shopping &lt;ord&gt; 75-100%, 75-100%, 75-100%, 75-100%, 75-100%, 75-1… ## $ lms_appearance &lt;dbl&gt; 9, 68, 11, 47, 71, 68, 73, 13, -11, -13, 75, 37, … ## $ lms_overall &lt;dbl&gt; -19, 74, 26, -28, 51, 51, 57, -10, 12, 36, 53, 39… ## $ lms_taste &lt;dbl&gt; -3, 77, 27, -28, 51, 48, 39, -10, 14, 25, 57, 34,… ## $ lms_texture &lt;dbl&gt; 20, 72, 27, 30, 68, 63, 72, 37, 53, 36, 77, 39, 1… ## $ lms_aroma &lt;dbl&gt; -1, 81, 76, 57, 72, 21, -29, 11, -1, 1, 58, -11, … ## $ cata_taste_floral &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0… ## $ cata_taste_berry &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0… ## $ cata_taste_grassy &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1… ## $ cata_taste_fermented &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cata_taste_fruity &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0… ## $ cata_taste_citrus &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cata_taste_earthy &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1… ## $ cata_taste_none &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0… ## $ cata_taste_peachy &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0… ## $ cata_taste_caramel &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cata_taste_grapey &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cata_taste_melon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ cata_taste_cherry &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0… While we have no real reason to do so, let’s build a model that predicts lms_overall from lms_appearance, lms_taste, lms_aroma, and lms_texture. We can frame the research question as: “In this dataset, can we understand overall liking as an interaction of multiple modalities?” So we would write the model we asked for in words above as: lms_overall ~ lms_taste + lms_appearance + lms_aroma + lms_texture (notice we are not adding interactions yet). berry_mlm &lt;- berry_data %&gt;% lm(lms_overall ~ lms_taste + lms_appearance + lms_aroma + lms_texture, data = .) summary(berry_mlm) ## ## Call: ## lm(formula = lms_overall ~ lms_taste + lms_appearance + lms_aroma + ## lms_texture, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -75.936 -8.169 -0.751 8.788 88.033 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.153003 0.883068 3.571 0.00039 *** ## lms_taste 0.769195 0.022335 34.439 &lt; 2e-16 *** ## lms_appearance 0.087427 0.021291 4.106 4.69e-05 *** ## lms_aroma 0.008425 0.024036 0.350 0.72612 ## lms_texture 0.124233 0.023159 5.364 1.24e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.66 on 505 degrees of freedom ## Multiple R-squared: 0.8619, Adjusted R-squared: 0.8608 ## F-statistic: 788 on 4 and 505 DF, p-value: &lt; 2.2e-16 This model explores the relationships we can visualize in various ways shown below: library(ggforce) berry_data %&gt;% ggplot(aes(x = .panel_x, y = .panel_y)) + geom_point(alpha = 0.1) + geom_smooth(method = lm, color = &quot;red&quot;) + geom_density_2d(color = &quot;black&quot;) + geom_autodensity() + facet_matrix(rows = vars(contains(&quot;lms&quot;)), cols = vars(contains(&quot;lms&quot;)), layer.lower = 3, layer.upper = c(1, 2), layer.diag = 4) + theme_classic() Just as in simple regression, we can use accessor functions to get coefficients, predicted values, etc. coefficients(berry_mlm) %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 5 × 2 ## effect value ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 3.15 ## 2 lms_taste 0.769 ## 3 lms_appearance 0.0874 ## 4 lms_aroma 0.00842 ## 5 lms_texture 0.124 # just looking at the first few predictions head(predict(berry_mlm)) ## 1 2 3 4 5 6 ## 4.108488 77.953222 28.877523 -10.068231 57.643668 54.022987 # the actual values of lms_overall berry_data %&gt;% head() %&gt;% select(contains(&quot;lms&quot;)) %&gt;% relocate(lms_overall) ## # A tibble: 6 × 5 ## lms_overall lms_appearance lms_taste lms_texture lms_aroma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -19 9 -3 20 -1 ## 2 74 68 77 72 81 ## 3 26 11 27 27 76 ## 4 -28 47 -28 30 57 ## 5 51 71 51 68 72 ## 6 51 68 48 63 21 Now, we will take a look at what happens when we allow interactions in the model: berry_mlm_interactions &lt;- berry_data %&gt;% lm(lms_overall ~ lms_taste * lms_appearance * lms_aroma * lms_texture, data = .) anova(berry_mlm_interactions) %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 16 × 6 ## effect Df Sum S…¹ Mean …² F valu…³ `Pr(&gt;F)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lms_taste 1 7.57e5 7.57e5 3.13e+3 5.82e-216 ## 2 lms_appearance 1 8.96e3 8.96e3 3.71e+1 2.30e- 9 ## 3 lms_aroma 1 2.42e1 2.42e1 1.00e-1 7.52e- 1 ## 4 lms_texture 1 7.06e3 7.06e3 2.92e+1 1.02e- 7 ## 5 lms_taste:lms_appearance 1 7.21e1 7.21e1 2.98e-1 5.85e- 1 ## 6 lms_taste:lms_aroma 1 3.85e2 3.85e2 1.59e+0 2.07e- 1 ## 7 lms_appearance:lms_aroma 1 6.57e2 6.57e2 2.72e+0 9.99e- 2 ## 8 lms_taste:lms_texture 1 2.39e2 2.39e2 9.89e-1 3.20e- 1 ## 9 lms_appearance:lms_texture 1 7.95e0 7.95e0 3.29e-2 8.56e- 1 ## 10 lms_aroma:lms_texture 1 4.04e2 4.04e2 1.67e+0 1.97e- 1 ## 11 lms_taste:lms_appearance:lms_aroma 1 5.38e2 5.38e2 2.23e+0 1.36e- 1 ## 12 lms_taste:lms_appearance:lms_textu… 1 4.01e2 4.01e2 1.66e+0 1.99e- 1 ## 13 lms_taste:lms_aroma:lms_texture 1 9.52e1 9.52e1 3.94e-1 5.31e- 1 ## 14 lms_appearance:lms_aroma:lms_textu… 1 1.09e3 1.09e3 4.49e+0 3.46e- 2 ## 15 lms_taste:lms_appearance:lms_aroma… 1 5.69e2 5.69e2 2.35e+0 1.26e- 1 ## 16 Residuals 494 1.19e5 2.42e2 NA NA ## # … with abbreviated variable names ¹​`Sum Sq`, ²​`Mean Sq`, ³​`F value` We have some significant higher-order interactions, but again we are ending up estimating a lot of new effects to gain very little in predictive power. We can compare the \\(R^2\\) for both models: map(list(berry_mlm, berry_mlm_interactions), ~summary(.) %&gt;% .$r.squared) ## [[1]] ## [1] 0.8619019 ## ## [[2]] ## [1] 0.8668665 We are adding a lot of “cruft” to our models without getting much in return. 10.3.2 A second example of stepwise model fitting We can use a stepwise model fitting approach on regression just as easily as we can on ANOVA. To do so, we will follow the same steps: Set up a “null” model that just estimates the mean of the outcome variable (lms_overall) Set up a “full” model that includes all possible predictors for the outcome variable Use the step() function to fit successive models until the best fit to the data is achieved # Trim our data for convenience berry_lms &lt;- select(berry_data, contains(&#39;lms&#39;)) # Set up our null model null_model &lt;- lm(lms_overall ~ 1, data = berry_lms) # Set up the full model, including all possible-level interactions full_model &lt;- lm(lms_overall ~ .^4, data = berry_lms) # Use step() to try all possible models selected_model &lt;- step(null_model, scope = formula(full_model), direction = &quot;both&quot;, trace = 0) summary(selected_model) ## ## Call: ## lm(formula = lms_overall ~ lms_taste + lms_texture + lms_appearance, ## data = berry_lms) ## ## Residuals: ## Min 1Q Median 3Q Max ## -75.461 -8.219 -0.825 8.963 87.996 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.28919 0.79231 4.151 3.88e-05 *** ## lms_taste 0.77229 0.02050 37.682 &lt; 2e-16 *** ## lms_texture 0.12323 0.02296 5.367 1.22e-07 *** ## lms_appearance 0.08864 0.02099 4.223 2.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.65 on 506 degrees of freedom ## Multiple R-squared: 0.8619, Adjusted R-squared: 0.861 ## F-statistic: 1052 on 3 and 506 DF, p-value: &lt; 2.2e-16 selected_model$coefficients %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 4 × 2 ## effect value ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 3.29 ## 2 lms_taste 0.772 ## 3 lms_texture 0.123 ## 4 lms_appearance 0.0886 In this case the model selection process dismisses all the higher-level interactions, as well as the main effect of lms_aroma. 10.3.3 Combining continuous and categorical data In berry_data we have both categorical and continuous predictors. Wouldn’t it be nice if we could combine them? In particular, we know that sample is an important variable, as different samples are definitely differently liked. Let’s see what happens when we include this in our multiple linear regression: berry_mlm_cat &lt;- berry_data %&gt;% # We will restrict our interactions to &quot;2nd-order&quot; (2-element) interactions # Because we think 3rd- and 4th-order are unlikely to be important and they are # A huge mess in the output lm(lms_overall ~ (sample + lms_appearance + lms_taste + lms_texture + lms_aroma)^2, data = .) anova(berry_mlm_cat) %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 16 × 6 ## effect Df `Sum Sq` `Mean Sq` `F value` `Pr(&gt;F)` ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sample 5 101078. 20216. 84.3 2.83e- 63 ## 2 lms_appearance 1 194144. 194144. 810. 1.25e-104 ## 3 lms_taste 1 472036. 472036. 1969. 6.58e-171 ## 4 lms_texture 1 7214. 7214. 30.1 6.70e- 8 ## 5 lms_aroma 1 4.24 4.24 0.0177 8.94e- 1 ## 6 sample:lms_appearance 5 852. 170. 0.711 6.15e- 1 ## 7 sample:lms_taste 5 1504. 301. 1.26 2.82e- 1 ## 8 sample:lms_texture 5 1681. 336. 1.40 2.22e- 1 ## 9 sample:lms_aroma 5 4280. 856. 3.57 3.52e- 3 ## 10 lms_appearance:lms_taste 1 2.19 2.19 0.00912 9.24e- 1 ## 11 lms_appearance:lms_texture 1 0.549 0.549 0.00229 9.62e- 1 ## 12 lms_appearance:lms_aroma 1 291. 291. 1.21 2.71e- 1 ## 13 lms_taste:lms_texture 1 20.6 20.6 0.0858 7.70e- 1 ## 14 lms_taste:lms_aroma 1 412. 412. 1.72 1.91e- 1 ## 15 lms_texture:lms_aroma 1 11.7 11.7 0.0489 8.25e- 1 ## 16 Residuals 474 113615. 240. NA NA coefficients(berry_mlm_cat) %&gt;% as_tibble(rownames = &quot;effect&quot;) ## # A tibble: 36 × 2 ## effect value ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 2.86 ## 2 sample2 3.02 ## 3 sample3 1.16 ## 4 sample4 -3.95 ## 5 sample5 2.00 ## 6 sample6 -2.12 ## 7 lms_appearance 0.209 ## 8 lms_taste 0.724 ## 9 lms_texture 0.0668 ## 10 lms_aroma 0.0975 ## # … with 26 more rows ## # ℹ Use `print(n = ...)` to see more rows Whoa, what just happened? Well, sample is a categorical variable with 6 levels, and when we enter it into a regression the most conventional way to deal with that data is to create a set of \\(6 - 1 = 5\\) “dummy” variables , and which take on a value of 1 whenever the sample is equal to that level, and 0 otherwise. That means that each adds some constant to the mean based on the estimated difference from that sample. This is where the difference between the summary() and anova() function becomes important. If we look at the summary table for our new model with both categorical and continuous variables we see something awful: summary(berry_mlm_cat) ## ## Call: ## lm(formula = lms_overall ~ (sample + lms_appearance + lms_taste + ## lms_texture + lms_aroma)^2, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -59.378 -7.651 -0.844 8.240 85.050 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.861e+00 2.908e+00 0.984 0.32572 ## sample2 3.017e+00 3.540e+00 0.852 0.39453 ## sample3 1.162e+00 3.939e+00 0.295 0.76808 ## sample4 -3.946e+00 3.656e+00 -1.079 0.28101 ## sample5 2.005e+00 4.209e+00 0.476 0.63408 ## sample6 -2.122e+00 3.538e+00 -0.600 0.54884 ## lms_appearance 2.093e-01 7.155e-02 2.925 0.00361 ** ## lms_taste 7.236e-01 5.926e-02 12.211 &lt; 2e-16 *** ## lms_texture 6.684e-02 6.104e-02 1.095 0.27410 ## lms_aroma 9.746e-02 7.094e-02 1.374 0.17014 ## sample2:lms_appearance -1.007e-01 8.436e-02 -1.194 0.23302 ## sample3:lms_appearance -1.471e-01 9.139e-02 -1.610 0.10812 ## sample4:lms_appearance -7.646e-02 8.866e-02 -0.862 0.38891 ## sample5:lms_appearance -1.062e-01 8.753e-02 -1.213 0.22558 ## sample6:lms_appearance -1.297e-01 9.381e-02 -1.382 0.16748 ## sample2:lms_taste 7.621e-02 8.298e-02 0.918 0.35890 ## sample3:lms_taste 2.964e-02 7.747e-02 0.383 0.70216 ## sample4:lms_taste -7.585e-02 8.206e-02 -0.924 0.35579 ## sample5:lms_taste 1.857e-01 9.133e-02 2.034 0.04254 * ## sample6:lms_taste -1.038e-02 8.185e-02 -0.127 0.89914 ## sample2:lms_texture -1.768e-02 7.820e-02 -0.226 0.82119 ## sample3:lms_texture 1.135e-01 8.243e-02 1.378 0.16900 ## sample4:lms_texture 7.640e-02 8.791e-02 0.869 0.38522 ## sample5:lms_texture 2.949e-02 9.143e-02 0.323 0.74715 ## sample6:lms_texture 1.286e-01 8.845e-02 1.454 0.14668 ## sample2:lms_aroma -1.079e-01 9.158e-02 -1.179 0.23914 ## sample3:lms_aroma -1.297e-01 9.394e-02 -1.381 0.16792 ## sample4:lms_aroma 5.080e-02 9.373e-02 0.542 0.58807 ## sample5:lms_aroma -2.864e-01 9.608e-02 -2.980 0.00303 ** ## sample6:lms_aroma -1.092e-01 9.636e-02 -1.134 0.25753 ## lms_appearance:lms_taste 1.939e-04 6.653e-04 0.291 0.77085 ## lms_appearance:lms_texture 5.889e-05 6.243e-04 0.094 0.92488 ## lms_appearance:lms_aroma -8.988e-04 6.320e-04 -1.422 0.15568 ## lms_taste:lms_texture -2.195e-04 5.244e-04 -0.419 0.67576 ## lms_taste:lms_aroma 7.886e-04 6.207e-04 1.270 0.20454 ## lms_texture:lms_aroma -1.709e-04 7.728e-04 -0.221 0.82509 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.48 on 474 degrees of freedom ## Multiple R-squared: 0.8734, Adjusted R-squared: 0.864 ## F-statistic: 93.4 on 35 and 474 DF, p-value: &lt; 2.2e-16 Not only is this ugly, but it also obscures a potentially important effect: according to this summary, we don’t have any significant effects of sample, which contradicts our initial ANOVA. So we need to be careful–when we use summary() on categorical variables in an lm() model object, it doesn’t give us the summary we’re expecting. But it does give us the expected coefficients for each level of the categorical variable. So when we have more complicated models we need to explore them carefully and thoroughly, using multiple functions. There do seem to be some potentially interesting interactions in this model–let’s take a look: p &lt;- berry_data %&gt;% select(sample, contains(&quot;lms&quot;)) %&gt;% pivot_longer(names_to = &quot;scale&quot;, values_to = &quot;rating&quot;, cols = -c(&quot;sample&quot;, &quot;lms_overall&quot;)) %&gt;% ggplot(aes(x = rating, y = lms_overall, color = sample)) + geom_point(alpha = 0.3) + geom_smooth(method = lm, se = FALSE) + facet_wrap(~scale) + scale_color_viridis_d() + theme_classic() p We can see that the response of sample 1 is quite different than the others for lms_appearance and lms_aroma. It seems like being rated higher on those attributes has a larger effect on lms_overall for sample 1 than for the other 5 samples. ggplotly(p) We can use this widget to explore these data for further hypotheses. Specifically, when we split up the data at the levels of different categorical variables and investigate other variables, we are exploring simple effects. These are the subject of the last topic we will explore–a simple machine-learning approach called Categorization, Partition, or Regression Trees. But, before we move there, let’s have… 10.4 Some notes of caution on model selection We’ve been talking about terms like “model selection”, which are jargon-y, so let’s refocus on what we’re doing: we are answering the question of “what of our experimental variables actually matter in explaining or predicting an outcome we care about?” In our example, we are asking, “if we ask people a lot of questions about berries, which ones are most related to how much they say they like each berry?” I’ve shown off some powerful, data-based approaches to letting an algorithm decide on which variables to include in the model. Why don’t we just use these and make the process “objective”? While we will be getting into this question in detail next week, here are several things to consider: Just remember the following: we are making predictions from data, not explaining causality. Without a theory that explains why any relationship is one we would expect to see, we run into the problem of just letting our coincidence-finding systems make up a story for us, which leads us to… Without expertise, we are just exploting patterns in the data that are quite possibly either accidental or are caused by a third, underlying variable. Remember the shark-attack example? The reason we cannot use “objective” processes is that, unfortunately, objectivity doesn’t exist! At least, not in the way we wish it did. For example, our step() function minimizes the AIC, which is the Akaike Information Criterion. This isn’t some universal constant that exists–it is a relationship derived from the Maximum Likelihood Estimate of a model with the number of parameters in the model. Essentially, this expresses a human hypothesis of what makes a model function well. But there are other criteria we could minimize that instantiate different arguments. Just because something is a number doesn’t mean it is objective. 10.5 Categorization and Regression Trees (CART): A data-based approach We’re going to end today with a machine-learning technique that combines and generalizes the tools of multi-way ANOVA, multiple regression, and stepwise model fitting we’ve been exploring today. We’re just going to dip our toes into so-called Categorization and Regression Trees (CART from now on) today, but I hope this will motivate you to examine them as a tool to use yourself. Practical Statistics for Data Scientists (Bruce and Bruce 2017, 220–30) gives an excellent introduction to this topic, and it will be part of the reading this week. Very briefly, CART is appealing because: It generates a simple, explanatory set of explanatory rules for explaining an outcome variable in a dataset that are easily communicated to a broad audience. The outcome variable being explained can be either continuous (a regression tree) or categorical (a categorization tree) without loss of generality in the model. Trees form the basis for more advanced, powerful machine-learning models such as Random Forest or Boosted Tree models which are among some of the most powerful and accessible models for machine learning. In general, CART models can be made to fit any kind of data without relying on the (generally wrong) assumptions of traditional linear models. So how does a tree work? Let’s take a look at two examples. 10.5.1 Regression trees First, we’ll begin by applying a tree approach to the same task we’ve been exploring in berry_data–predicting lms_overall from the best set of predictor variables. We will use some new packages for this: rpart, rpart.plot, and partykit. We will take a very similar approach to our stepwise model fitting, in that we will give the CART model a large set of variables to predict lms_overall from. We are going to use just our CATA and categorical data because it will make a more interesting tree, but in reality we’d want to include all variables we think might reasonably matter. tree_data &lt;- berry_data %&gt;% select(-subject, -lms_appearance, -lms_aroma, -lms_taste, -lms_texture) reg_cart &lt;- rpart(formula = lms_overall ~ ., data = tree_data) reg_cart ## n= 510 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 510 897147.10 15.290200 ## 2) cata_taste_berry&lt; 0.5 275 383234.50 -7.072727 ## 4) cata_taste_fermented&gt;=0.5 54 85208.00 -31.000000 * ## 5) cata_taste_fermented&lt; 0.5 221 259556.70 -1.226244 ## 10) cata_taste_fruity&lt; 0.5 165 182430.80 -5.969697 * ## 11) cata_taste_fruity&gt;=0.5 56 62474.50 12.750000 * ## 3) cata_taste_berry&gt;=0.5 235 215448.40 41.459570 ## 6) cata_taste_fruity&lt; 0.5 83 92591.23 28.903610 * ## 7) cata_taste_fruity&gt;=0.5 152 102626.80 48.315790 ## 14) sample=2,4,6 59 44403.80 38.372880 * ## 15) sample=1,3,5 93 48689.83 54.623660 * What this “stem and leaf” output tells us is how the data is split. In a CART, each possible predictor is examined for the largest effect size (mean difference) in the outcome variable (lms_overall). Then, the data are split according to that predictor, and the process is carried out again on each new dataset (this is called a “recursive” process). This continues until a stopping criterion is met. Usually stopping is either number of observations (rows) in the final “leaves” of the tree, but there are a number of possibilities. When we are predicting a continuous outcome like lms_overall, the test at each step is a simple ANOVA. This is probably easier envisioned as a diagram: reg_cart %&gt;% rpart.plot(type = 5, extra = 101) The “decision tree” output of this kind of model is a big part of it’s appeal. We can see that the most important predictor here is whether a subject has checked the CATA “berry” attribute. But after that, the next most important attribute is different. If there’s no “berry” flavor, it matters whether the sample is “fermented”, but if there is a berry flavor, then it matters whether it is “fruity”. And so on. In the terminal “leaves” we see the average rating for that combination of attributes as well as the number of samples that falls into that leaf. We can also use the partykit plotting functions to get some different output: reg_cart %&gt;% as.party %&gt;% plot I present this largely because it is possible (but somewhat complicated) to use partykit with the package ggparty to construct these plots within ggplot2, but I won’t be getting into it (because it is still very laborious for me). 10.5.2 Categorization trees In general, we cannot use a regression model to fit categorical outcomes (this is where we would get into logistic regression or other methods that use a transformation, which fall into the general linear model). However, CART models happily will predict a categorical outcome based on concepts of “homogeneity” and “impurity”–essentially, the model will try to find the splitting variables that end up with leaves having the most of one category of outcomes. Here, we are going to do something conceptually odd: we’re going to predict sample from all of our other variables. We’re going to do this for two reasons: This is actually an interesting question: we are asking what measured variables best separate our samples. This emphasizes the fact that our data is almost entirely observational: in general we should be careful about forming conclusions (as opposed to further hypotheses) from these kinds of data, because our variables do not have one-way causal relationships. catt_data &lt;- berry_data %&gt;% select(-subject) cat_tree &lt;- rpart(sample ~ ., data = catt_data) cat_tree ## n= 510 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 510 425 1 (0.17 0.17 0.17 0.17 0.17 0.17) ## 2) lms_aroma&gt;=12.5 237 178 1 (0.25 0.17 0.18 0.12 0.21 0.072) ## 4) lms_appearance&lt; 47.5 170 119 1 (0.3 0.2 0.12 0.11 0.19 0.082) ## 8) lms_overall&gt;=50.5 60 30 1 (0.5 0.15 0.13 0.067 0.12 0.033) * ## 9) lms_overall&lt; 50.5 110 84 5 (0.19 0.23 0.11 0.13 0.24 0.11) ## 18) lms_appearance&lt; -55 10 4 2 (0.1 0.6 0.1 0.2 0 0) * ## 19) lms_appearance&gt;=-55 100 74 5 (0.2 0.19 0.11 0.12 0.26 0.12) ## 38) cata_taste_grassy&gt;=0.5 35 25 2 (0.11 0.29 0.11 0.23 0.17 0.086) * ## 39) cata_taste_grassy&lt; 0.5 65 45 5 (0.25 0.14 0.11 0.062 0.31 0.14) ## 78) lms_aroma&gt;=56.5 18 12 1 (0.33 0.22 0.22 0.056 0.056 0.11) * ## 79) lms_aroma&lt; 56.5 47 28 5 (0.21 0.11 0.064 0.064 0.4 0.15) * ## 5) lms_appearance&gt;=47.5 67 44 3 (0.12 0.09 0.34 0.15 0.25 0.045) ## 10) lms_overall&gt;=53.5 37 25 5 (0.19 0.081 0.16 0.24 0.32 0) ## 20) lms_taste&gt;=75.5 9 4 1 (0.56 0 0.11 0.33 0 0) * ## 21) lms_taste&lt; 75.5 28 16 5 (0.071 0.11 0.18 0.21 0.43 0) * ## 11) lms_overall&lt; 53.5 30 13 3 (0.033 0.1 0.57 0.033 0.17 0.1) * ## 3) lms_aroma&lt; 12.5 273 205 6 (0.095 0.16 0.15 0.21 0.13 0.25) ## 6) lms_appearance&gt;=-9.5 180 130 4 (0.067 0.13 0.2 0.28 0.16 0.17) ## 12) cata_taste_berry&gt;=0.5 66 49 5 (0.14 0.11 0.23 0.17 0.26 0.11) * ## 13) cata_taste_berry&lt; 0.5 114 75 4 (0.026 0.15 0.18 0.34 0.096 0.2) * ## 7) lms_appearance&lt; -9.5 93 55 6 (0.15 0.23 0.065 0.075 0.075 0.41) * Again, this will be easier to see if we plot it: cat_tree %&gt;% rpart.plot(type = 5, extra = 101) Figuring out how to plot this requires a fair amount of fiddling, but ultimately we are able to speak to how differences in many, interacting variables lead us to groups that are majority one type of sample. 10.6 Wrap-up Today, we’ve spent our time on methods for understanding how multiple factors (variables) can be related to an outcome variable. We’ve moved from familiar models like ANOVA and regression to a more complex, machine-learning based approach (CART). What these models all have in common is that they attempt to explain a single outcome variable based on many possible predictors. We’ve just scratched the surface of this topic, and for your own work you’ll want to learn about both discipline-specific expectations for methods and what kind of approaches make sense for your own data. Also, this list will be important: Are you trying to predict: A single outcome from multiple categorical or continuous variables? Great! We did this! Multiple outcomes from multiple continuous or categorical variables? You’re going to need multivariate stats (MANOVA, PCA, etc). No outcomes, just trying to find patterns in your variables? You’re going to need machine learning (cluster analysis, PCA, etc). As should be clear from this little list, we’ve only just, just, just started to scratch the surface. But I hope this will make you feel confident that you can learn these tools and apply them as needed. No one, including me, is an expert in all of these approaches. The key is to learn the basic rules and learn as much as you need to accomplish whatever your current goals are! References References "],["data-literacy-and-an-applied-example-of-data-science.html", "11 Data literacy and an applied example of data science 11.1 Text mining and processing 11.2 Data literacy 11.3 Counting References", " 11 Data literacy and an applied example of data science # Setup chunk is made visible for clarity knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(skimr) library(naniar) library(ggforce) library(broom) set.seed(1414) # Some speciic text-mining tools library(rvest) library(tidyverse) library(robotstxt) library(tidytext) library(spacyr) 11.1 Text mining and processing Today we’re going to walk through some work that’s been ongoing since (*gulp*)… 2019. Being a faculty member sometimes means that projects you’re excited about get put really on the back burner! The overall goal of this project wasis to develop tools to analyze the flavor of cocktails, as described on a blog that I’ve been a big fan of since I graduated from undergrad–it’s called Cocktail Virgin, and the author(s) have posted and blogged about 1-3 cocktails per day since 2007! As an aside, if you want to learn to be a “mixologist”, this is a great resource. Love it to death. The Cocktail Virgin website today. 11.1.1 Introduction: the data source The reason I thought this would be an interesting project are the following: Cocktails are recipes for almost pure flavor–the flavor chemists in this room will tell you that ethanol is a better solvent for most flavor compounds than water, and the lack of other matrix effects means that the combinations of specific (mass-produced) ingredients allows for close-to-experimental control of flavor in cocktails. The Cocktail Virgin site has a very large database of cocktails (at last count &gt;5000, but since that was 6 months ago we might be &gt;6000 at this point). The format of the posts on the Cocktail Virgin site are very regular, which makes automatic text processing easier. The authors of Cocktail Virgin have always put a focus on flavor in their non-recipe text: they always talk about the way a cocktail smells and tastes. This makes it a potentially really interesting resource for connecting the ingredients in cocktails to the flavors in those cocktails, without spending thousands of dollars on experimentation. This last point is really what motivated me. Let’s take a look at a classic example, The Herbivore, posted September 10, 2020. Ignoring the photo, we have the classic elements of a Cocktail Virgin post. The title is the name of the cocktail: the herbivore This is followed by a well-structured recipe for the cocktail 1 1/2 oz Genever (Bols) 3/4 oz Cardamaro 1/4 oz Green Chartreuse 2 dash Celery Bitters Stir with ice, strain into a cocktail coupe and garnish with a lemon twist. This is followed by several paragraphs of expository text that explains the origin of the drink, and comments on how it tastes: Two Thursdays ago, the cooler evening weather made me seek out a stirred drink. The one that stood out as right for the moment was the Herbivore by Juliet Ceballos of the City House Nashville that was published in Punch Drinks. The Herbivore seemed like a win for I have found that Genever and Cardamaro are a great duo in drinks like the Walking Spanish, Deck Hand, and the Kid with the Replaceable Head. Moreover, Green Chartreuse and celery bitters felt like wonderful accents to bring the balance a touch more to the herbaceous side. The Herbavore proffered a lemon and malt aroma that gave way to a malt and grape sip. Next, Genever and herbal flavors on the swallow finished off with elegant Green Chartreuse and celery notes. Finally (and you’ll see this is key), the drinks are very well “tagged” with generic ingredient information in the tags section of the website: ingredients: amaro, bitters (other), chartreuse (green), genever I really can’t emphasize enough how well-maintained the website is, and what a good resource it is for people who love making mixed drinks! 11.1.2 Getting permission I reached out to Frederic Yarm, the primary owner of Cocktail Virgin, in 2018 to ask him if it would be alright for me to copy his website using some programmatic tools for research purposes. He gave me his permission, which was very kind! Technically, a lot of “text mining” or “web scraping” lives in a legal gray area. It is usually legal for us to use programs to copy this material, but subsequent use may or may not be legal. 11.1.3 A website is just (mark up) language You all have learned Markdown this semester. We know that knitr takes .Rmd files and translated them into whatever we want–usually .html files. These are the raw stuff of websites–so websites are just text! We can take a look using the Inspect or Source tools built right into your browsers. While this may look like a mess of garbage, the key insight here is that the structure of HTML actually makes it easy to pull out the bits of a website you want (if the website is built as a “flat” HTML page). There are many good tutorials on this process, but I think the one from Hadley Wickham and the one from Jakob Tures are probaly good places to start. [Stat545 also has a broader perspective on web data](https://stat545.com/web-data-slides.html. I will just walk through my approach. We start with checking whether we’re actually allowed to scrape a website (I got explicit permission, but most sites also tell you): robotstxt(&quot;https://cocktailvirgin.blogspot.com&quot;) ## $domain ## [1] &quot;https://cocktailvirgin.blogspot.com&quot; ## ## $text ## [robots.txt] ## -------------------------------------- ## ## User-agent: Mediapartners-Google ## Disallow: ## ## User-agent: * ## Disallow: /search ## Allow: / ## ## Sitemap: https://cocktailvirgin.blogspot.com/sitemap.xml ## ## ## ## ## ## ## $robexclobj ## &lt;Robots Exclusion Protocol Object&gt; ## $bots ## [1] &quot;Mediapartners-Google&quot; &quot;*&quot; ## ## $comments ## [1] line comment ## &lt;0 rows&gt; (or 0-length row.names) ## ## $permissions ## field useragent value ## 1 Disallow Mediapartners-Google ## 2 Disallow * /search ## 3 Allow * / ## ## $crawl_delay ## [1] field useragent value ## &lt;0 rows&gt; (or 0-length row.names) ## ## $host ## [1] field useragent value ## &lt;0 rows&gt; (or 0-length row.names) ## ## $sitemap ## field useragent value ## 1 Sitemap * https://cocktailvirgin.blogspot.com/sitemap.xml ## ## $other ## [1] field useragent value ## &lt;0 rows&gt; (or 0-length row.names) ## ## $check ## function (paths = &quot;/&quot;, bot = &quot;*&quot;) ## { ## spiderbar::can_fetch(obj = self$robexclobj, path = paths, ## user_agent = bot) ## } ## &lt;bytecode: 0x7ff96a1336d0&gt; ## &lt;environment: 0x7ff96a132b38&gt; ## ## attr(,&quot;class&quot;) ## [1] &quot;robotstxt&quot; For this example, we will look just at the recipe for The Herbivore. # rvest::read_html() is our workhorse function x &lt;- read_html(&quot;https://cocktailvirgin.blogspot.com/2020/09/the-herbivore.html&quot;) Notice that our URL (“address”) is just a text string. We use read_html() on that URL to get the HTML from that address. Now we will write a bunch of utility functions that will extract bits of the page we care about, based on that structure. You may have even added these kinds of structures to your Markdown already, using tags like {#tag}! # These are functions that will take a URL for a specific HTML page # from cocktailvirgin.blogspot.com and extract bits of it I want. get_body &lt;- function(x){ x %&gt;% html_nodes(&quot;#main&quot;) %&gt;% html_nodes(&quot;div.post-body&quot;) %&gt;% html_text(trim = T) } get_ingredients &lt;- function(x){ x %&gt;% html_nodes(&quot;.post-labels a&quot;) %&gt;% html_text() } get_title &lt;- function(x){ x %&gt;% html_nodes(&quot;h3&quot;) %&gt;% html_text(trim = T) } get_date &lt;- function(x){ x %&gt;% html_nodes(&quot;.date-header&quot;) %&gt;% html_text() } get_recipe &lt;- function(x){ x %&gt;% html_nodes(&quot;.recipeDirection&quot;) %&gt;% html_text() } get_links &lt;- function(x){ x %&gt;% html_nodes(&quot;div.post-body&quot;) %&gt;% html_nodes(&quot;a&quot;) %&gt;% html_attr(name = &quot;href&quot;) } When we run these on our extracted website x, we get useful things back out! get_title(x) ## [1] &quot;the herbivore&quot; get_ingredients(x) ## [1] &quot;amaro&quot; &quot;bitters (other)&quot; &quot;chartreuse (green)&quot; ## [4] &quot;genever&quot; get_body(x) ## [1] &quot;1 1/2 oz Genever (Bols)\\n3/4 oz Cardamaro\\n1/4 oz Green Chartreuse\\n2 dash Celery Bitters\\nStir with ice, strain into a cocktail coupe and garnish with a lemon twist.\\nTwo Thursdays ago, the cooler evening weather made me seek out a stirred drink. The one that stood out as right for the moment was the Herbivore by Juliet Ceballos of the City House Nashville that was published in Punch Drinks. The Herbivore seemed like a win for I have found that Genever and Cardamaro are a great duo in drinks like the Walking Spanish, Deck Hand, and the Kid with the Replaceable Head. Moreover, Green Chartreuse and celery bitters felt like wonderful accents to bring the balance a touch more to the herbaceous side.\\n\\nThe Herbavore proffered a lemon and malt aroma that gave way to a malt and grape sip. Next, Genever and herbal flavors on the swallow finished off with elegant Green Chartreuse and celery notes.&quot; get_date(x) ## [1] &quot;Thursday, September 10, 2020&quot; OK, that’s just one recipe. I said there were &gt;5000. How do we get them? Let me give you two clues that you should be able to put together: The blog is sequential–it has a first post. Each post has a “Newer Post” link as part of the HTML. We can “Inspect” the HTML to find the ID of that link, and pull it. How could we then iterate through each page? x %&gt;% html_nodes(&quot;.blog-pager-newer-link&quot;) %&gt;% html_attr(&quot;href&quot;) ## [1] &quot;https://cocktailvirgin.blogspot.com/2020/09/narragansett.html&quot; So if we put this all together, we can write a script that is actually a little robot, built to extract each post and get out useful information. 11.1.4 Looking at some actual data One post isn’t that interesting. And working with all &gt;5000 would be a bit unwieldy for a class. So I built a random slice (slice_sample()!) of 100 posts from the dataset for us to play with. It’s stored not in a .csv or .xlsx file, but in a .Rdata file–we use load() to put this into memory. load(&quot;Files/Week 14/cocktailvirgin_demodata.RData&quot;) glimpse(demo_posts) ## Rows: 100 ## Columns: 7 ## $ post_id &lt;dbl&gt; 1619, 4685, 4206, 1023, 1818, 4983, 1042, 3991, 1905, 2097… ## $ title &lt;chr&gt; &quot;\\ndown at the dinghy\\n&quot;, &quot;\\ntransatlantic orbit\\n&quot;, &quot;\\nlo… ## $ body &lt;chr&gt; &quot;2 oz Bushmills Irish Whiskey\\n1/2 oz Yellow Chartreuse\\n1… ## $ ingredients &lt;chr&gt; &quot;#chez henri ### #hawthorne ### chartreuse (yellow) ### le… ## $ links &lt;chr&gt; &quot;http://lemixeur.blogspot.com/2012/04/le-mixeur-sharky-nin… ## $ URL &lt;chr&gt; &quot;https://cocktailvirgin.blogspot.com/2012/04/down-at-dingh… ## $ date &lt;chr&gt; &quot;Friday, April 20, 2012&quot;, &quot;Friday, September 27, 2019&quot;, &quot;S… Now that’s some data! Let’s take a look first at the “ingredients” used in this set of drinks. # Let&#39;s look at the ingredients (based on tags) for this set of drinks demo_posts %&gt;% mutate(title = str_squish(title)) %&gt;% # get rid of whitespace in names filter(!str_detect(ingredients, &quot;\\\\*[^original|^hot]&quot;)) %&gt;% # here we are removing some posts that are not recipes unnest_tokens(output = ingredient, input = ingredients, token = &quot;regex&quot;, pattern = &quot; ### &quot;) %&gt;% # when I scraped these reviews I piled tags into a single column, separated by &quot; ### &quot; for easy later parsing filter(!str_detect(ingredient, &quot;#|\\\\*&quot;)) -&gt; # and finally we are removing symbols the author used to mark &quot;non-ingredient&quot; tags demo_ingredients demo_ingredients %&gt;% count(ingredient, sort = TRUE) ## # A tibble: 94 × 2 ## ingredient n ## &lt;chr&gt; &lt;int&gt; ## 1 lime juice 31 ## 2 rum 27 ## 3 bitters (angostura) 24 ## 4 gin 22 ## 5 lemon juice 22 ## 6 simple syrup 15 ## 7 vermouth (sweet) 12 ## 8 pineapple juice 11 ## 9 whiskey (rye) 11 ## 10 campari 9 ## # … with 84 more rows ## # ℹ Use `print(n = ...)` to see more rows Above, you see a new function, which is the tidytext::unnest_tokens() function. This comes from a reading I highly recommend to anyone interested in learning the basics of text analysis: Text Mining with R (Silge and Robinson 2022). Rather than reinvent the wheel, I will quote the authors on what a “token” is in text analysis: A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens. So unnest_tokens() splits a character vector (a column in a tibble that stores text) into many, individual “tokens”, which are the main unit of analysis for text analysis. The most common way to do this is to split at “whitespace”–spaces and hard returns, while eliminating punctuation. This gives us a rough “token ~ word” equivalence. A side effect of using unnest_tokens() is that it “tidies” our tibble–we get a one-line-per-token format: # This is now a much longer tibble! demo_ingredients %&gt;% select(title, ingredient) ## # A tibble: 471 × 2 ## title ingredient ## &lt;chr&gt; &lt;chr&gt; ## 1 down at the dinghy chartreuse (yellow) ## 2 down at the dinghy lemon juice ## 3 down at the dinghy simple syrup (other) ## 4 down at the dinghy whiskey ## 5 transatlantic orbit apricot liqueur ## 6 transatlantic orbit batavia arrack ## 7 transatlantic orbit bitters (angostura) ## 8 transatlantic orbit bitters (aromatic) ## 9 transatlantic orbit gin ## 10 transatlantic orbit grapefruit juice ## # … with 461 more rows ## # ℹ Use `print(n = ...)` to see more rows In many ways unnest_tokens() is kind of like a purpose-built version of pivot_longer(). In this particular case, I used some optional arguments to unnest_tokens() to tell it to split at the ### symbol. This is because when I scraped these recipes, I used that as a separator for the ingredients listed as tags in the HTML files. If we wanted, for example, to split up the post body column (where the recipe and descriptions are), we could just use the plain function: demo_posts %&gt;% unnest_tokens(output = token, input = body) %&gt;% select(title, token) ## # A tibble: 24,501 × 2 ## title token ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot;\\ndown at the dinghy\\n&quot; 2 ## 2 &quot;\\ndown at the dinghy\\n&quot; oz ## 3 &quot;\\ndown at the dinghy\\n&quot; bushmills ## 4 &quot;\\ndown at the dinghy\\n&quot; irish ## 5 &quot;\\ndown at the dinghy\\n&quot; whiskey ## 6 &quot;\\ndown at the dinghy\\n&quot; 1 ## 7 &quot;\\ndown at the dinghy\\n&quot; 2 ## 8 &quot;\\ndown at the dinghy\\n&quot; oz ## 9 &quot;\\ndown at the dinghy\\n&quot; yellow ## 10 &quot;\\ndown at the dinghy\\n&quot; chartreuse ## # … with 24,491 more rows ## # ℹ Use `print(n = ...)` to see more rows However, you might notice that now the first few lines are all bits of recipe (“2 oz of this, 1 tbsp of that, etc”). I started this off by saying we were interested in sensory terms (smell, flavor, aroma, mouthfeel, etc). When we look back at the actual website, we see that there is clearly a formatting distinction between the recipe and the rest of the post, where the descriptions occur. We can use a tool called a “Regular Expression” to separate out our raw text into (approximately) recipes and descriptions # Now let&#39;s look at splitting apart the recipes (first part of the posts) from # the descriptions and commentary demo_posts %&gt;% mutate(title = str_squish(title)) %&gt;% filter(!str_detect(ingredients, &quot;\\\\*[^original|^hot]&quot;)) %&gt;% # here we are splitting at the first period followed by a new-line (\\n), which in about 95% of the articles seems to be the normal split point separate(col = body, into = c(&quot;recipe&quot;, &quot;description&quot;), sep = &quot;\\\\.\\\\n&quot;, extra = &quot;merge&quot;) -&gt; demo_posts_separated demo_posts_separated$title[1] ## [1] &quot;down at the dinghy&quot; demo_posts_separated$recipe[1] ## [1] &quot;2 oz Bushmills Irish Whiskey\\n1/2 oz Yellow Chartreuse\\n1/2 oz Cucumber Syrup (1:1)\\n1/2 oz Lemon Juice\\nShake with ice and strain into a flute (or cocktail) glass&quot; demo_posts_separated$description[1] ## [1] &quot;One of the surprise winners of the Le Mixeur Sharky Nine Stories event for me was bartender Rob Kraemer&#39;s Down at the Dinghy. The drink was named after the J.D. Salinger story where a boy overhears two house servants talking about the family, and he hurries to the pier where he tries to run away in a boat. With Chartreuse and cucumber juice, the drink reminded me of LUPEC&#39;s Irma la Douce, and with whiskey and a vegetable juice syrup, it shared some similarities to Rob&#39;s Lamplighter at Chez Henri.\\n\\nThe Down at the Dinghy began with a soft Irish whiskey and cucumber aroma that was complemented by savory herbal notes from the Yellow Chartreuse. A malty cucumber-flavored sip had some crisp citrus notes from the lemon juice, and this led into a savory Yellow Chartreuse swallow that displayed the drink&#39;s sweetness.&quot; Now we’re ready to do some analysis! 11.1.5 Using basic text analysis to find descriptions We’re going to make a big assumption about how flavors are described in English in order to do some quick text analysis. Specifically, we know empirically that most descriptive terms–“sweet”, “sour”, “rose”–are either adjectives or nouns. So if we can find a tool that will identify tokens as Parts of Speech (POS) for us, we will be able to just select those POS we want and get a rough idea of the descriptive language in the description we’ve extracted for each cocktail. We can then see, for example, which descriptive terms are associated with which cocktail ingredients. We are going to make use of a freely available piece of software called spaCy, which is actually a Python-based neural network trained to do a whole suite of text analysis tasks. Happily, there is a nice interface for it in R called spacyr, which we will use to access it. NB: You will need to not only install the spacyr package, but also run spacyr::spacy_install() if you want to run this code. The defaults are fine for this demo. spacy_initialize() # This is the TIF format (https://github.com/ropensci/tif) required by spacyr: two-columns, one with a document ID and the other with the full text demo_posts_separated %&gt;% transmute(doc_id = post_id, text = description) %&gt;% # spaCy is a pre-trained neural-network for Natural Language Processing, here we request additional attributes: is it a stopword and is it like a number? spacy_parse(additional_attributes = c(&quot;is_stop&quot;, &quot;like_num&quot;)) %&gt;% as_tibble() -&gt; demo_posts_parsed spacy_finalize() demo_posts_parsed ## # A tibble: 16,434 × 9 ## doc_id sentence_id token_id token lemma pos entity is_stop like_…¹ ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1619 1 1 One one NUM &quot;CARDINA… TRUE TRUE ## 2 1619 1 2 of of ADP &quot;&quot; TRUE FALSE ## 3 1619 1 3 the the DET &quot;&quot; TRUE FALSE ## 4 1619 1 4 surprise surprise NOUN &quot;&quot; FALSE FALSE ## 5 1619 1 5 winners winner NOUN &quot;&quot; FALSE FALSE ## 6 1619 1 6 of of ADP &quot;&quot; TRUE FALSE ## 7 1619 1 7 the the DET &quot;&quot; TRUE FALSE ## 8 1619 1 8 Le Le PROPN &quot;&quot; FALSE FALSE ## 9 1619 1 9 Mixeur Mixeur PROPN &quot;NORP_B&quot; FALSE FALSE ## 10 1619 1 10 Sharky Sharky PROPN &quot;&quot; FALSE FALSE ## # … with 16,424 more rows, and abbreviated variable name ¹​like_num ## # ℹ Use `print(n = ...)` to see more rows We see that spaCy outputs another tidy-type data structure, where we now have a one-token-per-line dataframe (we cast it into a tibble for our own preferences), which lists each word in its in-context form (“One”) as well as it’s lemma or dictionary form (“one”), with information about its POS as well as some other properties we might care about. We then need to do some wrangling to re-attach our parsed data to our original data, which has metadata (factors, treatments, or other information) that we are interested in, like the ingredients, title, date, and URL: demo_posts_parsed %&gt;% # these conditions are that the part-of-speech is adjective, the lemma is not a &quot;named entity&quot; (like a proper noun), and is not a stop-word or number, and is not a singe character filter(pos == &quot;ADJ&quot;, !is_stop, !like_num, entity == &quot;&quot;, str_length(lemma) &gt; 1) %&gt;% select(doc_id, token, lemma) %&gt;% # here we rejoin the parsed tokens with the original data left_join(demo_posts_separated %&gt;% mutate(post_id = as.character(post_id)), by = c(&quot;doc_id&quot; = &quot;post_id&quot;)) -&gt; demo_posts_tokenized Now, for each post we have: Cleaned up metadata Separated recipes and descriptions Tidy-by-token information about adjectives used in the description We’re ready to do some analysis! 11.1.6 Quantifying text data First, let’s just look at the most frequent adjectives used in this dataset: demo_posts_tokenized %&gt;% count(lemma, sort = TRUE) ## # A tibble: 258 × 2 ## lemma n ## &lt;chr&gt; &lt;int&gt; ## 1 herbal 37 ## 2 bitter 25 ## 3 orange 25 ## 4 dry 20 ## 5 sweet 17 ## 6 dark 16 ## 7 floral 11 ## 8 good 11 ## 9 little 11 ## 10 nutty 10 ## # … with 248 more rows ## # ℹ Use `print(n = ...)` to see more rows Now, we’re going to do something a little more interesting: we’re going to use the per-ingredient neat dataframe we created earlier to add descriptions to each drink by descriptor, and then use that data to ask which ingredients are associated with each description: demo_ingredients %&gt;% transmute(post_id = as.character(post_id), title, ingredient) %&gt;% left_join(demo_posts_parsed %&gt;% filter(pos == &quot;ADJ&quot;, !is_stop, !like_num, entity == &quot;&quot;, str_length(lemma) &gt; 1) %&gt;% select(doc_id, token, lemma), by = c(&quot;post_id&quot; = &quot;doc_id&quot;)) -&gt; demo_descriptors demo_descriptors ## # A tibble: 3,457 × 5 ## post_id title ingredient token lemma ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1619 down at the dinghy chartreuse (yellow) soft soft ## 2 1619 down at the dinghy chartreuse (yellow) savory savory ## 3 1619 down at the dinghy chartreuse (yellow) herbal herbal ## 4 1619 down at the dinghy chartreuse (yellow) crisp crisp ## 5 1619 down at the dinghy chartreuse (yellow) savory savory ## 6 1619 down at the dinghy lemon juice soft soft ## 7 1619 down at the dinghy lemon juice savory savory ## 8 1619 down at the dinghy lemon juice herbal herbal ## 9 1619 down at the dinghy lemon juice crisp crisp ## 10 1619 down at the dinghy lemon juice savory savory ## # … with 3,447 more rows ## # ℹ Use `print(n = ...)` to see more rows Finally, we will use a methodology called “Term Frequency/Inverse Document-Frequency” (TF-IDF) to give us an idea of which descriptive adjective is more associated with particular ingredients. demo_descriptors %&gt;% count(ingredient, lemma, sort = TRUE) %&gt;% # TF-IDF is a metric that looks at the (log) ratio of frequency of term in a document (here an ingredient) to frequency of the term across the whole corpus bind_tf_idf(term = lemma, document = ingredient, n = n) -&gt; demo_tf_idf demo_tf_idf ## # A tibble: 2,554 × 6 ## ingredient lemma n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bitters (angostura) herbal 12 0.0741 0.466 0.0345 ## 2 rum dark 12 0.0638 0.782 0.0499 ## 3 gin herbal 10 0.0543 0.466 0.0253 ## 4 gin orange 10 0.0543 0.693 0.0377 ## 5 lemon juice herbal 10 0.0617 0.466 0.0288 ## 6 lime juice dark 10 0.0463 0.782 0.0362 ## 7 lime juice herbal 10 0.0463 0.466 0.0216 ## 8 benedictine herbal 9 0.141 0.466 0.0655 ## 9 rum herbal 9 0.0479 0.466 0.0223 ## 10 vermouth (sweet) herbal 9 0.125 0.466 0.0582 ## # … with 2,544 more rows ## # ℹ Use `print(n = ...)` to see more rows demo_tf_idf %&gt;% group_by(ingredient) %&gt;% mutate(lemma = reorder_within(lemma, by = tf_idf, within = ingredient)) %&gt;% top_n(n = 10, wt = tf_idf) %&gt;% filter(ingredient %in% c(&quot;gin&quot;, &quot;rum&quot;, &quot;campari&quot;, &quot;whiskey (rye)&quot;, &quot;scotch&quot;, &quot;vermouth (sweet)&quot;)) %&gt;% ungroup() %&gt;% mutate(lemma = reorder(lemma, tf_idf)) %&gt;% ggplot(aes(x = lemma, y = tf_idf)) + geom_col(aes(fill = ingredient %&gt;% as.factor), show.legend = F) + coord_flip() + scale_x_reordered() + facet_wrap(~ingredient, scales = &quot;free&quot;) + theme_classic() We can see some patterns that, in my opinion, give us a taste of both the possibilities and the perils of this kind of analysis. I will just give a brief set of thoughts: It appears that this fairly basic workflow gives us some real information about the tastes of cocktail ingredients–there are many descriptive terms when we ID adjectives in the tokens found in these blog posts. The terms we find associated with particular products matches expert knowledge. This is a really rough workflow: we see lots of terms we don’t want, we haven’t picked a big dataset, etc. Is our expertise biasing our opinion of how good these results are? And I will just end with a question that I’d love your thoughts on: What advantages and disadvantages do we see from turning language–these complex blog posts–into data we can count? What do we gain, what do we lose, how do we communicate this kind of work? Ok, I lied, one more question: What else would you like to see? What next? 11.2 Data literacy 11.2.1 Introduction We’re going to end our last new material/lecture (!!!) for the semester talking about some larger issues within data science. I hope that through this semester you have come to feel some confidence in your ability to tackle basic data-analytics tasks using a coding approach (with R, but also with whatever software you end up using in the future). While there is an infinite amount left for you to learn about coding for research, that is because there is an infinite amount for me to learn, as well. I am constantly learning (and relearning) new tricks and methods. With a basic understanding of good coding practice, control flow, and data simulation and wrangling, I honestly think you can tackle any problems that come your way. Therefore, today we’re going to frame the class and discussion a little bit differently, because I want to focus on what I am calling data literacy. While I am going to use coding examples throughout to illustrate the discussion, today isn’t going to be about learning new skills. It’s going to be about becoming better producers (and consumers) of quantitative information–better scientists! I think of data literacy as a combination of critical thinking about how data are produced, managed, analyzed, and interpreted and a concern with the ethics surrounding each step of the process. As scientists we are often privileged to not engage directly with the ethics of what we’re doing–sure, we have to write IRBs when we do human-subjects research or IACUCs when we work with animals, but we treat our work as inherently a net positive: producing new knowledge is fundamentally a worthwhile goal. In general, I agree with this statement, but I think when we embrace it uncritically, we end up not considering a host of the aspects of how and why we are doing the work we do, and who it ultimately serves. I will also give a big caveat to the way I am approaching this topic: data literacy is a really huge topic that deserves a class on its own (and if you’re interested in these general ideas, please come talk to me, read the references given here, and consider taking some courses in the Virginia Tech STS Department). In addition, my hope is that by learning how the sausage is made (how many times have I repeated “there is no right answer?” in this putatively quantitative course so far?) you will have developed some critical abilities already. So I am going to present a few provocative examples at various levels that will hopefully get you thinking a bit more about how we can do data analysis better: more humbly, more accurately to reality, and more sensitively to our work’s impact on the world. To this end, here are a number of resources I suggest you read to get some insight into the topics I am going to only address really briefly today: Books: Data Feminism, a book and web project by two data-science researchers in the digital humanities that gives some really good principles for thinking about and with data. Calling Bullshit, a website and book developed by two University of Washington professors that is entertaining and gives good, non-technical introductions to many aspects of data literacy. Counting, a book by Brandeis professor emerita Deborah Stone that gives a non-technical introduction to the politics of making numbers Short reads: Beall on open-access journals, an article by an academic librarian on the corrosive effects of “pressure to publish” and open-access journals on intellectual and academic discourse The gender gap and the 100-meter dash, a wonderful application of basic critical thinking with data applied to a prestigious academic article. “The Ethics of Counting” a lecture on the topic of the ethics and politics of making numbers by Deborah Stone (2018) “Inference for a Bernoulli process (a Bayesian view)”, by Lindley and Phillips (1976), is the most technical article I suggest, but it does offer a remarkably good introduction to Bayesian statistics and thinking, as well as pointing out some key epistemological holes in the frequentist models We’re going to start by surfacing many of the issues that point towards the need for data literacy by discussing a particular issue for psychological and social science in recent years: the replicability crisis. After presenting an overview of the topic, this is going to lead us pretty naturally into our next major topic: the p-value and the larger role of Null-Hypothesis Statistical Testing (also often put in the same bucket as “frequentist statistics”, as opposed to “Bayesian statistics”) in the replicability crisis and mishaps in data analysis in general. Finally, we will zoom out further and think about the politics of counting (credit for this general framework to Deborah Stone 2020): what do numbers actually mean? 11.2.2 The replicability crisis We will begin with a startling article published more than a decade ago by John Ioannidis (2005): “Why most published research findings are false”. In that article, the author provided statistical evidence based in meta-analytical techniques that the generally accepted practice of using the classic \\(p &lt; 0.05\\) formulation as evidence of a true effect in a quantitative, scientific study was more often than not leading researchers to declare that there was a significant effect when in fact there was not: a false positive. In the decade that followed, rigorous replication studies verified these concerns (with any number of caveats): for example, in psychology, in general reported effects are larger than those found in replicated studies when any effect is found, and in many studies it is impossible to replicate the original results at all (Collaboration 2015). There have been many hypotheses put forward for these problems, including (but almost certainly not limited to): Publication (positivity) bias: in general it is much more difficult to publish a negative (“nonsignificant”) result than it is to publish a significant one. Because academic success and grant funding is predicated on publication, there is pressure to “find” significance. This is a powerful and problematic force in science, and should be taken extremely seriously. Related but different is a bias against replication studies, because they tell us “what we already know” (if they reproduce the results) or they produce negative results. This means that in general false positives may remain undiscovered until they become conventional wisdom, at which point disproving them becomes a “positive” result. This publication bias also leads to a bias in the kinds of studies that are conducted: this is a type of selection bias. Imagine you are (like me!) a junior, pre-tenured faculty member, or (like you!) graduate students. Are you going to be willing to conduct research that may be unpublishable? Didn’t think so. These pressures combine with a culture of celebrity and newsworthiness to overvalue sensational (but perhaps unlikely) scientific results that can form the basis of TED Talks, little news segments, etc. We need to be especially skeptical of these kinds of results: a key principle we might hold dear is “extraordinary claims require extraordinary evidence” (Bergstrom and West 2021). “Science” has grown exponentially as an activity over the last century. The sheer volume of the research enterprise has led to the publication of many more results. By definition, a larger number of these results will be false positives, even if the proportion doesn’t change… …but the proportion is much higher than we might reasonably expect, which points to other systemic problems. Statistical incompetence: We all know that statistics are hard, right? We also know specifically (because I’ve said so in this class!) that we often pick and choose statistical and analytical heuristics (“models”) for convenience, not because they are correct. This leads to all kinds of problems that might result in problems with conclusions, of course. Picking the “wrong” statistical test. I hope that this class has given you a taste of an alternative picture of statistics, but often we as scientists learn a set of statistical tests as heuristics: we don’t learn the whys or hows, we just learn to apply an algebraic calculation and the magical statistics machine tells us whether our results are significant. In this context, we instead worry about whether our statistical test is “right”, instead of worrying about “what do the data actually tell us”. Remember this mess? After talking with me about whether your project requires regression or ANOVA, does this really inspire confidence? Inappropriate study designs. We are constrained in resources–time, money, help, samples. We often run studies that don’t really represent an ideal: not enough subjects, samples, treatments, whatever. There are many valid reasons for this, but it leads us to make compromises that then ramify out through our conclusions. Epistemological vagueness around distinctions between correlation, inference, and causality: we have already been introducede to the idea that our intuitions around patterns and causation are not great as humans, and this problem is actually made worse by the machinery we have for statistics. In general, once we have data, we can run all sorts of tests–whether statistical or simulations or both–on those data. But none of those tests can establish causality. It turns out that causal modeling is a topic in-and-of-itself, and too few scientists actually consider the (usually implicit) causal models that their statistical models instantiate (McElreath 2020). I like the non-technical discussion given in the Calling Bullshit case studies explaining how we build causal models in our head by telling ourselves stories. …and here I will get more specific: from the sub-point about “wrong” statistical tests about, let me say what I personally believe to be a major problem causing this (and other!) crises: we have a blind faith in the framework of “Null Hypothesis Statistical Testing” and the associated p-values and confidence intervals. Regardless of the exact whys (and it is almost certainly a combination of these and other factors), science as an enterprise is starting to grapple with how to adapt to all of these problems, but we’re definitely not there yet. For example, the American Statistical Association published a statement about p-values that essentially told people that, yes, we shouldn’t over-rely on them, but they’re fine when used in moderation (Wasserstein and Lazar 2016). Personally, I find this unsatisfying, but there is certainly awareness of the issue. 11.2.2.1 How can we avoid falling into the replication trap? So if all of science if facing a crisis with replicability, does that mean we just give up on science? … Obviously not. Let’s remind ourselves that science is not infallible and science is not really a belief system–so all of those houses with these signs are making a kind of weird leap: Yes and yes, but also…? Science is, at its heart, the process of formulating a testable proposition, testing it, and reporting the results to a broader community, which then starts the cycle again. We accrete a body of theory and knowledge. We don’t “mine facts” that exist “out there”. So science doesn’t depend on belief–it is a process. More importantly, the process of science itself should make us critical thinkers. That should mean we have curious, open minds that act like Bayesians (see below), rather than rigid thinkers who operate on hard and fast rules we come to believe without thinking about them–this makes us martinets. Anyway, that is to say that problems with replication and with a process of statistical proof–which is a process of probabilistic proof—are not flaws that undermine our system of making knowledge. Rather, they point to maybe a problem in what we’ve come to accept as sufficient evidence in adding to our body of knowledge. How can we avoid making it worse? We all operate within a system that isn’t working as it should, so our individual power to make change is limited, but I think that we can incorporate some basic ethics of research data production and consumption: Be realistic (humble?) about the meaning of your data. This applies both to the analysis and to the communication of your work. I don’t want to hear any more about how a tiny experiment is generalizable. It gives us an intriguing further hypothesis to test! Allow multiple interpretations and frameworks for the same phenomena to give overlapping information, from which we are able to gain a richer, more faithful model (this comes from Data Feminism and adapts the philosopher of science Sandra Harding) Be critical about the how and why of data (and science) production–sources matter. Don’t just listen to me–read better-written and better-explained sources on how to do data better, and put them into action. 11.2.3 Some quick thoughts on (Null) Hypothesis Testing (NHST) \\(H_0: \\mu_1 = \\mu_2\\) \\(H_a: \\mu_1 ≠ \\mu_2\\) We are all familiar with these kinds of formalized hypotheses. They are taught as part of basic statistics classes (I teach about them in my undergrad sensory class, FST 3024). They set up a dichotomy: \\(H_0\\) and \\(H_a\\) are mutually exclusive, and so, we reason, if we can “reject” \\(H_0\\) we can tentatively proceed with \\(H_a\\). There are many, many, many problems with this approach, it turns out, as Bayesian statistical methodologist John Kruschke (2010) points out: In collecting data, you take care to insulate the data from your intentions. For example, double-blind procedures in clinical trials insulate the data from experimenter intentions. As another example, in field research, the observers construct elaborate ‘duck blinds’ to minimize the impact of the observer on the data. After carefully collecting the data, you then go through the ritual invocation of p &lt; 0.05. Did you know that the computation of the p value depends crucially on the covert intentions of the analyst, or the analyst’s interpretations of the unknowable intentions of the data collector? This is true despite the emphasis by the data collector to make the data unaffected by his/her intentions, as will be shown below. Moreover, for any set of data, an intention can be found for which p is not less than 0.05. The basic argument Kruschke (2010) is making is that NHST has–in an attempt to provide an accessible heuristic for decision-making for the conscientious, working researcher–elided away the idea of uncertainty. We all know the chant: … \\(p&lt;0.05\\), we reject the null hypothesis. But this means we don’t take into account any information we have on the credibility of our \\(H_a\\), or anything we know about the quality of the experiment, or anything else we, experts in this research might know. Bet you $50 the sun hasn’t exploded Furthermore, this approach transforms what is really a judgment about probability–a description of our uncertainty–into a binary, black and white situation: if \\(p&lt;0.05\\) (or any other value), we reject \\(H_0\\). This just isn’t good reasoning. 11.2.3.1 Torture the data until it gives you the p-values you want As an example of this problem, consider the following. I am going to generate 21, 100-number draws from the normal distribution. These are, by definition, random numbers. y &lt;- rnorm(100) random_data &lt;- rnorm(20 * 100) %&gt;% matrix(ncol = 20, dimnames = list(NULL, paste0(&quot;x&quot;, 1:20))) summary(lm(y ~ random_data)) ## ## Call: ## lm(formula = y ~ random_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.65406 -0.73172 -0.01457 0.68195 2.23725 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.074130 0.110066 0.674 0.50259 ## random_datax1 -0.053767 0.118302 -0.454 0.65072 ## random_datax2 -0.090543 0.114024 -0.794 0.42953 ## random_datax3 0.140844 0.117021 1.204 0.23235 ## random_datax4 0.309496 0.106952 2.894 0.00492 ** ## random_datax5 -0.209708 0.121470 -1.726 0.08818 . ## random_datax6 0.113280 0.121736 0.931 0.35493 ## random_datax7 -0.010036 0.105058 -0.096 0.92414 ## random_datax8 -0.037409 0.111719 -0.335 0.73863 ## random_datax9 -0.141300 0.110750 -1.276 0.20575 ## random_datax10 0.179297 0.103636 1.730 0.08752 . ## random_datax11 0.032130 0.113646 0.283 0.77813 ## random_datax12 -0.001519 0.116344 -0.013 0.98961 ## random_datax13 0.104850 0.118515 0.885 0.37900 ## random_datax14 -0.153986 0.116616 -1.320 0.19050 ## random_datax15 -0.282690 0.118447 -2.387 0.01940 * ## random_datax16 0.171123 0.103183 1.658 0.10120 ## random_datax17 -0.079469 0.121520 -0.654 0.51504 ## random_datax18 -0.015800 0.120713 -0.131 0.89619 ## random_datax19 0.110788 0.124440 0.890 0.37601 ## random_datax20 0.170602 0.115921 1.472 0.14507 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.052 on 79 degrees of freedom ## Multiple R-squared: 0.228, Adjusted R-squared: 0.03261 ## F-statistic: 1.167 on 20 and 79 DF, p-value: 0.3049 What happens when we look for significant \\(p&lt;0.05\\) relationships? This is the kind of data-mining that seems ridiculous… until we start modeling the relationships among hundreds of predictors without some process for validating our findings. Our intentions, implicitly, which are erased by NHST, are that we want to find significance. So we keep testing until we find some (likely coincidental) relationships. This is a form of the next sort of problem we’re going to discuss: observer or selection bias. 11.2.4 Observer (selection) bias Calling Bullshit (Bergstrom and West 2021) gives one of the best explanations of the phenomenon of selection bias that I’m aware of. Like most of you, I am more familiar with the idea of a confounding factor: a variable we fail to include in our analysis that explains a spurious relationship. This is the classic shark and ice-cream relationship: Take that vanilla cone back out of the water! But this XKCD comic points to the other problem–what are called “colliders” in causal theory. The idea is kind of counterintuitive, so this is why I think that the long discussion in Calling Bullshit is worth paying attention to. It is framed, as things so often are, in terms of dating: A common complaint is that niceness and attractiveness seem to be anticorrelated in a dating pool: nice people are less attractive, and less attractive people are nicer. While it is possible that social reinforcement might explain rudeness in more attractive people (for example), this is equally likely to be a form of selection bias or a statistical collider. We can use data science to simulate the situation. Let’s have someone named “A” as our example. Let’s assume the (probable reality) that niceness and attractiveness are fully uncorrelated. But also assume that A has some basic threshold for niceness and for attractiveness: A simply won’t date people who are both unattractive and mean. Furthermore, let’s assume that A isn’t a celebrity–they have an upper limit on their own datability. These simple constraints, as we’ll see, are enough to induce a negative correlation between niceness and attractiveness in the group of people available for A to date. # Let&#39;s make a bunch of prospective mates dating_app &lt;- tibble(attractive = rnorm(1e4, mean = 5, sd = 2), nice = rnorm(1e4, mean = 5, sd = 2), datability = attractive + nice) the_pool &lt;- dating_app %&gt;% ggplot(aes(x = attractive, y = nice)) + geom_point(alpha = 0.5, shape = &quot;.&quot;) + geom_smooth(method = lm, se = FALSE, color = &quot;black&quot;) + annotate(geom = &quot;label&quot;, x = 0, y = 0, label = &quot;italic(r) == 0.006&quot;, parse = TRUE) + theme_classic() datable &lt;- dating_app %&gt;% ggplot(aes(x = attractive, y = nice)) + geom_point(alpha = 0.5, shape = &quot;.&quot;) + geom_point(data = . %&gt;% filter(datability &gt;= 8), color = &quot;red&quot;, alpha = 0.5, shape = &quot;.&quot;) + geom_smooth(data = . %&gt;% filter(datability &gt;= 8), method = lm, se = FALSE, color = &quot;red&quot;) + geom_abline(aes(intercept = 8, slope = -1), linetype = &quot;dashed&quot;) + annotate(geom = &quot;label&quot;, x = 0, y = 0, label = &quot;italic(r) == -0.29&quot;, parse = TRUE) + theme_classic() gettable &lt;- dating_app %&gt;% ggplot(aes(x = attractive, y = nice)) + geom_point(alpha = 0.5, shape = &quot;.&quot;) + geom_point(data = . %&gt;% filter(datability &lt;= 14), color = &quot;orange&quot;, alpha = 0.5, shape = &quot;.&quot;) + geom_smooth(data = . %&gt;% filter(datability &lt;= 14), method = lm, se = FALSE, color = &quot;orange&quot;) + geom_abline(aes(intercept = 14, slope = -1), linetype = &quot;dashed&quot;) + annotate(geom = &quot;label&quot;, x = 0, y = 0, label = &quot;italic(r) == -0.14&quot;, parse = TRUE) + theme_classic() the_actual_pool &lt;- dating_app %&gt;% ggplot(aes(x = attractive, y = nice)) + geom_point(alpha = 0.5, shape = &quot;.&quot;) + geom_point(data = . %&gt;% filter(datability &gt;= 8 &amp; datability &lt;= 14), color = &quot;violet&quot;, alpha = 0.5, shape = &quot;.&quot;) + geom_smooth(data = . %&gt;% filter(datability &gt;= 8 &amp; datability &lt;= 14), method = lm, se = FALSE, color = &quot;violet&quot;) + geom_abline(aes(intercept = 8, slope = -1), linetype = &quot;dashed&quot;) + geom_abline(aes(intercept = 14, slope = -1), linetype = &quot;dashed&quot;) + annotate(geom = &quot;label&quot;, x = 0, y = 0, label = &quot;italic(r) == -0.53&quot;, parse = TRUE) + theme_classic() library(patchwork) (the_pool + datable) / (gettable + the_actual_pool) + plot_annotation(caption = &quot;After Bergstrom &amp; West (2021)&quot;) dating_app %&gt;% filter(datability &gt;= 8 &amp; datability &lt;= 14) %&gt;% lm(nice ~ attractive, data = .) %&gt;% summary() ## ## Call: ## lm(formula = nice ~ attractive, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7750 -1.0262 -0.0619 1.0082 4.2828 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.14460 0.05725 142.27 &lt;2e-16 *** ## attractive -0.52252 0.01023 -51.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.372 on 6789 degrees of freedom ## Multiple R-squared: 0.2777, Adjusted R-squared: 0.2776 ## F-statistic: 2610 on 1 and 6789 DF, p-value: &lt; 2.2e-16 This may seem like a cute example, but let’s try looking at real-world consequences. Adapting a similar example from McElreath (2020), we could apply this same reasoning to looking at grant funding (or your grades!). We know that often rubrics for funding require rating proposals on a number of (ideally uncorrelated) aspects. For example, a simple rubric might evaluate proposals on how neat it is and scientific quality. These are not necessarily correlated (as you will know if you read the popular scientific press releases), so we are once again in a similar situation. Each of these could be rated on a 10-pt scale, and the funders might decide that they will only consider funding proposals that have a combined score of over 15. We are back in the same situation again, but in an even worse position: funding &lt;- tibble(neat = rnorm(200, 5, 2), quality = rnorm(200, 5, 2), score = neat + quality) funding %&gt;% ggplot(aes(x = neat, y = quality)) + geom_point(alpha = 0.5) + geom_point(data = . %&gt;% filter(score &gt;= 15), color = &quot;red&quot;) + geom_smooth(data = . %&gt;% filter(score &gt;= 15), method = lm, se = FALSE, color = &quot;red&quot;) + annotate(geom = &quot;label&quot;, x = 0, y = 0, label = &quot;italic(r) == -0.94&quot;, parse = TRUE) + geom_abline(slope = -1, intercept = 15, linetype = &quot;dashed&quot;) + theme_classic() + labs(caption = &quot;After McElreath (2020)&quot;) We’ve now created a situation in which if research we fund is neat, it is likely of lower scientific quality, and vice versa, through a simple (and unintentional) selection effect. 11.3 Counting Finally, because we’re way over time, I just want to use the previous examples to point out something important. No-one questioned my construction of my niceness/attractiveness or neat/quality metrics. But these are political choices by themselves. How do we assign people into numbers for their personality and appearance qualities? How do we make rubrics in the first place to make categories for funding? There is no objective reality in these decisions–they are the result of some sort of implicit or explicit social consensus. They change based on context–cultural, social, temporal. Take the infamous 3/5 compromise, proposed into law by the third president, James Madison. To quote Deborah Stone (2018): Numbers get their authority from people who are able to exert rhetorical and political power to assign words to things and things to categories. When the delegates at the Constitutional Convention discussed how to count the population in the federal census, the most contentious issue was whether to count slaves as property, in which case their owners would be taxed on them; or to count them as people, in which case the states where they lived would get more representatives in Congress. In the infamous compromise ultimately written into the US Constitution, slaves were counted as three-fifths of a person. In The Federalist No. 54, James Madison defended the rightness of this way of counting: “[T]he Federal Constitution therefore, decides with great propriety on the case of our slaves, when it views them in the mixt character of persons and property. This is in fact their true character.” (Madison, Federalist Papers No 54) … When we left off talking about validity, I asked you to explain what you cut—what didn’t get into the heap of peas to be counted. Now I want to ask you to justify your cuts. I encourage–no, I ask–you to take a look at the whole lecture, as I think the question of how we count is something we should really be asking ourselves more as data-literate scientists. And with that, we’re done! References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
